{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Continuous Assessment - Class Group: TU060 <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSc in Computer Science - Data Science (Part Time) <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Name: Ciaran Finnegan  <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student No: D21124026 <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student EMail: D21124026@mytudublin.ie <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## May 2022<a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H&M Personalised Fashion Recommendations <a class=\"tocSkip\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This project takes the datasets provided by Kaggle for the H&M Personalised Fashion Recommendations competition.  <a class=\"tocSkip\">\n",
    "\n",
    "Ipso forum..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The CA follows a n-part structure. <a class=\"tocSkip\">\n",
    "\n",
    "Section 1: Ipso forum...\n",
    "\n",
    "Section 2: Ipso forum...\n",
    "\n",
    "Section 3: Ipso forum...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Jupyter-Notebook-Set-Up\" data-toc-modified-id=\"Jupyter-Notebook-Set-Up-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Jupyter Notebook Set Up</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Import Libraries</a></span><ul class=\"toc-item\"><li><span><a href=\"#General-Utility-Libraries\" data-toc-modified-id=\"General-Utility-Libraries-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>General Utility Libraries</a></span></li><li><span><a href=\"#Panda-+-Numpy\" data-toc-modified-id=\"Panda-+-Numpy-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Panda + Numpy</a></span></li><li><span><a href=\"#Matplotlib-Library\" data-toc-modified-id=\"Matplotlib-Library-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Matplotlib Library</a></span></li><li><span><a href=\"#Plotly-Libraries\" data-toc-modified-id=\"Plotly-Libraries-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Plotly Libraries</a></span></li></ul></li></ul></li><li><span><a href=\"#Project-Data-Import,-Analysis-and-Preparation\" data-toc-modified-id=\"Project-Data-Import,-Analysis-and-Preparation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Project Data Import, Analysis and Preparation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-and-Sample-Kaggle-H&amp;M-Datasets\" data-toc-modified-id=\"Load-and-Sample-Kaggle-H&amp;M-Datasets-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Load and Sample Kaggle H&amp;M Datasets</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-Up-File-Paths\" data-toc-modified-id=\"Set-Up-File-Paths-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Set Up File Paths</a></span></li><li><span><a href=\"#H&amp;M-Transaction-Dataset\" data-toc-modified-id=\"H&amp;M-Transaction-Dataset-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>H&amp;M Transaction Dataset</a></span></li><li><span><a href=\"#H&amp;M-Articles-Dataset\" data-toc-modified-id=\"H&amp;M-Articles-Dataset-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>H&amp;M Articles Dataset</a></span></li><li><span><a href=\"#H&amp;M-Customers-Dataset\" data-toc-modified-id=\"H&amp;M-Customers-Dataset-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>H&amp;M Customers Dataset</a></span></li></ul></li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#H&amp;M-Sampled-Transaction-Dataset\" data-toc-modified-id=\"H&amp;M-Sampled-Transaction-Dataset-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>H&amp;M Sampled Transaction Dataset</a></span></li><li><span><a href=\"#H&amp;M-Sampled-Customer-Dataset\" data-toc-modified-id=\"H&amp;M-Sampled-Customer-Dataset-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>H&amp;M Sampled Customer Dataset</a></span></li><li><span><a href=\"#H&amp;M-Sampled-Articles-Dataset\" data-toc-modified-id=\"H&amp;M-Sampled-Articles-Dataset-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>H&amp;M Sampled Articles Dataset</a></span></li></ul></li><li><span><a href=\"#Data-Preparation-and-Feature-Enrichment\" data-toc-modified-id=\"Data-Preparation-and-Feature-Enrichment-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Data Preparation and Feature Enrichment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Transaction-Data-Feature-Enrichment-(Sample)\" data-toc-modified-id=\"Transaction-Data-Feature-Enrichment-(Sample)-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Transaction Data Feature Enrichment (Sample)</a></span></li><li><span><a href=\"#Extend-Dataframe-with-Customer-Data\" data-toc-modified-id=\"Extend-Dataframe-with-Customer-Data-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Extend Dataframe with Customer Data</a></span></li><li><span><a href=\"#Extend-Dataframe-with-Articles-Data\" data-toc-modified-id=\"Extend-Dataframe-with-Articles-Data-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>Extend Dataframe with Articles Data</a></span></li><li><span><a href=\"#Final-Data-Preparation-Steps\" data-toc-modified-id=\"Final-Data-Preparation-Steps-2.3.4\"><span class=\"toc-item-num\">2.3.4&nbsp;&nbsp;</span>Final Data Preparation Steps</a></span></li></ul></li></ul></li><li><span><a href=\"#Split-Data:-Train-and-Test-Sets\" data-toc-modified-id=\"Split-Data:-Train-and-Test-Sets-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Split Data: Train and Test Sets</a></span></li><li><span><a href=\"#Model-Preparation\" data-toc-modified-id=\"Model-Preparation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Model Preparation</a></span></li><li><span><a href=\"#K-Nearest-Neighbour-Operations\" data-toc-modified-id=\"K-Nearest-Neighbour-Operations-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>K Nearest Neighbour Operations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-Functions-for-NN-Analysis\" data-toc-modified-id=\"Create-Functions-for-NN-Analysis-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Create Functions for NN Analysis</a></span></li></ul></li><li><span><a href=\"#Build-Submission\" data-toc-modified-id=\"Build-Submission-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Build Submission</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-Sample-Submission-File\" data-toc-modified-id=\"Read-Sample-Submission-File-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Read Sample Submission File</a></span></li><li><span><a href=\"#Generate-NN-Search-Vector\" data-toc-modified-id=\"Generate-NN-Search-Vector-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Generate NN Search Vector</a></span><ul class=\"toc-item\"><li><span><a href=\"#Find-Customer-Transactions\" data-toc-modified-id=\"Find-Customer-Transactions-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Find Customer Transactions</a></span></li><li><span><a href=\"#Add-Customer-Features\" data-toc-modified-id=\"Add-Customer-Features-6.2.2\"><span class=\"toc-item-num\">6.2.2&nbsp;&nbsp;</span>Add Customer Features</a></span></li><li><span><a href=\"#Add-Article-Features\" data-toc-modified-id=\"Add-Article-Features-6.2.3\"><span class=\"toc-item-num\">6.2.3&nbsp;&nbsp;</span>Add Article Features</a></span></li><li><span><a href=\"#Set-Up-NN-Vector-for-Customer\" data-toc-modified-id=\"Set-Up-NN-Vector-for-Customer-6.2.4\"><span class=\"toc-item-num\">6.2.4&nbsp;&nbsp;</span>Set Up NN Vector for Customer</a></span></li></ul></li><li><span><a href=\"#Obtain-Customer-Article-Predictions\" data-toc-modified-id=\"Obtain-Customer-Article-Predictions-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Obtain Customer Article Predictions</a></span></li><li><span><a href=\"#Parse-Article-Predictions-for-Submission-File\" data-toc-modified-id=\"Parse-Article-Predictions-for-Submission-File-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Parse Article Predictions for Submission File</a></span></li><li><span><a href=\"#Write-Predictions-Submission-File-to-CSV\" data-toc-modified-id=\"Write-Predictions-Submission-File-to-CSV-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Write Predictions Submission File to CSV</a></span><ul class=\"toc-item\"><li><span><a href=\"#SetUp-Disk-Directory-for-Submission-File\" data-toc-modified-id=\"SetUp-Disk-Directory-for-Submission-File-6.5.1\"><span class=\"toc-item-num\">6.5.1&nbsp;&nbsp;</span>SetUp Disk Directory for Submission File</a></span></li><li><span><a href=\"#Create-Submission-.CSV-File\" data-toc-modified-id=\"Create-Submission-.CSV-File-6.5.2\"><span class=\"toc-item-num\">6.5.2&nbsp;&nbsp;</span>Create Submission .CSV File</a></span></li></ul></li></ul></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebook Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: It is assumed that the matplotlib and plotly library has been installed on the user's machine before it is available for import.\n",
    "\n",
    "```bash\n",
    "> pip install matplotlib\n",
    "> pip install plotly\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Utility Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Panda + Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python libraries for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up additional libraries for graphical presentations. A number of earlier graphs use Matplotlib. In later sections, Plotly graphs are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matplotlib Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotly Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For interactive graphics and global displays\n",
    "#import plotly.express as px\n",
    "#import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Data Import, Analysis and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Sample Kaggle H&M Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The assignment starts with a Kaggle H&M Personalised Fashion Recommendations datasets. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up File Paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a Global path file variable to use when loading each dataset\n",
    "Path_To_Inputs = '../input'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H&M Transaction Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load H&M Transaction dataset\n",
    "df_trxns = pd.read_csv(os.path.join(Path_To_Inputs,'transactions_train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Number of Rows, Number of Columns in Transaction dataset prior to Sampling\n",
    "df_trxns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract random sample of Transactions (without replacement) - 0.1% of records\n",
    "df_trxns_sample = df_trxns.sample(frac=0.001, replace=False)\n",
    "## NEW CODE - Check this out\n",
    "## df_trxns_sample.set_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Number of Rows, Number of Columns in Transaction dataset after Sampling\n",
    "df_trxns_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the index\n",
    "df_trxns_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H&M Articles Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file, located in 'input' folder above this notebook, for articles data - containing meta data on purchasable items.\n",
    "df_Articles = pd.read_csv(os.path.join(Path_To_Inputs,'articles.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Number of Rows, Number of Columns in Articles dataset\n",
    "df_Articles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic file data for the initial Kaggle H&M 'articles' datatset\n",
    "# df_Articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the H&M articles contained in the sample Transaction dataset \n",
    "articleIDs_in_Trxn_Sample  = set(df_trxns_sample[\"article_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Article Ids in Transaction Sample\n",
    "#len(articleIDs_in_Trxn_Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset that only contains H&M articles in the sample Transaction dataset\n",
    "df_articles_sample = df_Articles[df_Articles[\"article_id\"].isin(articleIDs_in_Trxn_Sample)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Number of Rows, Number of Columns in Sample Articles dataset\n",
    "df_articles_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H&M Customers Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load H&M Customer dataset\n",
    "df_customers = pd.read_csv(os.path.join(Path_To_Inputs,'customers.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Number of Rows, Number of Columns in Customers dataset prior to Sampling\n",
    "df_customers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the H&M Customers contained in the sample Transaction dataset \n",
    "custIDs_in_Trxn_Sample  = set(df_trxns_sample[\"customer_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Customer Ids in Transaction Sample\n",
    "len(custIDs_in_Trxn_Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset that only contains H&M Customers from the sample Transaction dataset\n",
    "df_customers_sample = df_customers[df_customers[\"customer_id\"].isin(custIDs_in_Trxn_Sample)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Number of Rows, Number of Columns in Sample Customers dataset\n",
    "df_customers_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display key data attributes and distributions in Sampled H&M datasets. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H&M Sampled Transaction Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic column information for the initial Kaggle 'transaction_train' datatset - after Sampling of original dataset\n",
    "df_trxns_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display datatype of colums in original H&M Transaction dataset\n",
    "df_trxns_sample.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H&M Sampled Customer Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic column information for the initial Kaggle 'customers' datatset - after Sampling of original dataset\n",
    "df_customers_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers_sample.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H&M Sampled Articles Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic column information for the initial Kaggle 'article' datatset - after Sampling of original dataset\n",
    "df_articles_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and Feature Enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a dataset upon which to build ML models. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin with Transaction Dataset (sampled) and merge in customer and article data. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transaction Data Feature Enrichment (Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataset for modelling. Initial basis for new dataset is the sampled transaction dataset.\n",
    "df_HM_CustTrxn = df_trxns_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_CustTrxn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose Datetime information \n",
    "df_HM_CustTrxn['t_dat'] = pd.to_datetime(df_HM_CustTrxn['t_dat'], format=\"%Y-%m-%d\")\n",
    "\n",
    "# Add Column for Transaction Year and convert to integer value\n",
    "df_HM_CustTrxn['txn_year'] = df_HM_CustTrxn['t_dat'].dt.strftime('%Y')\n",
    "df_HM_CustTrxn['txn_year'] = df_HM_CustTrxn['txn_year'].astype(str).astype(int)\n",
    "\n",
    "# Add Column for Transaction Month and convert to integer value\n",
    "df_HM_CustTrxn['txn_mth'] = df_HM_CustTrxn['t_dat'].dt.strftime('%m')\n",
    "df_HM_CustTrxn['txn_mth'] = df_HM_CustTrxn['txn_mth'].astype(str).astype(int)\n",
    "\n",
    "# Add Column for Transaction Day and convert to integer value\n",
    "df_HM_CustTrxn['txn_day'] = df_HM_CustTrxn['t_dat'].dt.strftime('%d')\n",
    "df_HM_CustTrxn['txn_day'] = df_HM_CustTrxn['txn_day'].astype(str).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out column to drop timestamp attribute\n",
    "df_HM_CustTrxn = df_HM_CustTrxn.iloc[:,1:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_CustTrxn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_CustTrxn.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in the Transaction Dataframe\n",
    "[print('Any Null Values in the Trxn (sample) dataframe?','\\n' ,df_HM_CustTrxn.isnull().any())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Dataframe with Customer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm dimensions of Trxn (sample) dataframe BEFORE merging with Customer data\n",
    "df_HM_CustTrxn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Customer Attributes into the Trxn (Sample) Dataframe. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Customer atributes from the Customer dataset based on a join with the 'customer_id' field\n",
    "df_HM_CustTrxn = pd.merge(df_HM_CustTrxn, df_customers_sample, how=\"left\", on=[\"customer_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm dimensions of Trxn (sample) dataframe AFTER merging with Customer data\n",
    "df_HM_CustTrxn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_CustTrxn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out other Customer specific attributes apart from; age + fashion_news_frequency + club_member_status + postal code\n",
    "# There are very many missing values in the other Customer dataset columns, so these will be ignored\n",
    "df_HM_CustTrxn = df_HM_CustTrxn.iloc[:, [0, 1, 2, 3, 4, 5, 6, 9, 10, 11, 12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_CustTrxn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_CustTrxn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in the merged Trxn/Customer dataframe\n",
    "[print('Any Null Values in the merged Trxn/Customer dataframe?','\\n' ,df_HM_CustTrxn.isnull().any())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Rows with Missing Club Member Status Attributes. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in the club_member_status column  \n",
    "df_null_club_member_status = df_HM_CustTrxn.loc[(df_HM_CustTrxn[\"club_member_status\"].isnull())]\n",
    "\n",
    "[print('Numbers of rows with missing club member status?','\\n' ,len(df_null_club_member_status.index))]\n",
    "[print('\\nPrecentage of total rows in Trxn dataframe with missing club member status attributes?\\n{:06.3f}%'\n",
    "                               .format((len(df_null_club_member_status.index)/len(df_HM_CustTrxn.index))*100))]\n",
    "\n",
    "\n",
    "# For simplicity the rows with missing values in the club member status will be ignored\n",
    "# Re-Build Txn/Customer Dataframe with no missing values in the club member status\n",
    "df_HM_CustTrxn = df_HM_CustTrxn.loc[(df_HM_CustTrxn[\"club_member_status\"].notnull())]\n",
    "\n",
    "# Confirm dimensions of Trxn (sample) dataframe AFTER removing rows with missing values in the club member status column\n",
    "[print('\\nCurrent Number of Rows/Columns in Trxn/Customer dataframe - with missing club mem status values removed','\\n' ,\n",
    "                                                                                                   df_HM_CustTrxn.shape)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Rows with Missing Fashion News Frequency Attributes. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in the club_member_status column  \n",
    "df_null_fn_freq = df_HM_CustTrxn.loc[(df_HM_CustTrxn[\"fashion_news_frequency\"].isnull())]\n",
    "\n",
    "[print('Numbers of rows with missing fashion_news_frequency values?','\\n' ,len(df_null_fn_freq.index))]\n",
    "[print('\\nPrecentage of total rows in Trxn dataframe with missing fashion_news_frequency attributes?\\n{:06.3f}%'\n",
    "                               .format((len(df_null_fn_freq.index)/len(df_HM_CustTrxn.index))*100))]\n",
    "\n",
    "# For simplicity the rows with missing values in the fashion_news_frequency column will be ignored\n",
    "# Re-Build Txn/Customer Dataframe with no missing values in the fashion_news_frequency column\n",
    "df_HM_CustTrxn = df_HM_CustTrxn.loc[(df_HM_CustTrxn[\"fashion_news_frequency\"].notnull())]\n",
    "\n",
    "# Confirm dimensions of Trxn (sample) dataframe AFTER removing rows with missing values in the fashion_news_frequency column\n",
    "[print('\\nCurrent Number of Rows/Columns in Trxn/Customer dataframe - missing Fash News Freq values removed','\\n' ,\n",
    "                                                                                                   df_HM_CustTrxn.shape)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Rows with Missing Age Attributes. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in the age column  \n",
    "df_null_age = df_HM_CustTrxn.loc[(df_HM_CustTrxn[\"age\"].isnull())]\n",
    "\n",
    "[print('Numbers of rows with missing age values?','\\n' ,len(df_null_age.index))]\n",
    "[print('\\nPrecentage of total rows in Trxn dataframe with missing age attributes?\\n{:06.3f}%'\n",
    "                               .format((len(df_null_age.index)/len(df_HM_CustTrxn.index))*100))]\n",
    "\n",
    "# Re-Build Txn/Customer Dataframe with no missing values in the age column\n",
    "df_HM_CustTrxn = df_HM_CustTrxn.loc[(df_HM_CustTrxn[\"age\"].notnull())]\n",
    "\n",
    "\n",
    "# Confirm dimensions of Trxn (sample) dataframe AFTER removing rows with missing values in the age column\n",
    "[print('\\nCurrent Number of Rows/Columns in Trxn/Customer dataframe - missing ages removed','\\n' ,df_HM_CustTrxn.shape)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the club_member_status + fashion_news_frequency attributes <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The categorical values for club_member_status must be encoded to numberical values\n",
    "df_encoded_member_stats = pd.get_dummies(df_HM_CustTrxn['club_member_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded_member_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Encoded club_member_status to our merged H&M dataframe\n",
    "df_HM_CustTrxn = df_HM_CustTrxn.join(df_encoded_member_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_CustTrxn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The categorical values for fashion_news_frequency must be encoded to numberical values\n",
    "df_encoded_fn_freq = pd.get_dummies(df_HM_CustTrxn['fashion_news_frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded_fn_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Encoded fashion_news_frequency to our merged H&M dataframe\n",
    "df_HM_CustTrxn = df_HM_CustTrxn.join(df_encoded_fn_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_CustTrxn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the prior Categorical columns from the Trxn/Customer dataframe\n",
    "# The 'inplace' parameter confirms that the drop operation takes place in the same dataframe (a new one is not created)\n",
    "df_HM_CustTrxn.drop([\"club_member_status\",\"fashion_news_frequency\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Postal Code - a non numerical value that is not easily converted\n",
    "df_HM_CustTrxn.drop([\"postal_code\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_CustTrxn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_CustTrxn.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm dimensions of Trxn (sample) dataframe AFTER encoding Categorical Values\n",
    "[print('\\nCurrent Number of Rows/Columns in Trxn/Customer dataframe - after encoding of categorical values completed','\\n' ,\n",
    "                                                                                                   df_HM_CustTrxn.shape)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in the Transaction Dataframe\n",
    "[print('Any Null Values in the merged Trxn/Customer (sample) dataframe?','\\n' ,df_HM_CustTrxn.isnull().any())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Dataframe with Articles Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove 'Redundant' Categorical Columns in Articles (sample) Dataframe. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic column information for the initial Kaggle 'article' datatset - after Sampling of original dataset\n",
    "df_articles_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'redundant' categotical columns from Articles (sample) dataframe\n",
    "# Consider a categorical column Redundant when is 'partnered' with a numerical code \n",
    "df_articles_sample.drop([\"prod_name\", \n",
    "                         \"product_type_name\", \n",
    "                         \"graphical_appearance_name\",\n",
    "                         \"colour_group_name\",\n",
    "                         \"perceived_colour_value_name\",\n",
    "                         \"perceived_colour_master_name\",\n",
    "                         \"department_name\",\n",
    "                         \"index_name\",\n",
    "                         \"index_group_name\",\n",
    "                         \"section_name\",\n",
    "                         \"garment_group_name\",\n",
    "                         \"detail_desc\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic column information for the initial Kaggle 'article' datatset - after Sampling of original dataset\n",
    "df_articles_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use One-Hot-Encoding on remaining categorical columns. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_Articles_CatFeatures(df, col_name):\n",
    "    \n",
    "    print('encode_Articles_CatFeatures  ' + col_name)\n",
    "    \n",
    "    # The categorical values in dataframe must be encoded to numberical values\n",
    "    df_encoded_cols = pd.get_dummies(df[col_name])\n",
    "    df_encoded      = df.join(df_encoded_cols)\n",
    "    \n",
    "    # Remove the prior Categorical columns from the Articles dataframe\n",
    "    df_encoded.drop([col_name], axis=1, inplace=True)\n",
    "    \n",
    "    # Display for Debugging purposes\n",
    "    display(df_encoded_cols)\n",
    "    \n",
    "    # Return the dataframe with encoded columsn added\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_cols_to_encode =['product_group_name', 'index_code']\n",
    "for column_name in article_cols_to_encode:\n",
    "    df_articles_sample = encode_Articles_CatFeatures(df_articles_sample, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic column information for the initial Kaggle 'article' datatset - after Sampling of original dataset\n",
    "df_articles_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the prior Categorical columns from the Articles dataframe\n",
    "# df_articles_sample.drop([\"product_group_name\",\"index_code\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic column information for the initial Kaggle 'article' datatset - after Sampling of original dataset\n",
    "df_articles_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles_sample.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Trxn/Customers Dataframe with Articles Dataframe (sample). <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Articles atributes from the Articles dataset based on a join with the 'article_id' field\n",
    "df_HM_CustTrxnArtcls = pd.merge(df_HM_CustTrxn, df_articles_sample, how=\"left\", on=[\"article_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm dimensions of Trxn (sample) dataframe AFTER merging with Articles data\n",
    "df_HM_CustTrxnArtcls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic column information for the initial Kaggle 'transaction_train' datatset - AFTER merging with Articles data\n",
    "df_HM_CustTrxnArtcls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_CustTrxnArtcls.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in the merged Trxn/Customer/Articles dataframe\n",
    "[print('Any Null Values in the merged Trxn/Customer/Articles dataframe?','\\n' ,df_HM_CustTrxnArtcls.isnull().any())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Data Preparation Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Any Duplicates in Final Merged Dataframe. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a check to see if any duplicate entires exist in the merged Trxn/Customer/Articles dataframe\n",
    "[print('Numbers of duplicated rows in the final merged Trxn/Customer/Articles dataframe?','\\n' \n",
    "                                                                       ,df_HM_CustTrxnArtcls.duplicated().sum())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicate rows across all columns\n",
    "duplicateRows = df_HM_CustTrxnArtcls[df_HM_CustTrxnArtcls.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicateRows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows in the merged Trxn/Customer/Articles dataframe\n",
    "df_HM_CustTrxnArtcls = df_HM_CustTrxnArtcls.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-check: Find duplicate rows across all columns\n",
    "duplicateRows = df_HM_CustTrxnArtcls[df_HM_CustTrxnArtcls.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicateRows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm dimensions of Trxn/Customer/Articles (sample) dataframe AFTER removing duplicate rows\n",
    "[print('\\nCurrent Number of Rows/Columns in merged Trxn/Customer/Articles dataframe - duplicates removed','\\n' ,\n",
    "                                                                                           df_HM_CustTrxnArtcls.shape)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### See if there is a way to check the integrity of the customer merging.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data: Train and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data into 70% for Training, 30% for Testing. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test set\n",
    "# Parameter set to 30% for size of Test set\n",
    "# Given that there is ordering in the dataset, the 'shuffle' parameter is set to 'True'\n",
    "train, test = train_test_split(df_HM_CustTrxnArtcls, test_size=0.3, random_state=101, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Confirmation of data split\n",
    "dataDescription = 'H&M Sample Data Split'\n",
    "print(\"\\n{} Training Set Shape : \\n\\t\".format(dataDescription))\n",
    "print(train.shape)\n",
    "print(\"\\n{} Test Set Shape : \\n\\t\".format(dataDescription))\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the Training Dataset (but not the Test set) <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chk_cols =  ['product_code',\n",
    "             'product_type_no',\n",
    "             'graphical_appearance_no',\n",
    "             'colour_group_code',\n",
    "             'perceived_colour_value_id',\n",
    "             'perceived_colour_master_id',\n",
    "             'department_no',\n",
    "             'index_group_no',\n",
    "             'section_no',\n",
    "             'garment_group_no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[chk_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_to_norm = ['price','age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing numerical features so that each feature has mean 0 and variance 1\n",
    "feature_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_trainset_features(df, dtype, no_scale):\n",
    "    for x in df.columns[df.dtypes == dtype]:\n",
    "        if x != no_scale:\n",
    "            df[x] = scaler.fit_transform(np.array(df[x]).reshape(-1,1))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_scale = 'article_id'\n",
    "scale_dtypes = ['int64','int32','float64']\n",
    "for data_types in scale_dtypes:\n",
    "    scale_trainset_features(train, data_types, no_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[chk_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbour Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Functions for NN Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up function to Build Nearest Neighbours Model <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Unsupervised learner for implementing neighbour searches of Customer Transactions\n",
    "# Set default parameters for NN algorithm - these can be overwritten during optimisationphase\n",
    "def setup_NN_model(df_train, NN_metric = 'cosine', NN_algo = 'brute'):\n",
    "       \n",
    "    # Set up numerical data inputs for NearestNeighbors function\n",
    "    df_cust_articles = df_train[chk_cols]\n",
    "    \n",
    "    # Set up parameters of NearestNeighbors function\n",
    "    model_knn = NearestNeighbors(metric = NN_metric, algorithm = NN_algo)\n",
    "    \n",
    "    # Implement NearestNeighbors function on merged Trxn/Customer/Article data elements\n",
    "    model_knn.fit(df_cust_articles)\n",
    "    \n",
    "    return model_knn, df_cust_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up function to  Nearest Neighbours (+ Articles bought) for a Customer <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a function to return a list of Nearest Neighbours for a given customer.\n",
    "# Return Neighbour Customer Ids and the articles they both\n",
    "def get_neighbourIDs_articles(model_knn, df_cust_articles, df_train, query_index, NN_Neighbours):\n",
    "     \n",
    "    # Temp view of training set - for validation\n",
    "    # train\n",
    "    \n",
    "    # Reset Training dataframe index - remove old one; this allows for extraction of correct index records\n",
    "    # returned by the NearestNeighbors function\n",
    "    # df_train.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    # Temp view of training set - for validation\n",
    "    # train\n",
    "    \n",
    "    # Set up numerical data inputs for NearestNeighbors function\n",
    "    # df_cust_articles = df_train[chk_cols]\n",
    "\n",
    "    \n",
    "    # Temp check on values - for verification\n",
    "    #df_train[['customer_id',\n",
    "    #  'article_id',\n",
    "    #  'product_code',\n",
    "    #  'product_type_no',\n",
    "    #  'graphical_appearance_no',\n",
    "    #  'colour_group_code',\n",
    "    #  'perceived_colour_value_id',\n",
    "    #  'perceived_colour_master_id',\n",
    "    #  'department_no',\n",
    "    #  'index_group_no',\n",
    "    #  'section_no',\n",
    "    #  'garment_group_no']]\n",
    "    #train[chk_cols]\n",
    "    \n",
    "    # Set up parameters of NearestNeighbors function\n",
    "    #model_knn = NearestNeighbors(metric = 'cosine', algorithm = 'brute')\n",
    "    #model_knn = NearestNeighbors(metric = NN_metric, algorithm = NN_algo)\n",
    "    \n",
    "    # Implement NearestNeighbors function on merged Trxn/Customer/Article data elements\n",
    "    #model_knn.fit(df_cust_articles)\n",
    "    \n",
    "    # Return index values for nearest neighbours - determined by 'Num_Neighbour' parameter\n",
    "    distances, indices = model_knn.kneighbors(df_cust_articles.iloc[query_index,:].values.reshape(1, -1), \n",
    "                                              n_neighbors = NN_Neighbours)\n",
    "    \n",
    "    # Temp selection of a chosen record for which to find Nearest Neighbours\n",
    "    #df_cust_articles.shape[0]\n",
    "    #df_cust_articles.iloc[query_index,:].values.reshape(1, -1)\n",
    "    #df_cust_articles.head()\n",
    "    \n",
    "    print('Query Index..')\n",
    "    print(query_index)\n",
    "    \n",
    "    print('Indices values... ')\n",
    "    print(indices)\n",
    "    \n",
    "    print('\\ndf_cust_articles.loc[query_index]\\n')\n",
    "    print(df_cust_articles.loc[query_index])\n",
    "    \n",
    "    print('\\n\\ndf_cust_articles.loc[query_index,[product_code,product_type_no,graphical_appearance_no]]\\n')\n",
    "    print(df_cust_articles.loc[query_index,['product_code','product_type_no','graphical_appearance_no']])\n",
    "    \n",
    "    print('\\n\\ntrain.loc[query_index,[product_code,product_type_no,graphical_appearance_no]]\\n')\n",
    "    print(df_train.loc[query_index,['product_code','product_type_no','graphical_appearance_no']])\n",
    "    \n",
    "    print('\\n\\ndf_cust_articles.index[query_index]\\n')\n",
    "    for i in range(0, len(distances.flatten())):\n",
    "        if i == 0:\n",
    "            print('\\nRecommendations for {0}:\\n'.format(df_cust_articles.index[query_index])) \n",
    "        else:\n",
    "            print('df_cust_articles: {0}: Index: {1} {2} with distance of {3}:'.\n",
    "              format(i,  \n",
    "                     df_cust_articles.index[indices.flatten()[i]], \n",
    "                     df_cust_articles.loc[indices.flatten()[i],['product_code']],\n",
    "                     distances.flatten()[i]))\n",
    "        \n",
    "    print('\\n\\ntrain.index[query_index]\\n')    \n",
    "    for i in range(0, len(distances.flatten())):\n",
    "        if i == 0:\n",
    "            print('\\nRecommendations for {0}:\\n'.format(df_train.index[query_index]))\n",
    "        else:\n",
    "            print('train: {0}: Index: {1} {2} {3} {4}, with distance of {5}:'\n",
    "              .format(i,  \n",
    "                      df_train.index[indices.flatten()[i]],\n",
    "                      df_train.loc[indices.flatten()[i],['customer_id']], \n",
    "                      df_train.loc[indices.flatten()[i],['article_id']], \n",
    "                      df_train.loc[indices.flatten()[i],['product_code']], \n",
    "                      distances.flatten()[i]))    \n",
    "        \n",
    "    customer_list = []\n",
    "    article_list  = []\n",
    "\n",
    "\n",
    "    # Using the Index values returned by the Nearest neighbour model\n",
    "    # Generate a list of Customer Ids and Article Ids for these neighbours\n",
    "    for i in range(0, len(distances.flatten())):\n",
    "        if i == 0:\n",
    "            print('\\nRecommendations for {0}:\\n'.format(df_train.index[query_index]))\n",
    "        else:\n",
    "            print('Indices[i]: {}'.format(indices.flatten()[i]))\n",
    "        \n",
    "            print('{0}: Index: {1} {2} {3} {4}, with distance of {5}:'\n",
    "              .format(i,  \n",
    "                      df_train.index[indices.flatten()[i]],\n",
    "                      df_train.loc[indices.flatten()[i],['customer_id']], \n",
    "                      df_train.loc[indices.flatten()[i],['article_id']], \n",
    "                      df_train.loc[indices.flatten()[i],['product_code']], \n",
    "                      distances.flatten()[i]))\n",
    "            customer_list.append(df_train.loc[indices.flatten()[i],'customer_id'])\n",
    "            article_list.append(df_train.loc[indices.flatten()[i],'article_id'])\n",
    "        \n",
    "    df_neighbourhood = pd.DataFrame({'customer_id': customer_list, 'article_id': article_list})\n",
    "    del customer_list, article_list\n",
    "    \n",
    "    return df_neighbourhood  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a function to return a list of Nearest Neighbours for a given customer.\n",
    "# Return Neighbour Customer Ids and the articles they both\n",
    "def get_CustSub_NeighbourIDs_articles(model_knn, df_cust_articles, df_train, NN_Neighbours):\n",
    "     \n",
    "    \n",
    "    # Return index values for nearest neighbours - determined by 'Num_Neighbour' parameter\n",
    "    distances, indices = model_knn.kneighbors(df_cust_articles.iloc[0].values.reshape(1, -1), \n",
    "                                              n_neighbors = NN_Neighbours)\n",
    "     \n",
    "        \n",
    "    customer_list = []\n",
    "    article_list  = []\n",
    "\n",
    "\n",
    "    # Using the Index values returned by the Nearest neighbour model\n",
    "    # Generate a list of Customer Ids and Article Ids for these neighbours\n",
    "    for i in range(0, len(distances.flatten())):\n",
    "        if i == 0:\n",
    "            print('\\nRecommendations for {0}:\\n'.format(df_train.index[query_index]))\n",
    "        else:\n",
    "            print('Indices[i]: {}'.format(indices.flatten()[i]))\n",
    "        \n",
    "            print('{0}: Index: {1} {2} {3} {4}, with distance of {5}:'\n",
    "              .format(i,  \n",
    "                      df_train.index[indices.flatten()[i]],\n",
    "                      df_train.loc[indices.flatten()[i],['customer_id']], \n",
    "                      df_train.loc[indices.flatten()[i],['article_id']], \n",
    "                      df_train.loc[indices.flatten()[i],['product_code']], \n",
    "                      distances.flatten()[i]))\n",
    "            customer_list.append(df_train.loc[indices.flatten()[i],'customer_id'])\n",
    "            article_list.append(df_train.loc[indices.flatten()[i],'article_id'])\n",
    "        \n",
    "    df_neighbourhood = pd.DataFrame({'customer_id': customer_list, 'article_id': article_list})\n",
    "    del customer_list, article_list\n",
    "    \n",
    "    return df_neighbourhood  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Top Selling Products in 'Neighbourhood' for a Customer <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return Top 12 Articles Purchased by the Customer 'Neighbourhood'\n",
    "def get_Top12_Articles_from_CustNNs(df, num_of_articles):\n",
    "    \n",
    "    print('get_Top12_Articles_from_CustNNs')\n",
    "    \n",
    "    # Finding number of times a particular item is purchased by customers in the neighbourhood\n",
    "    temp = df.groupby(['article_id'])['customer_id'].agg('count').reset_index() \n",
    "    temp.columns = ['article_id','count']\n",
    "    \n",
    "    # Sort the articles by the number of time purchased by the 'neighbourhood' customers\n",
    "    temp = temp.sort_values('count', ascending=False)\n",
    "    \n",
    "    # Temp Print out of Article Id List\n",
    "    print(temp)\n",
    "    \n",
    "    # Select the Top 12 - if there are duplicate just select the first entry\n",
    "    temp = temp.nlargest(num_of_articles,'count', keep='first')\n",
    "    \n",
    "    # Isolate the Article Ids\n",
    "    temp = temp['article_id']\n",
    "    \n",
    "    # Convert to dataframe and reset index\n",
    "    df_article_list = pd.DataFrame({'article_id': temp})\n",
    "    df_article_list = df_article_list.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # Return the list of Top 12 articles\n",
    "    return df_article_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke NN Functions <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Training dataframe index - remove old one; this allows for extraction of correct index records\n",
    "# returned by the NearestNeighbors function\n",
    "train.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to set up NN learner\n",
    "hm_model_knn, df_cust_articles_nn = setup_NN_model(train, NN_metric = 'cosine', NN_algo = 'brute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to find nearest neighbour Customer Ids and Article Ids\n",
    "# Pass in index, parameters for Nearest Neighbour function, and the number of neighbours to retrun\n",
    "query_index = 3\n",
    "num_of_neighbours = 5\n",
    "df_neighbours = get_neighbourIDs_articles(hm_model_knn, df_cust_articles_nn, train, query_index, num_of_neighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temp Check \n",
    "df_neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_neighbours.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to return list of the most popular Articles in terms of the \n",
    "# purchases made by the customers neighbourhood\n",
    "TopN = 12\n",
    "df_cust_preds = get_Top12_Articles_from_CustNNs(df_neighbours, TopN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cust_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cust_preds.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The solution file to have 1371980 prediction rows - use Sample Submission to construct  <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Sample Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Sample Submission File\n",
    "df_sub  = pd.read_csv('../input/sample_submission.csv',\n",
    "                            #usecols= ['customer_id'], \n",
    "                            dtype={'customer_id': 'string'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numCustomers = df_sub.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return Customer Ids from sample submission file in sequence\n",
    "def get_next_Customer(df_full_cust_sub, df_train, idx):\n",
    "    \n",
    "    print('Get Next Customer')\n",
    "    \n",
    "   \n",
    "    # Extract only the H&M Customers contained in the sample Transaction dataset \n",
    "    custIDs_in_Trxn_Sub_Sample  = set(df_trxns_sample[\"customer_id\"])\n",
    "    \n",
    "    # Create dataset that only contains H&M Customers from the sample Transaction dataset\n",
    "    #df_sub_customer = df_sub_sample[df_sub_sample[\"customer_id\"].isin(custIDs_in_Trxn_Sub_Sample)]\n",
    "    df_sub_customer = df_full_cust_sub[df_full_cust_sub[\"customer_id\"].isin(custIDs_in_Trxn_Sub_Sample)]\n",
    "    \n",
    "    # Temp code to build submission framework\n",
    "    df_sub_sample = df_sub_customer.sample(frac=0.001, replace=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Get Customer Id from Sample Sub\n",
    "    cust       = df_sub_sample.iloc[idx,0]\n",
    "    display(cust)\n",
    "    \n",
    "    # Find Index of Customer in Train dataframe - just for debugging\n",
    "    cust_loc     = df_train.loc[train['customer_id'] == cust]\n",
    "    display(cust_loc)\n",
    "    \n",
    "    # Find Index of Customer in Train dataframe\n",
    "    index        = df_train.index\n",
    "    condition    = df_train['customer_id'] == cust\n",
    "    cust_idx     = index[condition]\n",
    "    display(cust_idx)\n",
    "    num_cust_idx = cust_idx[0]\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    cust_idx = num_cust_idx \n",
    "    \n",
    "    # Return index of customer id in training dataframe and the customer id itself\n",
    "    return cust_idx, cust\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return Customer Ids from sample submission file in sequence\n",
    "def get_next_SubmissionCustomer(df_full_cust_sub, idx):\n",
    "    \n",
    "    print('get_next_SubmissionCustomer')\n",
    "    \n",
    "    # Temp code to build submission framework\n",
    "    df_sub_sample = df_full_cust_sub.sample(frac=0.001, replace=False)  \n",
    "    \n",
    "    # Get Customer Id from Sample Sub\n",
    "    cust_id       = df_sub_sample.iloc[idx,0]\n",
    "    display(cust_id)\n",
    "    \n",
    "    df_cust_id = df_sub_sample.head(idx+1)\n",
    "    df_cust_id = df_cust_id.drop(['prediction'], axis = 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Return index of customer id in training dataframe and the customer id itself\n",
    "    return df_cust_id, cust_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_cust, cust_id = get_next_SubmissionCustomer(df_sub, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_cust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate NN Search Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Customer Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Customer atributes from the Customer dataset based on a join with the 'customer_id' field\n",
    "df_SubCust_Trxn = pd.merge(df_one_cust, df_trxns, how=\"left\", on=[\"customer_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm dimensions of Trxn (sample) dataframe AFTER merging with Customer data\n",
    "df_SubCust_Trxn.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On rare occassion that a customer has no transactions - create a dummy records to avoid a failure\n",
    "if df_SubCust_Trxn.shape[0] == 0:\n",
    "    print('NO Transactions!')\n",
    "    # Create dummy dataframe\n",
    "    df_Dummy = pd.DataFrame([[cust_id,\n",
    "                '2016-10-25',\n",
    "                 651329001,\n",
    "                 0.021593,\n",
    "                 1]], columns=['customer_id','t_dat','article_id','price','sales_channel_id'])\n",
    "    # Stack the DataFrames on top of each other\n",
    "    df_SubCust_Trxn = pd.concat([df_SubCust_Trxn, df_Dummy], axis=0)\n",
    "    df_SubCust_Trxn.reset_index(drop=True, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SubCust_Trxn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SubCust_Trxn.sort_values(by=['t_dat'], ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Customer Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use last transaction\n",
    "df_SubCust_Trxn1 = df_SubCust_Trxn.sort_values(by=['t_dat'], ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SubCust_Trxn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Customer atributes from the Customer dataset based on a join with the 'customer_id' field\n",
    "df_SubCust_Trxn1 = pd.merge(df_SubCust_Trxn1, df_customers, how=\"left\", on=[\"customer_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SubCust_Trxn1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out other Customer specific attributes apart from; age + fashion_news_frequency + club_member_status + postal code\n",
    "# There are very many missing values in the other Customer dataset columns, so these will be ignored\n",
    "df_SubCust_Trxn1 = df_SubCust_Trxn1.iloc[:, [0, 1, 2, 3, 4, 7, 8, 9, 10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SubCust_Trxn1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Article Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Articles atributes from the Articles dataset based on a join with the 'article_id' field\n",
    "df_SubCust_Trxn1 = pd.merge(df_SubCust_Trxn1, df_Articles, how=\"left\", on=[\"article_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SubCust_Trxn1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'redundant' categotical columns from Articles (sample) dataframe\n",
    "# Consider a categorical column Redundant when is 'partnered' with a numerical code \n",
    "df_SubCust_Trxn1.drop([\"prod_name\", \n",
    "                       \"product_type_name\", \n",
    "                       \"graphical_appearance_name\",\n",
    "                       \"colour_group_name\",\n",
    "                       \"perceived_colour_value_name\",\n",
    "                       \"perceived_colour_master_name\",\n",
    "                       \"department_name\",\n",
    "                       \"index_name\",\n",
    "                       \"index_group_name\",\n",
    "                       \"section_name\",\n",
    "                       \"garment_group_name\",\n",
    "                       \"detail_desc\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SubCust_Trxn1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Categorical Values\n",
    "for column_name in article_cols_to_encode:\n",
    "    df_SubCust_Trxn1 = encode_Articles_CatFeatures(df_SubCust_Trxn1, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SubCust_Trxn1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the prior Categorical columns from the Articles dataframe\n",
    "# df_SubCust_Trxn1.drop([\"product_group_name\",\"index_code\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SubCust_Trxn1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up NN Vector for Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data_types in scale_dtypes:\n",
    "#    scale_trainset_features(df_SubCust_Trxn1, data_types, no_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolate NN Columns from Customer Submission Data. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CustNNVectors = df_SubCust_Trxn1[chk_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CustNNVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CustNNVectors.iloc[0].values.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Customer Article Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to find nearest neighbour Customer Ids and Article Ids\n",
    "# Pass in index, parameters for Nearest Neighbour function, and the number of neighbours to retrun\n",
    "num_of_neighbours = 100\n",
    "#df_neighbours = get_neighbourIDs_articles(hm_model_knn, df_cust_articles_nn, train, customer_submission_idx, num_of_neighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neighbours = get_CustSub_NeighbourIDs_articles(hm_model_knn, df_CustNNVectors, train, num_of_neighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to return list of the most popular Articles in terms of the \n",
    "# purchases made by the customers neighbourhood\n",
    "TopN = 12\n",
    "df_cust_preds = get_Top12_Articles_from_CustNNs(df_neighbours, TopN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cust_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Article Predictions for Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty Dataframe with column names only - will be filled with each Customer and their set of predictions\n",
    "df_SubmissionData = pd.DataFrame(columns=['customer_id', 'prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return Article Id Predictions in String format required for Kaggle competition\n",
    "def get_str_Article_Predictions(df):\n",
    "    \n",
    "    article_pred = df['article_id'].values.tolist()\n",
    "    #print(article_pred)\n",
    "    \n",
    "    article_pred = ['0' + str(article_id) for article_id in article_pred]\n",
    "    article_pred_str =  ' '.join(article_pred)\n",
    "    #print(article_pred_str)\n",
    "    \n",
    "    str_article_preds = article_pred_str\n",
    "    \n",
    "    return str_article_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Build up dataframe for submission\n",
    "def build_up_submission_df(cust, str_preds):\n",
    "    \n",
    "    str = cust + ' ' + str_preds\n",
    "    print(str)   \n",
    "    \n",
    "    # List1 \n",
    "    lst = [[cust, str_preds]]    \n",
    "    df = pd.DataFrame(lst, columns =['customer_id', 'prediction'])\n",
    "    final_df =df\n",
    "    \n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a string output of Customer Article Predicitons - format as required for Kaggle competition\n",
    "str_articles_predictions = get_str_Article_Predictions(df_cust_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Customer ID and list of Article predictions - add to dataframe to be used for CSV output\n",
    "df_NextCust_Preds = build_up_submission_df(cust_id, str_articles_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SubmissionData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NextCust_Preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the DataFrames on top of each other\n",
    "df_submission_stack = pd.concat([df_SubmissionData, df_NextCust_Preds], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Predictions Submission File to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SetUp Disk Directory for Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up folder for submission file with predictions\n",
    "submission_path = '../submission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder above project directory\n",
    "os.makedirs(submission_path, exist_ok=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Submission .CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission_stack = df_submission_stack.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write pre-test submission file to disk\n",
    "df_submission_stack.to_csv(submission_path+'/submission_pretest.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTempFinal  = pd.read_csv(submission_path+'/submission_pretest.csv')\n",
    "\n",
    "[print(\"Matching the number of dfSub rows to the correct submission size. \\nSubmission Row: {} v Expected Rows: {}\"\n",
    "                                                                                   .format(dfTempFinal.shape[0],numCustomers))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check format of submission file\n",
    "dfTempFinal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write final submission file to disk\n",
    "dfTempFinal.to_csv(submission_path+'/submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a class=\"tocSkip\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "659px",
    "left": "0px",
    "right": "1227.5px",
    "top": "87px",
    "width": "341.5px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
