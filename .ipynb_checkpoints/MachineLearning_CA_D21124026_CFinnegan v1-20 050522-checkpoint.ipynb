{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Continuous Assessment - Class Group: TU060 <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSc in Computer Science - Data Science (Part Time) <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Name: Ciaran Finnegan  <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student No: D21124026 <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student EMail: D21124026@mytudublin.ie <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## May 2022<a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H&M Personalised Fashion Recommendations <a class=\"tocSkip\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This project takes the datasets provided by Kaggle for the H&M Personalised Fashion Recommendations competition.  <a class=\"tocSkip\">\n",
    "\n",
    "The objective of the Kaggle competition is to produce a list of 12 predicted purchases for each client for the seven days after the end of the provided transaction file. Kaggle submission are verified against unseen test data.\n",
    "    \n",
    "To train a model, this project uses the last calendar week of transaction data (mid September) as the basis for the 'actual data' and trains on customer purchases in the previous weeks (Aug - September) across the available years in the transaction files.\n",
    "    \n",
    "The list of 'Actual' purchases made by a customer in the mid September test data are compared against model predictons generated from top 12 purchases made by the 'Nearest Neigbours'.\n",
    "    \n",
    "    \n",
    "In line with standard ML workflows the data is cleaned, augmented, and scaled in advance of the modelling process.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The CA follows a 7-part structure. <a class=\"tocSkip\">\n",
    "\n",
    "Section 1: Python Library Imports\n",
    "    \n",
    "Section 2: Data Analysis, Import and Preparation\n",
    "\n",
    "Section 3: Building of Training and Test Datasets\n",
    "\n",
    "Section 4: Scaling and splitting the data\n",
    "    \n",
    "Section 5: Training a KNN 'Nearest Neighbour' model to generate predictions\n",
    "    \n",
    "Section 6: Evaluation of the predictions generated by the trained model against Test Data\n",
    "    \n",
    "Section 7: Submission of a test file to the Kaggle competition   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Jupyter-Notebook-Set-Up\" data-toc-modified-id=\"Jupyter-Notebook-Set-Up-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Jupyter Notebook Set Up</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Import Libraries</a></span><ul class=\"toc-item\"><li><span><a href=\"#General-Utility-Libraries\" data-toc-modified-id=\"General-Utility-Libraries-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>General Utility Libraries</a></span></li><li><span><a href=\"#Panda-+-Numpy\" data-toc-modified-id=\"Panda-+-Numpy-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Panda + Numpy</a></span></li><li><span><a href=\"#Plotly-Libraries\" data-toc-modified-id=\"Plotly-Libraries-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Plotly Libraries</a></span></li></ul></li></ul></li><li><span><a href=\"#Project-Data-Import,-Analysis-and-Preparation\" data-toc-modified-id=\"Project-Data-Import,-Analysis-and-Preparation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Project Data Import, Analysis and Preparation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-and-Sample-Kaggle-H&amp;M-Datasets\" data-toc-modified-id=\"Load-and-Sample-Kaggle-H&amp;M-Datasets-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Load and Sample Kaggle H&amp;M Datasets</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-Up-File-Paths\" data-toc-modified-id=\"Set-Up-File-Paths-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Set Up File Paths</a></span></li><li><span><a href=\"#H&amp;M-Transaction-Dataset\" data-toc-modified-id=\"H&amp;M-Transaction-Dataset-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>H&amp;M Transaction Dataset</a></span></li><li><span><a href=\"#H&amp;M-Articles-Dataset\" data-toc-modified-id=\"H&amp;M-Articles-Dataset-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>H&amp;M Articles Dataset</a></span></li><li><span><a href=\"#H&amp;M-Customers-Dataset\" data-toc-modified-id=\"H&amp;M-Customers-Dataset-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>H&amp;M Customers Dataset</a></span></li></ul></li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#H&amp;M-Sampled-Transaction-Dataset\" data-toc-modified-id=\"H&amp;M-Sampled-Transaction-Dataset-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>H&amp;M Sampled Transaction Dataset</a></span></li><li><span><a href=\"#H&amp;M-Sampled-Customer-Dataset\" data-toc-modified-id=\"H&amp;M-Sampled-Customer-Dataset-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>H&amp;M Sampled Customer Dataset</a></span></li><li><span><a href=\"#H&amp;M-Sampled-Articles-Dataset\" data-toc-modified-id=\"H&amp;M-Sampled-Articles-Dataset-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>H&amp;M Sampled Articles Dataset</a></span></li></ul></li><li><span><a href=\"#Data-Preparation-and-Feature-Enrichment\" data-toc-modified-id=\"Data-Preparation-and-Feature-Enrichment-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Data Preparation and Feature Enrichment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Transaction-Data-Feature-Enrichment-(Sample)\" data-toc-modified-id=\"Transaction-Data-Feature-Enrichment-(Sample)-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Transaction Data Feature Enrichment (Sample)</a></span></li><li><span><a href=\"#Extend-Dataframe-with-Customer-Data\" data-toc-modified-id=\"Extend-Dataframe-with-Customer-Data-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Extend Dataframe with Customer Data</a></span></li><li><span><a href=\"#Extend-Dataframe-with-Articles-Data\" data-toc-modified-id=\"Extend-Dataframe-with-Articles-Data-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>Extend Dataframe with Articles Data</a></span></li><li><span><a href=\"#Final-Data-Preparation-Steps\" data-toc-modified-id=\"Final-Data-Preparation-Steps-2.3.4\"><span class=\"toc-item-num\">2.3.4&nbsp;&nbsp;</span>Final Data Preparation Steps</a></span></li></ul></li></ul></li><li><span><a href=\"#Mark-Data-:-Train-and-Test-Sets\" data-toc-modified-id=\"Mark-Data-:-Train-and-Test-Sets-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Mark Data : Train and Test Sets</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-Training-Set-Based-on-Date-Rane-(-2-Wk-to--4-Wk)\" data-toc-modified-id=\"Create-Training-Set-Based-on-Date-Rane-(-2-Wk-to--4-Wk)-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Create Training Set Based on Date Rane (-2 Wk to -4 Wk)</a></span></li><li><span><a href=\"#Create-Test-Set-Based-on-Date-Range-(-1-Wk)\" data-toc-modified-id=\"Create-Test-Set-Based-on-Date-Range-(-1-Wk)-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Create Test Set Based on Date Range (-1 Wk)</a></span></li><li><span><a href=\"#Verify-Date-Ranges-in-Train/Test-Sets\" data-toc-modified-id=\"Verify-Date-Ranges-in-Train/Test-Sets-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Verify Date Ranges in Train/Test Sets</a></span></li><li><span><a href=\"#Set-up-Train/Test-Sets\" data-toc-modified-id=\"Set-up-Train/Test-Sets-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Set up Train/Test Sets</a></span><ul class=\"toc-item\"><li><span><a href=\"#Mark-Date-Filtered-Dataframes\" data-toc-modified-id=\"Mark-Date-Filtered-Dataframes-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Mark Date Filtered Dataframes</a></span></li><li><span><a href=\"#Concatenate-to-Combine-Date-Filtered-Dataframes\" data-toc-modified-id=\"Concatenate-to-Combine-Date-Filtered-Dataframes-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>Concatenate to Combine Date Filtered Dataframes</a></span></li></ul></li><li><span><a href=\"#Visualisation-of-Train/Test-Sets-Size\" data-toc-modified-id=\"Visualisation-of-Train/Test-Sets-Size-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Visualisation of Train/Test Sets Size</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-Up-Temp-Dataframes-for-Bar-Chart-Displays\" data-toc-modified-id=\"Set-Up-Temp-Dataframes-for-Bar-Chart-Displays-3.5.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>Set Up Temp Dataframes for Bar Chart Displays</a></span></li><li><span><a href=\"#Display-Bar-Charts-with-Train/Test/Trxn-Breakdowns\" data-toc-modified-id=\"Display-Bar-Charts-with-Train/Test/Trxn-Breakdowns-3.5.2\"><span class=\"toc-item-num\">3.5.2&nbsp;&nbsp;</span>Display Bar Charts with Train/Test/Trxn Breakdowns</a></span></li></ul></li></ul></li><li><span><a href=\"#Scale-and-Split-Data:-Train-and-Test-Sets\" data-toc-modified-id=\"Scale-and-Split-Data:-Train-and-Test-Sets-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Scale and Split Data: Train and Test Sets</a></span><ul class=\"toc-item\"><li><span><a href=\"#Scale-Date-Filtered-Transactions\" data-toc-modified-id=\"Scale-Date-Filtered-Transactions-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Scale Date Filtered Transactions</a></span></li><li><span><a href=\"#Split-Date-Filtered-Transactions:-Train-and-Test-Data\" data-toc-modified-id=\"Split-Date-Filtered-Transactions:-Train-and-Test-Data-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Split Date Filtered Transactions: Train and Test Data</a></span></li></ul></li><li><span><a href=\"#K-Nearest-Neighbour-Operations\" data-toc-modified-id=\"K-Nearest-Neighbour-Operations-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>K Nearest Neighbour Operations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-Up-the-Columns-to-be-Input-to-KNN\" data-toc-modified-id=\"Set-Up-the-Columns-to-be-Input-to-KNN-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Set Up the Columns to be Input to KNN</a></span><ul class=\"toc-item\"><li><span><a href=\"#Column-Input-for-Local-KNN-Model-Training-/-Evaluation\" data-toc-modified-id=\"Column-Input-for-Local-KNN-Model-Training-/-Evaluation-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Column Input for Local KNN Model Training / Evaluation</a></span></li><li><span><a href=\"#Column-Input-for-Kaggle-KNN-Model-Training-/-Evaluation\" data-toc-modified-id=\"Column-Input-for-Kaggle-KNN-Model-Training-/-Evaluation-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Column Input for Kaggle KNN Model Training / Evaluation</a></span></li></ul></li><li><span><a href=\"#Create-Functions-for-NN-Analysis\" data-toc-modified-id=\"Create-Functions-for-NN-Analysis-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Create Functions for NN Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-up-function-to-Build-Nearest-Neighbours-Model\" data-toc-modified-id=\"Set-up-function-to-Build-Nearest-Neighbours-Model-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Set up function to Build Nearest Neighbours Model</a></span></li><li><span><a href=\"#Set-up-function-to--Nearest-Neighbours-(+-Articles-bought)-for-a-Customer\" data-toc-modified-id=\"Set-up-function-to--Nearest-Neighbours-(+-Articles-bought)-for-a-Customer-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>Set up function to  Nearest Neighbours (+ Articles bought) for a Customer</a></span></li><li><span><a href=\"#Find-Top-Selling-Products-in-'Neighbourhood'-for-a-Customer\" data-toc-modified-id=\"Find-Top-Selling-Products-in-'Neighbourhood'-for-a-Customer-5.2.3\"><span class=\"toc-item-num\">5.2.3&nbsp;&nbsp;</span>Find Top Selling Products in 'Neighbourhood' for a Customer</a></span></li><li><span><a href=\"#Invoke-Function-to-Build-KNN-Model\" data-toc-modified-id=\"Invoke-Function-to-Build-KNN-Model-5.2.4\"><span class=\"toc-item-num\">5.2.4&nbsp;&nbsp;</span>Invoke Function to Build KNN Model</a></span></li></ul></li></ul></li><li><span><a href=\"#Evaluate-Model\" data-toc-modified-id=\"Evaluate-Model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Evaluate Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-Functions-to-Extract-Customer-IDs-and-Article-Predictions-from-Test-Data\" data-toc-modified-id=\"Define-Functions-to-Extract-Customer-IDs-and-Article-Predictions-from-Test-Data-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Define Functions to Extract Customer IDs and Article Predictions from Test Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-Function-to-Obtain-Article-Predictions\" data-toc-modified-id=\"Define-Function-to-Obtain-Article-Predictions-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>Define Function to Obtain Article Predictions</a></span></li></ul></li><li><span><a href=\"#Define-Functions-to-Evaluate-Model-Predictions-aginst-Test-Data\" data-toc-modified-id=\"Define-Functions-to-Evaluate-Model-Predictions-aginst-Test-Data-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Define Functions to Evaluate Model Predictions aginst Test Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-up-function-to-return-latest-Customer-transactions-in-the-Test-Data\" data-toc-modified-id=\"Set-up-function-to-return-latest-Customer-transactions-in-the-Test-Data-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Set up function to return latest Customer transactions in the Test Data</a></span></li><li><span><a href=\"#Define-Function-to-Return-Evaluation-Score\" data-toc-modified-id=\"Define-Function-to-Return-Evaluation-Score-6.2.2\"><span class=\"toc-item-num\">6.2.2&nbsp;&nbsp;</span>Define Function to Return Evaluation Score</a></span></li><li><span><a href=\"#Iterate-Through-Test-Data-and-Generate-Predictions\" data-toc-modified-id=\"Iterate-Through-Test-Data-and-Generate-Predictions-6.2.3\"><span class=\"toc-item-num\">6.2.3&nbsp;&nbsp;</span>Iterate Through Test Data and Generate Predictions</a></span></li><li><span><a href=\"#Present-Accuracy-of-Predictions\" data-toc-modified-id=\"Present-Accuracy-of-Predictions-6.2.4\"><span class=\"toc-item-num\">6.2.4&nbsp;&nbsp;</span>Present Accuracy of Predictions</a></span></li></ul></li></ul></li><li><span><a href=\"#Build-Submission\" data-toc-modified-id=\"Build-Submission-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Build Submission</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-Sample-Submission-File\" data-toc-modified-id=\"Read-Sample-Submission-File-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Read Sample Submission File</a></span></li><li><span><a href=\"#Define-Functions-to-Parse-Article-Predictions-for-Kaggle-Submission\" data-toc-modified-id=\"Define-Functions-to-Parse-Article-Predictions-for-Kaggle-Submission-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Define Functions to Parse Article Predictions for Kaggle Submission</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-Function-to-read-Customer-IDs-from-Submission-File\" data-toc-modified-id=\"Define-Function-to-read-Customer-IDs-from-Submission-File-7.2.1\"><span class=\"toc-item-num\">7.2.1&nbsp;&nbsp;</span>Define Function to read Customer IDs from Submission File</a></span></li><li><span><a href=\"#Define-Function-to-Obtain-Customer-Transactions\" data-toc-modified-id=\"Define-Function-to-Obtain-Customer-Transactions-7.2.2\"><span class=\"toc-item-num\">7.2.2&nbsp;&nbsp;</span>Define Function to Obtain Customer Transactions</a></span></li><li><span><a href=\"#Define-Function-to-Obtain-Customer-Features\" data-toc-modified-id=\"Define-Function-to-Obtain-Customer-Features-7.2.3\"><span class=\"toc-item-num\">7.2.3&nbsp;&nbsp;</span>Define Function to Obtain Customer Features</a></span></li><li><span><a href=\"#Define-Function-to-Obtain-Article-Features\" data-toc-modified-id=\"Define-Function-to-Obtain-Article-Features-7.2.4\"><span class=\"toc-item-num\">7.2.4&nbsp;&nbsp;</span>Define Function to Obtain Article Features</a></span></li><li><span><a href=\"#Define-Functions-to-Parse-+-Build-Kaggle-Submission\" data-toc-modified-id=\"Define-Functions-to-Parse-+-Build-Kaggle-Submission-7.2.5\"><span class=\"toc-item-num\">7.2.5&nbsp;&nbsp;</span>Define Functions to Parse + Build Kaggle Submission</a></span></li></ul></li><li><span><a href=\"#Train-New-KNN-Model-for-Kaggle-Submission\" data-toc-modified-id=\"Train-New-KNN-Model-for-Kaggle-Submission-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Train New KNN Model for Kaggle Submission</a></span></li><li><span><a href=\"#Iterate-Through-Submission-File-to-Generate-Article-Predictions\" data-toc-modified-id=\"Iterate-Through-Submission-File-to-Generate-Article-Predictions-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Iterate Through Submission File to Generate Article Predictions</a></span></li><li><span><a href=\"#Write-Predictions-Submission-File-to-CSV\" data-toc-modified-id=\"Write-Predictions-Submission-File-to-CSV-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>Write Predictions Submission File to CSV</a></span><ul class=\"toc-item\"><li><span><a href=\"#SetUp-Disk-Directory-for-Submission-File\" data-toc-modified-id=\"SetUp-Disk-Directory-for-Submission-File-7.5.1\"><span class=\"toc-item-num\">7.5.1&nbsp;&nbsp;</span>SetUp Disk Directory for Submission File</a></span></li><li><span><a href=\"#Create-Submission-.CSV-File\" data-toc-modified-id=\"Create-Submission-.CSV-File-7.5.2\"><span class=\"toc-item-num\">7.5.2&nbsp;&nbsp;</span>Create Submission .CSV File</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebook Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: It is assumed that the matplotlib and plotly library has been installed on the user's machine before it is available for import.\n",
    "\n",
    "```bash\n",
    "> pip install plotly\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Utility Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Panda + Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python libraries for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up additional libraries for graphical presentations. Plotly graphs are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotly Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For interactive graphics and global displays\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Data Import, Analysis and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Sample Kaggle H&M Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The assignment starts with a Kaggle H&M Personalised Fashion Recommendations datasets. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up File Paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a Global path file variable to use when loading each dataset\n",
    "Path_To_Inputs = '../input'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H&M Transaction Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up display of file load - useful when file converted and run in .py format\n",
    "print('Running...  pd.read_csv(os.path.join(Path_To_Inputs, - transactions_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load H&M Transaction dataset\n",
    "df_trxns = pd.read_csv(os.path.join(Path_To_Inputs,'transactions_train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Number of Rows, Number of Columns in Transaction dataset prior to Sampling\n",
    "df_trxns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract random sample of Transactions (without replacement) - 1% of records\n",
    "sample_fraction = 0.01\n",
    "df_trxns_sample = df_trxns.sample(frac=sample_fraction, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up display of sample fraction from trxn dataset - useful when file converted and run in .py format\n",
    "[print('Extract {}% of records from Trxn file'.format(sample_fraction * 100))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Number of Rows, Number of Columns in Transaction dataset after Sampling\n",
    "df_trxns_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H&M Articles Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up display of file load - useful when file converted and run in .py format\n",
    "print('Running...  pd.read_csv(os.path.join(Path_To_Inputs, - articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file, located in 'input' folder above this notebook, for articles data - containing meta data on purchasable items.\n",
    "df_Articles = pd.read_csv(os.path.join(Path_To_Inputs,'articles.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Number of Rows, Number of Columns in Articles dataset\n",
    "df_Articles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the H&M articles contained in the sample Transaction dataset \n",
    "articleIDs_in_Trxn_Sample  = set(df_trxns_sample[\"article_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset that only contains H&M articles in the sample Transaction dataset\n",
    "df_articles_sample = df_Articles[df_Articles[\"article_id\"].isin(articleIDs_in_Trxn_Sample)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Number of Rows, Number of Columns in Sample Articles dataset\n",
    "df_articles_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H&M Customers Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up display of file load - useful when file converted and run in .py format\n",
    "print('Running...  pd.read_csv(os.path.join(Path_To_Inputs, - customers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load H&M Customer dataset\n",
    "df_customers = pd.read_csv(os.path.join(Path_To_Inputs,'customers.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Number of Rows, Number of Columns in Customers dataset prior to Sampling\n",
    "df_customers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the H&M Customers contained in the sample Transaction dataset \n",
    "custIDs_in_Trxn_Sample  = set(df_trxns_sample[\"customer_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Customer Ids in Transaction Sample\n",
    "len(custIDs_in_Trxn_Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset that only contains H&M Customers from the sample Transaction dataset\n",
    "df_customers_sample = df_customers[df_customers[\"customer_id\"].isin(custIDs_in_Trxn_Sample)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Number of Rows, Number of Columns in Sample Customers dataset\n",
    "df_customers_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display key data attributes and distributions in Sampled H&M datasets. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H&M Sampled Transaction Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic column information for the initial Kaggle 'transaction_train' datatset - after Sampling of original dataset\n",
    "df_trxns_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display datatype of colums in original H&M Transaction dataset\n",
    "df_trxns_sample.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H&M Sampled Customer Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic column information for the initial Kaggle 'customers' datatset - after Sampling of original dataset\n",
    "df_customers_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column datatypes in sampled Customer dataframe\n",
    "df_customers_sample.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H&M Sampled Articles Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic column information for the initial Kaggle 'article' datatset - after Sampling of original dataset\n",
    "df_articles_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column datatypes in sampled Articles dataframe\n",
    "df_articles_sample.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and Feature Enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a dataset upon which to build ML models. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin with Transaction Dataset (sampled) and merge in customer and article data. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transaction Data Feature Enrichment (Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataset for modelling. Initial basis for new dataset is the sampled transaction dataset.\n",
    "df_HM_CustTrxn = df_trxns_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose Datetime information \n",
    "df_HM_CustTrxn['t_dat'] = pd.to_datetime(df_HM_CustTrxn['t_dat'], format=\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "# The decomposition of the timestamp is used to filter trxn data into Train and Test datsets\n",
    "# Add Column for Transaction Year and convert to integer value\n",
    "df_HM_CustTrxn['txn_year'] = df_HM_CustTrxn['t_dat'].dt.strftime('%Y')\n",
    "df_HM_CustTrxn['txn_year'] = df_HM_CustTrxn['txn_year'].astype(str).astype(int)\n",
    "\n",
    "# Add Column for Transaction Month and convert to integer value\n",
    "df_HM_CustTrxn['txn_mth'] = df_HM_CustTrxn['t_dat'].dt.strftime('%m')\n",
    "df_HM_CustTrxn['txn_mth'] = df_HM_CustTrxn['txn_mth'].astype(str).astype(int)\n",
    "\n",
    "# Add Column for Transaction Day and convert to integer value\n",
    "df_HM_CustTrxn['txn_day'] = df_HM_CustTrxn['t_dat'].dt.strftime('%d')\n",
    "df_HM_CustTrxn['txn_day'] = df_HM_CustTrxn['txn_day'].astype(str).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show breakout of timestamp data\n",
    "df_HM_CustTrxn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in the Transaction Dataframe\n",
    "[print('Any Null Values in the Trxn (sample) dataframe?','\\n' ,df_HM_CustTrxn.isnull().any())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Dataframe with Customer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm dimensions of Trxn (sample) dataframe BEFORE merging with Customer data\n",
    "df_HM_CustTrxn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Customer Attributes into the Trxn (Sample) Dataframe. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Customer atributes from the Customer dataset based on a join with the 'customer_id' field\n",
    "df_HM_CustTrxn = pd.merge(df_HM_CustTrxn, df_customers_sample, how=\"left\", on=[\"customer_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm dimensions of Trxn (sample) dataframe AFTER merging with Customer data\n",
    "df_HM_CustTrxn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_CustTrxn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out other Customer specific attributes apart from; age + fashion_news_frequency + club_member_status + postal code\n",
    "# There are very many missing values in the other Customer dataset columns, so these will be ignored\n",
    "#df_HM_CustTrxn = df_HM_CustTrxn.iloc[:, [0, 1, 2, 3, 4, 5, 6, 9, 10, 11, 12]]\n",
    "df_HM_CustTrxn.drop([\"FN\", \"Active\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_CustTrxn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_CustTrxn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in the merged Trxn/Customer dataframe\n",
    "[print('Any Null Values in the merged Trxn/Customer dataframe?','\\n' ,df_HM_CustTrxn.isnull().any())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Rows with Missing Club Member Status Attributes. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in the club_member_status column  \n",
    "df_null_club_member_status = df_HM_CustTrxn.loc[(df_HM_CustTrxn[\"club_member_status\"].isnull())]\n",
    "\n",
    "[print('Numbers of rows with missing club member status?','\\n' ,len(df_null_club_member_status.index))]\n",
    "[print('\\nPrecentage of total rows in Trxn dataframe with missing club member status attributes?\\n{:06.3f}%'\n",
    "                               .format((len(df_null_club_member_status.index)/len(df_HM_CustTrxn.index))*100))]\n",
    "\n",
    "\n",
    "# For simplicity the rows with missing values in the club member status will be ignored\n",
    "# Re-Build Txn/Customer Dataframe with no missing values in the club member status\n",
    "df_HM_CustTrxn = df_HM_CustTrxn.loc[(df_HM_CustTrxn[\"club_member_status\"].notnull())]\n",
    "\n",
    "# Confirm dimensions of Trxn (sample) dataframe AFTER removing rows with missing values in the club member status column\n",
    "[print('\\nCurrent Number of Rows/Columns in Trxn/Customer dataframe - with missing club mem status values removed','\\n' ,\n",
    "                                                                                                   df_HM_CustTrxn.shape)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Rows with Missing Fashion News Frequency Attributes. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in the club_member_status column  \n",
    "df_null_fn_freq = df_HM_CustTrxn.loc[(df_HM_CustTrxn[\"fashion_news_frequency\"].isnull())]\n",
    "\n",
    "[print('Numbers of rows with missing fashion_news_frequency values?','\\n' ,len(df_null_fn_freq.index))]\n",
    "[print('\\nPrecentage of total rows in Trxn dataframe with missing fashion_news_frequency attributes?\\n{:06.3f}%'\n",
    "                               .format((len(df_null_fn_freq.index)/len(df_HM_CustTrxn.index))*100))]\n",
    "\n",
    "# For simplicity the rows with missing values in the fashion_news_frequency column will be ignored\n",
    "# Re-Build Txn/Customer Dataframe with no missing values in the fashion_news_frequency column\n",
    "df_HM_CustTrxn = df_HM_CustTrxn.loc[(df_HM_CustTrxn[\"fashion_news_frequency\"].notnull())]\n",
    "\n",
    "# Confirm dimensions of Trxn (sample) dataframe AFTER removing rows with missing values in the fashion_news_frequency column\n",
    "[print('\\nCurrent Number of Rows/Columns in Trxn/Customer dataframe - missing Fash News Freq values removed','\\n' ,\n",
    "                                                                                                   df_HM_CustTrxn.shape)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Rows with Missing Age Attributes. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in the age column  \n",
    "df_null_age = df_HM_CustTrxn.loc[(df_HM_CustTrxn[\"age\"].isnull())]\n",
    "\n",
    "[print('Numbers of rows with missing age values?','\\n' ,len(df_null_age.index))]\n",
    "[print('\\nPrecentage of total rows in Trxn dataframe with missing age attributes?\\n{:06.3f}%'\n",
    "                               .format((len(df_null_age.index)/len(df_HM_CustTrxn.index))*100))]\n",
    "\n",
    "# Re-Build Txn/Customer Dataframe with no missing values in the age column\n",
    "df_HM_CustTrxn = df_HM_CustTrxn.loc[(df_HM_CustTrxn[\"age\"].notnull())]\n",
    "\n",
    "\n",
    "# Confirm dimensions of Trxn (sample) dataframe AFTER removing rows with missing values in the age column\n",
    "[print('\\nCurrent Number of Rows/Columns in Trxn/Customer dataframe - missing ages removed','\\n' ,df_HM_CustTrxn.shape)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the club_member_status + fashion_news_frequency attributes <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_CustTrxn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_Categor_Features(df, col_name):\n",
    "       \n",
    "    # The categorical values in dataframe must be encoded to numberical values\n",
    "    df_encoded_cols = pd.get_dummies(df[col_name])\n",
    "    df_encoded      = df.join(df_encoded_cols)\n",
    "    \n",
    "    # Remove the prior Categorical column from the Articles dataframe\n",
    "    df_encoded.drop([col_name], axis=1, inplace=True)\n",
    "    \n",
    "    # Return the dataframe with encoded columsn added\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_cols_to_encode = [\"club_member_status\",\"fashion_news_frequency\"]\n",
    "for column_name in cust_cols_to_encode:\n",
    "    df_HM_CustTrxn = encode_Categor_Features(df_HM_CustTrxn, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Postal Code - a non numerical value that is not easily converted\n",
    "df_HM_CustTrxn.drop([\"postal_code\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic column information for the initial Kaggle 'article' datatset - after Sampling of original dataset\n",
    "df_HM_CustTrxn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm dimensions of Trxn (sample) dataframe AFTER encoding Categorical Values\n",
    "[print('\\nCurrent Number of Rows/Columns in Trxn/Customer dataframe - after encoding of categorical values completed','\\n' ,\n",
    "                                                                                                   df_HM_CustTrxn.shape)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in the Transaction Dataframe\n",
    "[print('Any Null Values in the merged Trxn/Customer (sample) dataframe?','\\n' ,df_HM_CustTrxn.isnull().any())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Dataframe with Articles Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove 'Redundant' Categorical Columns in Articles (sample) Dataframe. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic column information for the initial Kaggle 'article' datatset - after Sampling of original dataset\n",
    "df_articles_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'redundant' categotical columns from Articles (sample) dataframe\n",
    "# Consider a categorical column redundant when is 'partnered' with a numerical code \n",
    "\n",
    "redundo_catcols_to_drop =  [\"prod_name\", \n",
    "                            \"product_type_name\", \n",
    "                            \"graphical_appearance_name\",\n",
    "                            \"colour_group_name\",\n",
    "                            \"perceived_colour_value_name\",\n",
    "                            \"perceived_colour_master_name\",\n",
    "                            \"department_name\",\n",
    "                            \"index_name\",\n",
    "                            \"index_group_name\",\n",
    "                            \"section_name\",\n",
    "                            \"garment_group_name\",\n",
    "                            \"detail_desc\"]\n",
    "\n",
    "\n",
    "df_articles_sample.drop(redundo_catcols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic column information for the initial Kaggle 'article' datatset - after Sampling of original dataset\n",
    "df_articles_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use One-Hot-Encoding on remaining categorical columns. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_cols_to_encode =['product_group_name', 'index_code']\n",
    "for column_name in article_cols_to_encode:\n",
    "    df_articles_sample = encode_Categor_Features(df_articles_sample, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic column information for sampled articles dataframe after encoding\n",
    "df_articles_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Trxn/Customers Dataframe with Articles Dataframe (sample). <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Articles atributes from the Articles dataset based on a join with the 'article_id' field\n",
    "df_HM_CustTrxnArtcls = pd.merge(df_HM_CustTrxn, df_articles_sample, how=\"left\", on=[\"article_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm dimensions of Trxn (sample) dataframe AFTER merging with Articles data\n",
    "df_HM_CustTrxnArtcls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic column information for the initial Kaggle 'transaction_train' datatset - AFTER merging with Articles data\n",
    "df_HM_CustTrxnArtcls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in the merged Trxn/Customer/Articles dataframe\n",
    "[print('Any Null Values in the merged Trxn/Customer/Articles dataframe?','\\n' ,df_HM_CustTrxnArtcls.isnull().any())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Data Preparation Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Any Duplicates in Final Merged Dataframe. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a check to see if any duplicate entires exist in the merged Trxn/Customer/Articles dataframe\n",
    "[print('Numbers of duplicated rows in the final merged Trxn/Customer/Articles dataframe?','\\n' \n",
    "                                                                       ,df_HM_CustTrxnArtcls.duplicated().sum())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows in the merged Trxn/Customer/Articles dataframe\n",
    "df_HM_CustTrxnArtcls = df_HM_CustTrxnArtcls.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-check: Find duplicate rows across all columns\n",
    "duplicateRows = df_HM_CustTrxnArtcls[df_HM_CustTrxnArtcls.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will advise if duplicates are successfully removed.\n",
    "if duplicateRows.shape[0] < 1:\n",
    "    [print('Success! Duplicate Rows Removed from merged Trxn/Customer/Articles dataframe')]\n",
    "else:\n",
    "    [print('Warning! Duplicate Rows Remain in merged Trxn/Customer/Articles dataframe')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm dimensions of Trxn/Customer/Articles (sample) dataframe AFTER removing duplicate rows\n",
    "[print('\\nCurrent Number of Rows/Columns in merged Trxn/Customer/Articles dataframe - duplicates removed','\\n' ,\n",
    "                                                                                           df_HM_CustTrxnArtcls.shape)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm column datatypes of Trxn/Customer/Articles (sample) dataframe - useful references view\n",
    "[print('\\nDisplay Column Datatypes in merged Trxn/Customer/Articles dataframe;','\\n' ,\n",
    "                                                                                           df_HM_CustTrxnArtcls.dtypes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mark Data : Train and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Set Based on Date Rane (-2 Wk to -4 Wk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transaction data runs up to 20th Sep 2020.  <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given that the objective is to predict purchases for a following week, the Test data is taken from the last week of transactions for each year, and the Training dats is based on the previous three weeks for each year.  <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data is split based on the August 23rd to September 14th period for each year in Transaction Data.  <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows from the merged Transaction/Customer/Articles dataframe for the Training Data period - all years\n",
    "df_train_Wk321 = df_HM_CustTrxnArtcls.loc[  (\n",
    "                                               (df_HM_CustTrxnArtcls['txn_mth'] == 9)  \n",
    "                                             & (df_HM_CustTrxnArtcls['txn_day'] <= 14) # September 1 - 14\n",
    "                                             \n",
    "                                             )\n",
    "                                          |\n",
    "                                            (\n",
    "                                                (df_HM_CustTrxnArtcls['txn_mth'] == 8)\n",
    "                                              & (df_HM_CustTrxnArtcls['txn_day'] >= 23) # August 23 - 31\n",
    "                                             )\n",
    "                                          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Test Set Based on Date Range (-1 Wk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Data is split based on the last week in Transaction Data (Sept 15th - 22nd) for each year.  <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows from the merged Transaction/Customer/Articles dataframe for the Test Data period - all years\n",
    "df_test_LastWk = df_HM_CustTrxnArtcls.loc[  (df_HM_CustTrxnArtcls['txn_mth'] == 9)\n",
    "                                   & (df_HM_CustTrxnArtcls['txn_day'] >= 15)\n",
    "                                   & (df_HM_CustTrxnArtcls['txn_day'] <= 22)] # September 15 - 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Date Ranges in Train/Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a temp dataframe to display check the date ranges extracted for Training dataset\n",
    "df_chkTraindf = pd.Series(df_train_Wk321['t_dat'].unique()).sort_values(ascending=False).head(200)\n",
    "\n",
    "df_chkTraindf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a temp dataframe to display check the date ranges extracted for Test dataset\n",
    "df_chkTestdf = pd.Series(df_test_LastWk['t_dat'].unique()).sort_values(ascending=False).head(100)\n",
    "\n",
    "df_chkTestdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Train/Test Sets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mark Dataframe as 'Train' and 'Test', then Concatenate. This will allow for later Scaling and Splitting  <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mark Date Filtered Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'marker' to time filtered data extracted from merged trxn dataframe to make Training dataset\n",
    "df_train_Wk321['Split'] = 'Train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'marker' to time filtered data extracted from merged trxn dataframe to make Test dataset\n",
    "df_test_LastWk['Split'] = 'Test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate to Combine Date Filtered Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show concatenated dataframe\n",
    "df_DateFilteted_Train_and_Test_Data = pd.concat([df_train_Wk321, df_test_LastWk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Train and Test Concatenated Dataframe - Note 'Train' at head, and 'Test' at tail.\n",
    "df_DateFilteted_Train_and_Test_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of Train/Test Sets Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up Temp Dataframes for Bar Chart Displays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to alter columns in temp dataframe to improve Plotly Bar Chart display\n",
    "def setup_BarChartCols_forDisplay(df):\n",
    "    \n",
    "    # Rename 'txn_year' column to 'Year' for Bar Chart  display\n",
    "    df.rename(columns = {'txn_year':'Year'}, inplace = True)\n",
    "\n",
    "    # Convert 'Year' column to str for Bar Chart legend display\n",
    "    df['Year'] = df['Year'].astype(str)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up temp dataframe to group Train and Test transaction by size\n",
    "df_barchart1 = df_DateFilteted_Train_and_Test_Data.groupby([\"Split\", \"txn_year\"]).count().reset_index()\n",
    "\n",
    "# Call Function to change column name and datatype to improve display\n",
    "df_barchart1 = setup_BarChartCols_forDisplay(df_barchart1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up temp dataframe to group full transaction dataset by size per year\n",
    "df_barchart2 = df_HM_CustTrxnArtcls.groupby([\"txn_year\"]).count().reset_index()\n",
    "\n",
    "# Call Function to change column name and datatype to improve display\n",
    "df_barchart2 = setup_BarChartCols_forDisplay(df_barchart2)\n",
    "\n",
    "# Add 'marker' to merged trxn dataframe to improve Bar Chart display\n",
    "df_barchart2['Split'] = 'Original_Transactions'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Bar Charts with Train/Test/Trxn Breakdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reusable function to set up and refine Plotly Bar Chart display\n",
    "def display_HMBarChart(df1, df2, col_lst, barchartTitle_Text, X_Axis_Text):\n",
    "    \n",
    "    figTrainTest = px.bar(df1,\n",
    "             x=\"Split\",     \n",
    "             y=df2.groupby(col_lst).size(),\n",
    "             labels={\"y\": \"Number of Trxns\"},\n",
    "             color=df1['Year'],\n",
    "             color_discrete_map={ # replaces default color mapping by value\n",
    "                        \"2020\": \"indianred\", \n",
    "                        \"2019\": \"goldenrod\", \n",
    "                        \"2018\": \"teal\"\n",
    "                        },\n",
    "             template=\"simple_white\"  # Use a clearer template\n",
    "                     )\n",
    "\n",
    "\n",
    "    figTrainTest.update_yaxes( # the y-axis \n",
    "        title=dict(\n",
    "            font_size=20\n",
    "            ), \n",
    "        title_text='Transaction Volume',\n",
    "        showgrid=True\n",
    "    )\n",
    "\n",
    "    figTrainTest.update_xaxes( # the x-axis \n",
    "        title=dict(\n",
    "            font_size=20\n",
    "            ),\n",
    "        ticks=\"outside\",\n",
    "        title_text=X_Axis_Text,\n",
    "        showgrid=True\n",
    "    )\n",
    "    \n",
    "    figTrainTest.update_layout( # customize font and legend orientation & position\n",
    "    font_family=\"Rockwell\",\n",
    "    height=700,\n",
    "    title=dict(\n",
    "        text=barchartTitle_Text,\n",
    "        font_size=20\n",
    "        ),\n",
    "    legend=dict(\n",
    "         title='Trxn Years  - Click To Zoom',\n",
    "         traceorder=\"reversed\", # Vertical Legend matches the data order in the bars\n",
    "         itemclick=\"toggleothers\", # Easily highlight one regime\n",
    "         orientation=\"v\", \n",
    "         y=1.25, \n",
    "         yanchor=\"top\", \n",
    "         x=1, \n",
    "         xanchor=\"right\",\n",
    "         bgcolor=\"ghostwhite\", # Create a standout legend\n",
    "         bordercolor=\"Black\",\n",
    "         borderwidth=2\n",
    "        ),\n",
    "    margin=dict(\n",
    "          l=10,\n",
    "          r=60,\n",
    "          b=10,\n",
    "          t=150\n",
    "        )\n",
    "     )\n",
    "\n",
    "\n",
    "    # Display Bar Chart\n",
    "    figTrainTest.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to display Bar Chart for Train and Test Size Breakdown per Year\n",
    "col_list1 = [\"Split\", \"txn_year\"]\n",
    "barchart_Text=\"Split of Data Across Test and Train Dataset by Volume\"\n",
    "xAxis_txt='Train and Test Dataset Split'\n",
    "display_HMBarChart(df_barchart1, df_DateFilteted_Train_and_Test_Data, col_list1, barchart_Text, xAxis_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to display Bar Chart for original Trxn Size Breakdown per Year\n",
    "col_list2 = [\"txn_year\"]\n",
    "barchart_Text2=\"Total Volume of Transactions Sampled from Kaggle CSV\"\n",
    "xAxis_txt='Original Volume of Transactions'\n",
    "display_HMBarChart(df_barchart2, df_HM_CustTrxnArtcls, col_list2, barchart_Text2, xAxis_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale and Split Data: Train and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run scaling routines on numerical data before splitting into Train/Test. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Date Filtered Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing numerical features so that each feature has a value between 0 and 1\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to scale numerical datatypes within a dataframe based on data type value\n",
    "def scale_trainset_features(df, dtype, no_scale):\n",
    "    for x in df.columns[df.dtypes == dtype]:\n",
    "        if x != no_scale: # Allow for one particular column to avoid scalaing\n",
    "            df[x] = scaler.fit_transform(np.array(df[x]).reshape(-1,1))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_scale = 'article_id' # Allow for one particular colum to avoid scalaing - Article Id in this case\n",
    "\n",
    "# Set up data types in dataframe to scale\n",
    "scale_dtypes = ['int64','int32','float64']\n",
    "\n",
    "# Call function to scale the numerical values in the datafeame created from transactions/customer/article data\n",
    "for data_types in scale_dtypes:\n",
    "    scale_trainset_features(df_DateFilteted_Train_and_Test_Data, data_types, no_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Date Filtered Transactions: Train and Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the Scaled Data - which is the date filtered trxn data - into Train/Test Datasets. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_DateFilteted_Train_and_Test_Data.loc[(df_DateFilteted_Train_and_Test_Data['Split'] == 'Train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_DateFilteted_Train_and_Test_Data.loc[(df_DateFilteted_Train_and_Test_Data['Split'] == 'Test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Confirmation of data split\n",
    "dataDescription = 'H&M Sample Data Split'\n",
    "print(\"\\n{} Training Set Shape : \\n\\t\".format(dataDescription))\n",
    "print(train.shape)\n",
    "print(\"\\n{} Test Set Shape : \\n\\t\".format(dataDescription))\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbour Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up the Columns to be Input to KNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column Input for Local KNN Model Training / Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These dataset columns are used for the local training and evaluation of the KNN Model in this project.  <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the subset of numerical columns used in the KNN model building for Nearest Neighbours\n",
    "nn_cols =    ['price',\n",
    "              'age',\n",
    "              'product_code',\n",
    "              'product_type_no',\n",
    "              'graphical_appearance_no',\n",
    "              'colour_group_code',\n",
    "              'perceived_colour_value_id',\n",
    "              'perceived_colour_master_id',\n",
    "              'department_no',\n",
    "              'index_group_no',\n",
    "              'section_no',\n",
    "              'garment_group_no',\n",
    "              'Accessories',\n",
    "              'Bags',\n",
    "              'Cosmetic',\n",
    "              'Furniture',\n",
    "              'Garment Full body',\n",
    "              'Garment Lower body',\n",
    "              'Garment Upper body',\n",
    "              'Garment and Shoe care',\n",
    "              'Items',\n",
    "              'Nightwear',\n",
    "              'Shoes',\n",
    "              'Socks & Tights',\n",
    "              'Stationery',\n",
    "              'Swimwear',\n",
    "              'Underwear',\n",
    "              'Underwear/nightwear',\n",
    "              'Unknown',\n",
    "              'A',\n",
    "              'B',\n",
    "              'C',\n",
    "              'D',\n",
    "              'F',\n",
    "              'G',\n",
    "              'H',\n",
    "              'I',\n",
    "              'J',\n",
    "              'S']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column Input for Kaggle KNN Model Training / Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This reduced list of dataset columns are used for the training of the KNN Model for a Kaggle Submission. This is done to reduce processing time for the large Kaggle Submission file, and to ultimately compare the external Kaggle evalations against the local test results (with a larger set of input colums to the model training). <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the subset of numerical columns used in the KNN model building for Nearest Neighbours for Kaggle Submission\n",
    "nn_Kaggle_cols =    ['price',\n",
    "                     'sales_channel_id',\n",
    "                     'age',\n",
    "                     'product_code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Functions for NN Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up function to Build Nearest Neighbours Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Unsupervised learner for implementing neighbour searches of Customer Transactions\n",
    "# Set default parameters for NN algorithm - these can be overwritten during optimisationphase\n",
    "def setup_NN_model(df_train, NN_metric = 'cosine', NN_algo = 'brute', submission_Type = 'local'):\n",
    "       \n",
    "    \n",
    "    if (submission_Type == 'local'):\n",
    "        # Set up numerical data inputs for NearestNeighbors function for local training and evaluation\n",
    "        #print('df_cust_articles = df_train[nn_cols]')\n",
    "        df_cust_articles = df_train[nn_cols]\n",
    "    else:\n",
    "        # Set up numerical data inputs for NearestNeighbors function for Kaggle training and evaluation\n",
    "        #print('df_cust_articles = df_train[nn_Kaggle_cols]')\n",
    "        df_cust_articles = df_train[nn_Kaggle_cols]\n",
    "    \n",
    "    \n",
    "    # Set up parameters of NearestNeighbors function\n",
    "    model_knn = NearestNeighbors(metric = NN_metric, algorithm = NN_algo)\n",
    "    \n",
    "    # Implement NearestNeighbors function on merged Trxn/Customer/Article data elements\n",
    "    model_knn.fit(df_cust_articles)\n",
    "    \n",
    "    return model_knn, df_cust_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up function to  Nearest Neighbours (+ Articles bought) for a Customer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a function to return a list of Nearest Neighbours for a given customer.\n",
    "# Return Neighbour Customer Ids and the articles they both\n",
    "def get_CustSub_NeighbourIDs_articles(model_knn, df_cust_articles, df_train, NN_Neighbours):\n",
    "     \n",
    "    \n",
    "    # Return index values for nearest neighbours - determined by 'Num_Neighbour' parameter\n",
    "    distances, indices = model_knn.kneighbors(df_cust_articles.iloc[0].values.reshape(1, -1), \n",
    "                                              n_neighbors = NN_Neighbours)\n",
    "    \n",
    "    # Using the Index values returned by the Nearest neighbour model\n",
    "    # Generate a list of Customer Ids and Article Ids for these neighbours\n",
    "    for i in range(0, len(distances.flatten())):\n",
    "        if i == 0:\n",
    "            customer_list = []\n",
    "            article_list  = []\n",
    "        else:\n",
    "            customer_list.append(df_train.loc[indices.flatten()[i],'customer_id'])\n",
    "            article_list.append(df_train.loc[indices.flatten()[i],'article_id'])\n",
    "        \n",
    "    df_neighbourhood = pd.DataFrame({'customer_id': customer_list, 'article_id': article_list})\n",
    "    del customer_list, article_list\n",
    "    \n",
    "    return df_neighbourhood  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Top Selling Products in 'Neighbourhood' for a Customer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return Top 12 Articles Purchased by the Customer 'Neighbourhood' Or Actual Data on Purchases\n",
    "def get_Top12_Articles_for_Cust(df, num_of_articles):\n",
    "      \n",
    "    # Finding number of times a particular item is purchased by customers in the neighbourhood\n",
    "    temp = df.groupby(['article_id'])['customer_id'].agg('count').reset_index() \n",
    "    temp.columns = ['article_id','count']\n",
    "    \n",
    "    # Sort the articles by the number of time purchased by the 'neighbourhood' customers\n",
    "    temp = temp.sort_values('count', ascending=False)\n",
    "    \n",
    "    # Select the Top 12 - if there are duplicate just select the first entry\n",
    "    temp = temp.nlargest(num_of_articles,'count', keep='first')\n",
    "    \n",
    "    # Isolate the Article Ids\n",
    "    temp = temp['article_id']\n",
    "    \n",
    "    # Convert to dataframe and reset index\n",
    "    df_article_list = pd.DataFrame({'article_id': temp})\n",
    "    df_article_list = df_article_list.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # Return the list of Top 12 articles\n",
    "    return df_article_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke Function to Build KNN Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Training dataframe index - remove old one; this allows for extraction of correct index records\n",
    "# returned by the NearestNeighbors function\n",
    "train.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to set up NN learner\n",
    "hm_model_knn, df_cust_articles_nn = setup_NN_model(train, NN_metric = 'cosine', NN_algo = 'brute', submission_Type = 'local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The code segments below implement functions to extract predictions and evaluate results.  <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions to Extract Customer IDs and Article Predictions from Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Function to Obtain Article Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ArticlePredictions(knn_mdl, df, iNum_of_neighbours, TopN):\n",
    "    \n",
    "    # Call function to find nearest neighbour Customer Ids and Article Ids\n",
    "    # Pass in index, parameters for Nearest Neighbour function, and the number of neighbours to retrun\n",
    "    df_neighbours = get_CustSub_NeighbourIDs_articles(knn_mdl, df, train, iNum_of_neighbours)\n",
    "    \n",
    "    # Call function to return list of the most popular Articles in terms of the \n",
    "    # purchases made by the customers neighbourhood\n",
    "    df_cust_preds = get_Top12_Articles_for_Cust(df_neighbours, TopN)\n",
    "\n",
    "    \n",
    "    return df_cust_preds    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions to Evaluate Model Predictions aginst Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up column layout for dataframe that will score match accuracy of the KNN model against Test Data\n",
    "eval_Cols = ['customer_id', 'prediction_accuracy','Num_Act_Test_Purchases','Num_Pred_Purchases', 'Num_Act_Purch_Pred']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up function to return latest Customer transactions in the Test Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data of unique customers in Test Data\n",
    "def get_Latest_CustTxn_in_TestData():\n",
    "        \n",
    "    df = test.sort_values('t_dat', ascending=False).groupby('customer_id').head(1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Function to Return Evaluation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_Cust_Prediction_Score(cust_id, df_pred, df_actual, df_score):  \n",
    "\n",
    "    # Returns just the rows from the new dataframe that differ from the source dataframe\n",
    "    merged_df = df_pred.merge(df_actual, indicator=True, how='outer')\n",
    "\n",
    "    \n",
    "    matched_rows_df = merged_df[merged_df['_merge'] == 'both']  \n",
    "\n",
    "    df_no_pred_match = matched_rows_df.drop('_merge', axis=1)\n",
    "    \n",
    "    pred_scr = ( len(df_no_pred_match) / len(df_actual) )\n",
    "    \n",
    "    lst = [[cust_id, pred_scr, len(df_actual), len(df_pred), len(df_no_pred_match)]]    \n",
    "    df = pd.DataFrame(lst, columns = eval_Cols)\n",
    "\n",
    "    \n",
    "    # Stack the DataFrames on top of each other\n",
    "    df_score = pd.concat([df_score, df], axis=0)\n",
    "    \n",
    "    \n",
    "    return df_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterate Through Test Data and Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty Dataframe with column names only - will be filled with each predicton evaluation\n",
    "df_ModelEvalResults_Stack = pd.DataFrame(columns= eval_Cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The last transaction recorded for a given customer will be the basis of the 'Nearest Neighbour' prediction.  <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the characteristics of the last customer trxn and find the 12 most propular products bought in 'neighbouring' transactions.  <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Work Through Test Data to obtain prediction list of Articles\n",
    "\n",
    "# Get unique customers in Test Data\n",
    "df_Testdata_Customers_LastTrxn = get_Latest_CustTxn_in_TestData()\n",
    "[print('Number of customers in Test Data is: {}\\n'.format(len(df_Testdata_Customers_LastTrxn)))]\n",
    "\n",
    "# Set loop based on number of customers in Trxn datafile\n",
    "customer_records = (len(df_Testdata_Customers_LastTrxn))\n",
    "\n",
    "#customer_records = 2 # Temp loop control for testing\n",
    "\n",
    "# Generate article predictions for each customer write to a new submission file\n",
    "for cust_rec in range(0, customer_records):\n",
    "    \n",
    "    #print('cust_rec is...  {}\\n'.format(df_Testdata_Customers_LastTrxn.iloc[cust_rec]))\n",
    "       \n",
    "    # Extract last trxn record for a given customer record from trxn file.\n",
    "    # This data will be the input vector to submit to find 'Nearest Neighbours'\n",
    "    index_list = [cust_rec]\n",
    "    df_CustRec = df_Testdata_Customers_LastTrxn.loc[df_Testdata_Customers_LastTrxn.index[index_list]]         \n",
    "    df_CustTestNNVectors  = df_CustRec[nn_cols]\n",
    "    \n",
    "       \n",
    "    \n",
    "    # Obtain the KNN predictions for the Customer from the model\n",
    "    iNeighbours       = 15 # Number of Neighbours parameter for KNN model - 15\n",
    "    TopN              = 12  # Number of Article predictions/actual purchases to return     \n",
    "    df_TestCust_Preds     = get_ArticlePredictions(hm_model_knn, df_CustTestNNVectors, iNeighbours, TopN) # Predictions\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Obtain the 'actual' transactions made by the Customer - reading the Test dataset\n",
    "    # Create a dataframe of all transactions by the customer in the Test Data\n",
    "    C_Id = df_CustRec.iloc[0]['customer_id']  # Isolate Customer ID \n",
    "    df_CustRecId = df_CustRec[['customer_id']].copy()\n",
    "    df_test = test.loc[test[\"customer_id\"] == C_Id]  \n",
    "    df_TestCust_ActualPurchases = get_Top12_Articles_for_Cust(df_test, TopN) # Actual purchases (from Test dataset)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Build up a cumulative set of results as the loop iterates through each customer\n",
    "    df_ModelEvalResults_Stack = return_Cust_Prediction_Score(df_CustRecId['customer_id'], \n",
    "                                                             df_TestCust_Preds, \n",
    "                                                             df_TestCust_ActualPurchases,\n",
    "                                                             df_ModelEvalResults_Stack)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up display of customer count - useful when file converted and run in .py format\n",
    "[print('Number of customers in Test Data is: {}\\n'.format(len(df_Testdata_Customers_LastTrxn)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Present Accuracy of Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The determination of accuracy is based on;  <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ( (1/Number of Actual Purchases)  *  (Sum(Actual Purchases Predicted)/1) )   *  1/Total Number of Customers ) <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Index on final dataframe with prediction / actual results\n",
    "df_ModelEvalResults_Stack.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum all results to present an overall accuracy\n",
    "df_test_eval = df_ModelEvalResults_Stack.drop('customer_id', axis=1)\n",
    "df_test_eval = pd.DataFrame(np.sum(df_test_eval.values, axis=0), columns=['sum'])\n",
    "df_test_eval_T = df_test_eval.T # call transpose() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column to capture total numbers of customers\n",
    "df_test_eval_T['tc'] = customer_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for ease of understanding\n",
    "df_test_eval_T.columns = ['Sum Prediction Accuracies', 'Actual Test Purchases', 'Predicted Purchases', \n",
    "                          'Matching Predictions', 'Total Customers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display evaluation results \n",
    "df_test_eval_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up display of outcomes - useful when file converted and run in .py format\n",
    "[print('''Summation of Evaluation Outcome\\n \n",
    "          Actual Test Purchases {}\\n \n",
    "          Pred Purchases {}\\n \n",
    "          Matching Preds {}\\n \n",
    "          Total Customers {}\\n\\n\\n\n",
    "          No of Test Dataset Rows {}\\n\n",
    "          No of Train Dataset Rows {}\\n'''\n",
    "       .format(df_test_eval_T.iloc[0]['Actual Test Purchases'],\n",
    "                df_test_eval_T.iloc[0]['Predicted Purchases'],\n",
    "                df_test_eval_T.iloc[0]['Matching Predictions'],\n",
    "                df_test_eval_T.iloc[0]['Total Customers'],\n",
    "                (len(test)),\n",
    "                (len(train))\n",
    "               ))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print('Overall Accuracy Prediction with Test Data is: {:04.1f}%\\n'\n",
    "       .format( ((df_test_eval_T.iloc[0]['Sum Prediction Accuracies']/df_test_eval_T.iloc[0]['Total Customers'])*100) ))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The solution file to have 1371980 prediction rows - use Sample Submission to construct  <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Sample Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Full Sample Submission File\n",
    "df_sub  = pd.read_csv('../input/sample_submission.csv', dtype={'customer_id': 'string'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp code to build submission framework based on a smaller fraction of the Sample Submission file\n",
    "df_sub_sample = df_sub.sample(frac=0.01, replace=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp code to check dataframe matches file output\n",
    "df_sub_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record record size of file for later validation.  <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numCustomers = df_sub.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions to Parse Article Predictions for Kaggle Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Function to read Customer IDs from Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return Customer Ids from sample submission file in sequence\n",
    "def get_next_SubmissionCustomer(idx):\n",
    "           \n",
    "    # Get Customer Id from Sample Sub\n",
    "    cust_id_s  = df_sub_sample.iloc[idx,0]\n",
    "    \n",
    "    df_cust_id = df_sub_sample.head(idx+1)\n",
    "    df_cust_id = df_cust_id.drop(['prediction'], axis = 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Return index of customer id in training dataframe and the customer id itself\n",
    "    return df_cust_id, cust_id_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Function to Obtain Customer Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_CustTrxnArtcls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_CustomerTrxn(rec):\n",
    "    \n",
    "    # Get Next Customer Id from Submission Table\n",
    "    df_one_cust, cust_id_t = get_next_SubmissionCustomer(rec)\n",
    "\n",
    "\n",
    "    df_SubCust_Trxn = df_trxns.loc[(df_trxns['customer_id'] == cust_id_t)]\n",
    "    \n",
    "    # On rare occassion that a customer has no transactions - create a dummy records to avoid a failure\n",
    "    if df_SubCust_Trxn.shape[0] == 0:\n",
    "        print('NO Transactions!')\n",
    "        # Create dummy dataframe\n",
    "        df_Dummy = pd.DataFrame([[cust_id_t,\n",
    "                '2016-10-25',\n",
    "                 651329001,\n",
    "                 0.021593,\n",
    "                 1]], columns=['customer_id','t_dat','article_id','price','sales_channel_id'])\n",
    "        # Stack the DataFrames on top of each other\n",
    "        df_SubCust_Trxn = pd.concat([df_SubCust_Trxn, df_Dummy], axis=0)\n",
    "        df_SubCust_Trxn.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    \n",
    "    # Use last transaction\n",
    "    df_SubCust_Trxn1 = df_SubCust_Trxn.sort_values(by=['t_dat'], ascending=False).head(1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df_SubCust_Trxn1, cust_id_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Function to Obtain Customer Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_CustomerAttrs(df, cust_id_c):\n",
    "\n",
    "    df_cust = df_customers.loc[(df_customers['customer_id'] == cust_id_c)] \n",
    "    \n",
    "    column = df_cust[\"age\"]\n",
    "    age = column.max()\n",
    "    \n",
    "    df['age'] = age\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Function to Obtain Article Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_ArticleAttrs(df):\n",
    "    \n",
    "    # Get Article Id from Sample Sub\n",
    "    artcl_id_s  = df.iloc[0,2]\n",
    "    \n",
    "\n",
    "    df_art = df_Articles.loc[(df_Articles['article_id'] == artcl_id_s)]\n",
    "    \n",
    "    # Use last article value for Product code\n",
    "    prod_code = df_art['product_code'].iat[-1]\n",
    "    #print('prod_code is...  {}'.format(prod_code))\n",
    "    \n",
    "    df['product_code'] = prod_code\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Functions to Parse + Build Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return Article Id Predictions in String format required for Kaggle competition\n",
    "def get_str_Article_Predictions(df):\n",
    "    \n",
    "    article_pred = df['article_id'].values.tolist()\n",
    "    \n",
    "    article_pred = ['0' + str(article_id) for article_id in article_pred]\n",
    "    article_pred_str =  ' '.join(article_pred)\n",
    "    \n",
    "    str_article_preds = article_pred_str\n",
    "    \n",
    "    return str_article_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Build up dataframe for submission\n",
    "def build_up_submission_df(cust, str_preds, custid_str, last_col):\n",
    "    \n",
    "    lst = [[cust, str_preds]]    \n",
    "\n",
    "    df = pd.DataFrame(lst, columns =[custid_str, last_col])\n",
    "    final_df =df\n",
    "    \n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train New KNN Model for Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to generate new KNN model for Kaggle Submission \n",
    "kaggle_hm_model_knn, df_cust_articles_nn = setup_NN_model(train, NN_metric = 'cosine', NN_algo = 'brute', \n",
    "                                                          submission_Type = 'Kaggle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate Through Submission File to Generate Article Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty Dataframe with column names only - will be filled with each Customer and their set of predictions\n",
    "df_Submission_Stack = pd.DataFrame(columns=['customer_id', 'prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up loop based on Submission file size\n",
    "customer_records = 0 # Skip this processing if zero\n",
    "\n",
    "\n",
    "# Generate article predictions for each customer write to a new submission file\n",
    "for cust_rec in range(0, customer_records):\n",
    "    \n",
    "    print('cust_rec is... ' + str(cust_rec) +'\\n')\n",
    "    \n",
    "    df_SubCust_Trxn1, cust_id_k  = find_latest_CustomerTrxn(cust_rec)  \n",
    "    \n",
    "    #print('\\ndf_SubCust_Trxn1 is...\\n')\n",
    "    #display(df_SubCust_Trxn1)\n",
    "        \n",
    "    df_SubCust_Trxn1  = find_latest_CustomerAttrs(df_SubCust_Trxn1, cust_id_k)\n",
    "    \n",
    "    df_SubCust_Trxn1  = find_latest_ArticleAttrs(df_SubCust_Trxn1)\n",
    "    \n",
    "    #print('\\ndf_SubCust_Trxn1 is...\\n')\n",
    "    #display(df_SubCust_Trxn1)\n",
    "    \n",
    "    df_CustNNVectors  = df_SubCust_Trxn1[nn_Kaggle_cols]\n",
    "    \n",
    "    #print('\\ndf_CustNNVectors is...\\n')\n",
    "    #display(df_CustNNVectors)\n",
    "        \n",
    "    iNeighbours       = 5 # Number of Neighbours parameter for KNN model\n",
    "    TopN              = 12  # Number of Article predictions to return\n",
    "    df_cust_preds     = get_ArticlePredictions(kaggle_hm_model_knn, df_CustNNVectors, iNeighbours, TopN)\n",
    "    \n",
    "    # Get a string output of Customer Article Predicitons - format as required for Kaggle competition\n",
    "    str_articles_predictions = get_str_Article_Predictions(df_cust_preds)\n",
    "    \n",
    "    # Format Customer ID and list of Article predictions - add to dataframe to be used for CSV output\n",
    "    df_NextCust_Preds = build_up_submission_df(cust_id_k, str_articles_predictions,'customer_id','prediction')\n",
    "    \n",
    "    \n",
    "    # Stack the DataFrames on top of each other\n",
    "    df_Submission_Stack = pd.concat([df_Submission_Stack, df_NextCust_Preds], axis=0)\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Predictions Submission File to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SetUp Disk Directory for Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up folder for submission file with predictions\n",
    "submission_path = '../submission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder above project directory\n",
    "os.makedirs(submission_path, exist_ok=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Submission .CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Submission_Stack = df_Submission_Stack.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write pre-test submission file to disk\n",
    "df_Submission_Stack.to_csv(submission_path+'/submission_pretest.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTempFinal  = pd.read_csv(submission_path+'/submission_pretest.csv')\n",
    "\n",
    "[print(\"Matching the number of dfSub rows to the correct submission size. \\nSubmission Row: {} v Expected Rows: {}\"\n",
    "                                                                                   .format(dfTempFinal.shape[0],numCustomers))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check format of final submission file\n",
    "dfTempFinal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write final submission file to disk\n",
    "dfTempFinal.to_csv(submission_path+'/submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a class=\"tocSkip\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "659px",
    "left": "0px",
    "right": "1227.5px",
    "top": "87px",
    "width": "305.5px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
