{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Continuous Assessment - Class Group: TU060 <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSc in Computer Science - Data Science (Part Time) <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Name: Ciaran Finnegan  <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student No: D21124026 <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student EMail: D21124026@mytudublin.ie <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## May 2022<a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H&M Personalised Fashion Recommendations <a class=\"tocSkip\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This project takes the datasets provided by Kaggle for the H&M Personalised Fashion Recommendations competition.  <a class=\"tocSkip\">\n",
    "\n",
    "Ipso forum..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The CA follows a n-part structure. <a class=\"tocSkip\">\n",
    "\n",
    "Section 1: Ipso forum...\n",
    "\n",
    "Section 2: Ipso forum...\n",
    "\n",
    "Section 3: Ipso forum...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Jupyter-Notebook-Set-Up\" data-toc-modified-id=\"Jupyter-Notebook-Set-Up-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Jupyter Notebook Set Up</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Import Libraries</a></span><ul class=\"toc-item\"><li><span><a href=\"#General-Utility-Libraries\" data-toc-modified-id=\"General-Utility-Libraries-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>General Utility Libraries</a></span></li><li><span><a href=\"#Panda-+-Numpy\" data-toc-modified-id=\"Panda-+-Numpy-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Panda + Numpy</a></span></li><li><span><a href=\"#Matplotlib-Library\" data-toc-modified-id=\"Matplotlib-Library-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Matplotlib Library</a></span></li><li><span><a href=\"#Plotly-Libraries\" data-toc-modified-id=\"Plotly-Libraries-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Plotly Libraries</a></span></li></ul></li></ul></li><li><span><a href=\"#Project-Data-Import,-Analysis-and-Preparation\" data-toc-modified-id=\"Project-Data-Import,-Analysis-and-Preparation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Project Data Import, Analysis and Preparation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-and-Sample-Kaggle-H&amp;M-Datasets\" data-toc-modified-id=\"Load-and-Sample-Kaggle-H&amp;M-Datasets-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Load and Sample Kaggle H&amp;M Datasets</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-Up-File-Paths\" data-toc-modified-id=\"Set-Up-File-Paths-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Set Up File Paths</a></span></li><li><span><a href=\"#H&amp;M-Transaction-Dataset\" data-toc-modified-id=\"H&amp;M-Transaction-Dataset-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>H&amp;M Transaction Dataset</a></span></li><li><span><a href=\"#H&amp;M-Articles-Dataset\" data-toc-modified-id=\"H&amp;M-Articles-Dataset-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>H&amp;M Articles Dataset</a></span></li><li><span><a href=\"#H&amp;M-Customers-Dataset\" data-toc-modified-id=\"H&amp;M-Customers-Dataset-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>H&amp;M Customers Dataset</a></span></li></ul></li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#H&amp;M-Sampled-Transaction-Dataset\" data-toc-modified-id=\"H&amp;M-Sampled-Transaction-Dataset-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>H&amp;M Sampled Transaction Dataset</a></span></li><li><span><a href=\"#H&amp;M-Sampled-Customer-Dataset\" data-toc-modified-id=\"H&amp;M-Sampled-Customer-Dataset-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>H&amp;M Sampled Customer Dataset</a></span></li><li><span><a href=\"#H&amp;M-Sampled-Articles-Dataset\" data-toc-modified-id=\"H&amp;M-Sampled-Articles-Dataset-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>H&amp;M Sampled Articles Dataset</a></span></li></ul></li><li><span><a href=\"#Data-Preparation-and-Feature-Enrichment\" data-toc-modified-id=\"Data-Preparation-and-Feature-Enrichment-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Data Preparation and Feature Enrichment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Transaction-Data-Feature-Enrichment-(Sample)\" data-toc-modified-id=\"Transaction-Data-Feature-Enrichment-(Sample)-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Transaction Data Feature Enrichment (Sample)</a></span></li><li><span><a href=\"#Extend-Dataframe-with-Customer-Data\" data-toc-modified-id=\"Extend-Dataframe-with-Customer-Data-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Extend Dataframe with Customer Data</a></span></li><li><span><a href=\"#Extend-Dataframe-with-Articles-Data\" data-toc-modified-id=\"Extend-Dataframe-with-Articles-Data-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>Extend Dataframe with Articles Data</a></span></li><li><span><a href=\"#Final-Data-Preparation-Steps\" data-toc-modified-id=\"Final-Data-Preparation-Steps-2.3.4\"><span class=\"toc-item-num\">2.3.4&nbsp;&nbsp;</span>Final Data Preparation Steps</a></span></li></ul></li></ul></li><li><span><a href=\"#Split-Data:-Train-and-Test-Sets\" data-toc-modified-id=\"Split-Data:-Train-and-Test-Sets-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Split Data: Train and Test Sets</a></span></li><li><span><a href=\"#Model-Preparation\" data-toc-modified-id=\"Model-Preparation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Model Preparation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-Up-the-Scaling-Algorithm\" data-toc-modified-id=\"Set-Up-the-Scaling-Algorithm-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Set Up the Scaling Algorithm</a></span></li><li><span><a href=\"#Set-Up-the-Columns-to-be-Scaled\" data-toc-modified-id=\"Set-Up-the-Columns-to-be-Scaled-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Set Up the Columns to be Scaled</a></span></li><li><span><a href=\"#Scale-the-Training-Dataset\" data-toc-modified-id=\"Scale-the-Training-Dataset-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Scale the Training Dataset</a></span></li></ul></li><li><span><a href=\"#K-Nearest-Neighbour-Operations\" data-toc-modified-id=\"K-Nearest-Neighbour-Operations-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>K Nearest Neighbour Operations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-Functions-for-NN-Analysis\" data-toc-modified-id=\"Create-Functions-for-NN-Analysis-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Create Functions for NN Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-up-function-to-Build-Nearest-Neighbours-Model\" data-toc-modified-id=\"Set-up-function-to-Build-Nearest-Neighbours-Model-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Set up function to Build Nearest Neighbours Model</a></span></li><li><span><a href=\"#Set-up-function-to--Nearest-Neighbours-(+-Articles-bought)-for-a-Customer\" data-toc-modified-id=\"Set-up-function-to--Nearest-Neighbours-(+-Articles-bought)-for-a-Customer-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Set up function to  Nearest Neighbours (+ Articles bought) for a Customer</a></span></li><li><span><a href=\"#Find-Top-Selling-Products-in-'Neighbourhood'-for-a-Customer\" data-toc-modified-id=\"Find-Top-Selling-Products-in-'Neighbourhood'-for-a-Customer-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Find Top Selling Products in 'Neighbourhood' for a Customer</a></span></li><li><span><a href=\"#Invoke-Function-to-Build-KNN-Model\" data-toc-modified-id=\"Invoke-Function-to-Build-KNN-Model-5.1.4\"><span class=\"toc-item-num\">5.1.4&nbsp;&nbsp;</span>Invoke Function to Build KNN Model</a></span></li></ul></li></ul></li><li><span><a href=\"#Build-Submission\" data-toc-modified-id=\"Build-Submission-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Build Submission</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-Sample-Submission-File\" data-toc-modified-id=\"Read-Sample-Submission-File-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Read Sample Submission File</a></span></li><li><span><a href=\"#Define-Function-to-read-Customer-IDs-from-Submission-File\" data-toc-modified-id=\"Define-Function-to-read-Customer-IDs-from-Submission-File-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Define Function to read Customer IDs from Submission File</a></span></li><li><span><a href=\"#Extract-Customer-IDs-from-Submission-File-and-Find-Article-Predictions\" data-toc-modified-id=\"Extract-Customer-IDs-from-Submission-File-and-Find-Article-Predictions-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Extract Customer IDs from Submission File and Find Article Predictions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-Function-to-Obtain-Customer-Transactions\" data-toc-modified-id=\"Define-Function-to-Obtain-Customer-Transactions-6.3.1\"><span class=\"toc-item-num\">6.3.1&nbsp;&nbsp;</span>Define Function to Obtain Customer Transactions</a></span></li><li><span><a href=\"#Define-Function-to-Obtain-Customer-Features\" data-toc-modified-id=\"Define-Function-to-Obtain-Customer-Features-6.3.2\"><span class=\"toc-item-num\">6.3.2&nbsp;&nbsp;</span>Define Function to Obtain Customer Features</a></span></li><li><span><a href=\"#Define-Function-to-Obtain-Article-Features\" data-toc-modified-id=\"Define-Function-to-Obtain-Article-Features-6.3.3\"><span class=\"toc-item-num\">6.3.3&nbsp;&nbsp;</span>Define Function to Obtain Article Features</a></span></li><li><span><a href=\"#Define-Function-to-Obtain-Article-Predictions\" data-toc-modified-id=\"Define-Function-to-Obtain-Article-Predictions-6.3.4\"><span class=\"toc-item-num\">6.3.4&nbsp;&nbsp;</span>Define Function to Obtain Article Predictions</a></span></li><li><span><a href=\"#Define-Functions-to-Parse-Article-Predictions-for-Kaggle-Submission\" data-toc-modified-id=\"Define-Functions-to-Parse-Article-Predictions-for-Kaggle-Submission-6.3.5\"><span class=\"toc-item-num\">6.3.5&nbsp;&nbsp;</span>Define Functions to Parse Article Predictions for Kaggle Submission</a></span></li><li><span><a href=\"#Iterate-Through-Submission-File-to-Generate-Article-Predictions\" data-toc-modified-id=\"Iterate-Through-Submission-File-to-Generate-Article-Predictions-6.3.6\"><span class=\"toc-item-num\">6.3.6&nbsp;&nbsp;</span>Iterate Through Submission File to Generate Article Predictions</a></span></li></ul></li><li><span><a href=\"#Write-Predictions-Submission-File-to-CSV\" data-toc-modified-id=\"Write-Predictions-Submission-File-to-CSV-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Write Predictions Submission File to CSV</a></span><ul class=\"toc-item\"><li><span><a href=\"#SetUp-Disk-Directory-for-Submission-File\" data-toc-modified-id=\"SetUp-Disk-Directory-for-Submission-File-6.4.1\"><span class=\"toc-item-num\">6.4.1&nbsp;&nbsp;</span>SetUp Disk Directory for Submission File</a></span></li><li><span><a href=\"#Create-Submission-.CSV-File\" data-toc-modified-id=\"Create-Submission-.CSV-File-6.4.2\"><span class=\"toc-item-num\">6.4.2&nbsp;&nbsp;</span>Create Submission .CSV File</a></span></li></ul></li></ul></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebook Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: It is assumed that the matplotlib and plotly library has been installed on the user's machine before it is available for import.\n",
    "\n",
    "```bash\n",
    "> pip install matplotlib\n",
    "> pip install plotly\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Utility Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Panda + Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python libraries for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up additional libraries for graphical presentations. A number of earlier graphs use Matplotlib. In later sections, Plotly graphs are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matplotlib Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotly Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For interactive graphics and global displays\n",
    "#import plotly.express as px\n",
    "#import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Data Import, Analysis and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Sample Kaggle H&M Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The assignment starts with a Kaggle H&M Personalised Fashion Recommendations datasets. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up File Paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a Global path file variable to use when loading each dataset\n",
    "Path_To_Inputs = '../input'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H&M Transaction Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load H&M Transaction dataset\n",
    "df_trxns = pd.read_csv(os.path.join(Path_To_Inputs,'transactions_train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Number of Rows, Number of Columns in Transaction dataset prior to Sampling\n",
    "df_trxns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract random sample of Transactions (without replacement) - 0.1% of records\n",
    "df_trxns_sample = df_trxns.sample(frac=0.05, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Number of Rows, Number of Columns in Transaction dataset after Sampling\n",
    "df_trxns_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the index\n",
    "df_trxns_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H&M Articles Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file, located in 'input' folder above this notebook, for articles data - containing meta data on purchasable items.\n",
    "df_Articles = pd.read_csv(os.path.join(Path_To_Inputs,'articles.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Number of Rows, Number of Columns in Articles dataset\n",
    "df_Articles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the H&M articles contained in the sample Transaction dataset \n",
    "articleIDs_in_Trxn_Sample  = set(df_trxns_sample[\"article_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset that only contains H&M articles in the sample Transaction dataset\n",
    "df_articles_sample = df_Articles[df_Articles[\"article_id\"].isin(articleIDs_in_Trxn_Sample)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Number of Rows, Number of Columns in Sample Articles dataset\n",
    "df_articles_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H&M Customers Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load H&M Customer dataset\n",
    "df_customers = pd.read_csv(os.path.join(Path_To_Inputs,'customers.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Number of Rows, Number of Columns in Customers dataset prior to Sampling\n",
    "df_customers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the H&M Customers contained in the sample Transaction dataset \n",
    "custIDs_in_Trxn_Sample  = set(df_trxns_sample[\"customer_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Customer Ids in Transaction Sample\n",
    "len(custIDs_in_Trxn_Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset that only contains H&M Customers from the sample Transaction dataset\n",
    "df_customers_sample = df_customers[df_customers[\"customer_id\"].isin(custIDs_in_Trxn_Sample)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Number of Rows, Number of Columns in Sample Customers dataset\n",
    "df_customers_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display key data attributes and distributions in Sampled H&M datasets. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H&M Sampled Transaction Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic column information for the initial Kaggle 'transaction_train' datatset - after Sampling of original dataset\n",
    "df_trxns_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display datatype of colums in original H&M Transaction dataset\n",
    "df_trxns_sample.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H&M Sampled Customer Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic column information for the initial Kaggle 'customers' datatset - after Sampling of original dataset\n",
    "df_customers_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers_sample.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H&M Sampled Articles Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic column information for the initial Kaggle 'article' datatset - after Sampling of original dataset\n",
    "df_articles_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles_sample.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and Feature Enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a dataset upon which to build ML models. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin with Transaction Dataset (sampled) and merge in customer and article data. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transaction Data Feature Enrichment (Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataset for modelling. Initial basis for new dataset is the sampled transaction dataset.\n",
    "df_HM_CustTrxn = df_trxns_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_CustTrxn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose Datetime information \n",
    "df_HM_CustTrxn['t_dat'] = pd.to_datetime(df_HM_CustTrxn['t_dat'], format=\"%Y-%m-%d\")\n",
    "\n",
    "# Add Column for Transaction Year and convert to integer value\n",
    "df_HM_CustTrxn['txn_year'] = df_HM_CustTrxn['t_dat'].dt.strftime('%Y')\n",
    "df_HM_CustTrxn['txn_year'] = df_HM_CustTrxn['txn_year'].astype(str).astype(int)\n",
    "\n",
    "# Add Column for Transaction Month and convert to integer value\n",
    "df_HM_CustTrxn['txn_mth'] = df_HM_CustTrxn['t_dat'].dt.strftime('%m')\n",
    "df_HM_CustTrxn['txn_mth'] = df_HM_CustTrxn['txn_mth'].astype(str).astype(int)\n",
    "\n",
    "# Add Column for Transaction Day and convert to integer value\n",
    "df_HM_CustTrxn['txn_day'] = df_HM_CustTrxn['t_dat'].dt.strftime('%d')\n",
    "df_HM_CustTrxn['txn_day'] = df_HM_CustTrxn['txn_day'].astype(str).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out column to drop timestamp attribute\n",
    "df_HM_CustTrxn = df_HM_CustTrxn.iloc[:,1:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_CustTrxn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in the Transaction Dataframe\n",
    "[print('Any Null Values in the Trxn (sample) dataframe?','\\n' ,df_HM_CustTrxn.isnull().any())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Dataframe with Customer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm dimensions of Trxn (sample) dataframe BEFORE merging with Customer data\n",
    "df_HM_CustTrxn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Customer Attributes into the Trxn (Sample) Dataframe. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Customer atributes from the Customer dataset based on a join with the 'customer_id' field\n",
    "df_HM_CustTrxn = pd.merge(df_HM_CustTrxn, df_customers_sample, how=\"left\", on=[\"customer_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm dimensions of Trxn (sample) dataframe AFTER merging with Customer data\n",
    "df_HM_CustTrxn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_CustTrxn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out other Customer specific attributes apart from; age + fashion_news_frequency + club_member_status + postal code\n",
    "# There are very many missing values in the other Customer dataset columns, so these will be ignored\n",
    "df_HM_CustTrxn = df_HM_CustTrxn.iloc[:, [0, 1, 2, 3, 4, 5, 6, 9, 10, 11, 12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_CustTrxn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_CustTrxn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in the merged Trxn/Customer dataframe\n",
    "[print('Any Null Values in the merged Trxn/Customer dataframe?','\\n' ,df_HM_CustTrxn.isnull().any())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Rows with Missing Club Member Status Attributes. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in the club_member_status column  \n",
    "df_null_club_member_status = df_HM_CustTrxn.loc[(df_HM_CustTrxn[\"club_member_status\"].isnull())]\n",
    "\n",
    "[print('Numbers of rows with missing club member status?','\\n' ,len(df_null_club_member_status.index))]\n",
    "[print('\\nPrecentage of total rows in Trxn dataframe with missing club member status attributes?\\n{:06.3f}%'\n",
    "                               .format((len(df_null_club_member_status.index)/len(df_HM_CustTrxn.index))*100))]\n",
    "\n",
    "\n",
    "# For simplicity the rows with missing values in the club member status will be ignored\n",
    "# Re-Build Txn/Customer Dataframe with no missing values in the club member status\n",
    "df_HM_CustTrxn = df_HM_CustTrxn.loc[(df_HM_CustTrxn[\"club_member_status\"].notnull())]\n",
    "\n",
    "# Confirm dimensions of Trxn (sample) dataframe AFTER removing rows with missing values in the club member status column\n",
    "[print('\\nCurrent Number of Rows/Columns in Trxn/Customer dataframe - with missing club mem status values removed','\\n' ,\n",
    "                                                                                                   df_HM_CustTrxn.shape)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Rows with Missing Fashion News Frequency Attributes. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in the club_member_status column  \n",
    "df_null_fn_freq = df_HM_CustTrxn.loc[(df_HM_CustTrxn[\"fashion_news_frequency\"].isnull())]\n",
    "\n",
    "[print('Numbers of rows with missing fashion_news_frequency values?','\\n' ,len(df_null_fn_freq.index))]\n",
    "[print('\\nPrecentage of total rows in Trxn dataframe with missing fashion_news_frequency attributes?\\n{:06.3f}%'\n",
    "                               .format((len(df_null_fn_freq.index)/len(df_HM_CustTrxn.index))*100))]\n",
    "\n",
    "# For simplicity the rows with missing values in the fashion_news_frequency column will be ignored\n",
    "# Re-Build Txn/Customer Dataframe with no missing values in the fashion_news_frequency column\n",
    "df_HM_CustTrxn = df_HM_CustTrxn.loc[(df_HM_CustTrxn[\"fashion_news_frequency\"].notnull())]\n",
    "\n",
    "# Confirm dimensions of Trxn (sample) dataframe AFTER removing rows with missing values in the fashion_news_frequency column\n",
    "[print('\\nCurrent Number of Rows/Columns in Trxn/Customer dataframe - missing Fash News Freq values removed','\\n' ,\n",
    "                                                                                                   df_HM_CustTrxn.shape)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Rows with Missing Age Attributes. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in the age column  \n",
    "df_null_age = df_HM_CustTrxn.loc[(df_HM_CustTrxn[\"age\"].isnull())]\n",
    "\n",
    "[print('Numbers of rows with missing age values?','\\n' ,len(df_null_age.index))]\n",
    "[print('\\nPrecentage of total rows in Trxn dataframe with missing age attributes?\\n{:06.3f}%'\n",
    "                               .format((len(df_null_age.index)/len(df_HM_CustTrxn.index))*100))]\n",
    "\n",
    "# Re-Build Txn/Customer Dataframe with no missing values in the age column\n",
    "df_HM_CustTrxn = df_HM_CustTrxn.loc[(df_HM_CustTrxn[\"age\"].notnull())]\n",
    "\n",
    "\n",
    "# Confirm dimensions of Trxn (sample) dataframe AFTER removing rows with missing values in the age column\n",
    "[print('\\nCurrent Number of Rows/Columns in Trxn/Customer dataframe - missing ages removed','\\n' ,df_HM_CustTrxn.shape)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the club_member_status + fashion_news_frequency attributes <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_CustTrxn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_Categor_Features(df, col_name):\n",
    "       \n",
    "    # The categorical values in dataframe must be encoded to numberical values\n",
    "    df_encoded_cols = pd.get_dummies(df[col_name])\n",
    "    df_encoded      = df.join(df_encoded_cols)\n",
    "    \n",
    "    # Remove the prior Categorical column from the Articles dataframe\n",
    "    df_encoded.drop([col_name], axis=1, inplace=True)\n",
    "    \n",
    "    # Return the dataframe with encoded columsn added\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_cols_to_encode = [\"club_member_status\",\"fashion_news_frequency\"]\n",
    "for column_name in cust_cols_to_encode:\n",
    "    df_HM_CustTrxn = encode_Categor_Features(df_HM_CustTrxn, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Postal Code - a non numerical value that is not easily converted\n",
    "df_HM_CustTrxn.drop([\"postal_code\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic column information for the initial Kaggle 'article' datatset - after Sampling of original dataset\n",
    "df_HM_CustTrxn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm dimensions of Trxn (sample) dataframe AFTER encoding Categorical Values\n",
    "[print('\\nCurrent Number of Rows/Columns in Trxn/Customer dataframe - after encoding of categorical values completed','\\n' ,\n",
    "                                                                                                   df_HM_CustTrxn.shape)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in the Transaction Dataframe\n",
    "[print('Any Null Values in the merged Trxn/Customer (sample) dataframe?','\\n' ,df_HM_CustTrxn.isnull().any())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Dataframe with Articles Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove 'Redundant' Categorical Columns in Articles (sample) Dataframe. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic column information for the initial Kaggle 'article' datatset - after Sampling of original dataset\n",
    "df_articles_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'redundant' categotical columns from Articles (sample) dataframe\n",
    "# Consider a categorical column Redundant when is 'partnered' with a numerical code \n",
    "df_articles_sample.drop([\"prod_name\", \n",
    "                         \"product_type_name\", \n",
    "                         \"graphical_appearance_name\",\n",
    "                         \"colour_group_name\",\n",
    "                         \"perceived_colour_value_name\",\n",
    "                         \"perceived_colour_master_name\",\n",
    "                         \"department_name\",\n",
    "                         \"index_name\",\n",
    "                         \"index_group_name\",\n",
    "                         \"section_name\",\n",
    "                         \"garment_group_name\",\n",
    "                         \"detail_desc\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic column information for the initial Kaggle 'article' datatset - after Sampling of original dataset\n",
    "df_articles_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use One-Hot-Encoding on remaining categorical columns. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_cols_to_encode =['product_group_name', 'index_code']\n",
    "for column_name in article_cols_to_encode:\n",
    "    df_articles_sample = encode_Categor_Features(df_articles_sample, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic column information for sampled articles dataframe after encoding\n",
    "df_articles_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Trxn/Customers Dataframe with Articles Dataframe (sample). <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Articles atributes from the Articles dataset based on a join with the 'article_id' field\n",
    "df_HM_CustTrxnArtcls = pd.merge(df_HM_CustTrxn, df_articles_sample, how=\"left\", on=[\"article_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm dimensions of Trxn (sample) dataframe AFTER merging with Articles data\n",
    "df_HM_CustTrxnArtcls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic column information for the initial Kaggle 'transaction_train' datatset - AFTER merging with Articles data\n",
    "df_HM_CustTrxnArtcls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in the merged Trxn/Customer/Articles dataframe\n",
    "[print('Any Null Values in the merged Trxn/Customer/Articles dataframe?','\\n' ,df_HM_CustTrxnArtcls.isnull().any())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Data Preparation Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Any Duplicates in Final Merged Dataframe. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a check to see if any duplicate entires exist in the merged Trxn/Customer/Articles dataframe\n",
    "[print('Numbers of duplicated rows in the final merged Trxn/Customer/Articles dataframe?','\\n' \n",
    "                                                                       ,df_HM_CustTrxnArtcls.duplicated().sum())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows in the merged Trxn/Customer/Articles dataframe\n",
    "df_HM_CustTrxnArtcls = df_HM_CustTrxnArtcls.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-check: Find duplicate rows across all columns\n",
    "duplicateRows = df_HM_CustTrxnArtcls[df_HM_CustTrxnArtcls.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will advise if duplicates are successfully removed.\n",
    "if duplicateRows.shape[0] < 1:\n",
    "    [print('Success! Duplicate Rows Removed from merged Trxn/Customer/Articles dataframe')]\n",
    "else:\n",
    "    [print('Warning! Duplicate Rows Remain in merged Trxn/Customer/Articles dataframe')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm dimensions of Trxn/Customer/Articles (sample) dataframe AFTER removing duplicate rows\n",
    "[print('\\nCurrent Number of Rows/Columns in merged Trxn/Customer/Articles dataframe - duplicates removed','\\n' ,\n",
    "                                                                                           df_HM_CustTrxnArtcls.shape)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### See if there is a way to check the integrity of the customer merging.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data: Train and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data into 70% for Training, 30% for Testing. <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test set\n",
    "# Parameter set to 30% for size of Test set\n",
    "# Given that there is ordering in the dataset, the 'shuffle' parameter is set to 'True'\n",
    "train, test = train_test_split(df_HM_CustTrxnArtcls, test_size=0.1, random_state=101, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Confirmation of data split\n",
    "dataDescription = 'H&M Sample Data Split'\n",
    "print(\"\\n{} Training Set Shape : \\n\\t\".format(dataDescription))\n",
    "print(train.shape)\n",
    "print(\"\\n{} Test Set Shape : \\n\\t\".format(dataDescription))\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up the Scaling Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Normalizing numerical features so that each feature has a value between 0 and 1\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing numerical features so that each feature has mean 0 and variance 1\n",
    "# feature_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up the Columns to be Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chk_cols =  ['product_code',\n",
    "             'product_type_no',\n",
    "             'graphical_appearance_no',\n",
    "             'colour_group_code',\n",
    "             'perceived_colour_value_id',\n",
    "             'perceived_colour_master_id',\n",
    "             'department_no',\n",
    "             'index_group_no',\n",
    "             'section_no',\n",
    "             'garment_group_no']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the Training Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_trainset_features(df, dtype, no_scale):\n",
    "    for x in df.columns[df.dtypes == dtype]:\n",
    "        if x != no_scale:\n",
    "            df[x] = scaler.fit_transform(np.array(df[x]).reshape(-1,1))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_scale = 'article_id'\n",
    "scale_dtypes = ['int64','int32','float64']\n",
    "for data_types in scale_dtypes:\n",
    "    scale_trainset_features(train, data_types, no_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbour Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Functions for NN Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up function to Build Nearest Neighbours Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Unsupervised learner for implementing neighbour searches of Customer Transactions\n",
    "# Set default parameters for NN algorithm - these can be overwritten during optimisationphase\n",
    "def setup_NN_model(df_train, NN_metric = 'cosine', NN_algo = 'brute'):\n",
    "       \n",
    "    # Set up numerical data inputs for NearestNeighbors function\n",
    "    df_cust_articles = df_train[chk_cols]\n",
    "    \n",
    "    # Set up parameters of NearestNeighbors function\n",
    "    model_knn = NearestNeighbors(metric = NN_metric, algorithm = NN_algo)\n",
    "    \n",
    "    # Implement NearestNeighbors function on merged Trxn/Customer/Article data elements\n",
    "    model_knn.fit(df_cust_articles)\n",
    "    \n",
    "    return model_knn, df_cust_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up function to  Nearest Neighbours (+ Articles bought) for a Customer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a function to return a list of Nearest Neighbours for a given customer.\n",
    "# Return Neighbour Customer Ids and the articles they both\n",
    "def get_CustSub_NeighbourIDs_articles(model_knn, df_cust_articles, df_train, NN_Neighbours):\n",
    "     \n",
    "    \n",
    "    # Return index values for nearest neighbours - determined by 'Num_Neighbour' parameter\n",
    "    distances, indices = model_knn.kneighbors(df_cust_articles.iloc[0].values.reshape(1, -1), \n",
    "                                              n_neighbors = NN_Neighbours)\n",
    "     \n",
    "    \n",
    "\n",
    "    # Using the Index values returned by the Nearest neighbour model\n",
    "    # Generate a list of Customer Ids and Article Ids for these neighbours\n",
    "    for i in range(0, len(distances.flatten())):\n",
    "        if i == 0:\n",
    "            customer_list = []\n",
    "            article_list  = []\n",
    "        else:\n",
    "            customer_list.append(df_train.loc[indices.flatten()[i],'customer_id'])\n",
    "            article_list.append(df_train.loc[indices.flatten()[i],'article_id'])\n",
    "        \n",
    "    df_neighbourhood = pd.DataFrame({'customer_id': customer_list, 'article_id': article_list})\n",
    "    del customer_list, article_list\n",
    "    \n",
    "    return df_neighbourhood  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Top Selling Products in 'Neighbourhood' for a Customer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return Top 12 Articles Purchased by the Customer 'Neighbourhood'\n",
    "def get_Top12_Articles_from_CustNNs(df, num_of_articles):\n",
    "      \n",
    "    # Finding number of times a particular item is purchased by customers in the neighbourhood\n",
    "    temp = df.groupby(['article_id'])['customer_id'].agg('count').reset_index() \n",
    "    temp.columns = ['article_id','count']\n",
    "    \n",
    "    # Sort the articles by the number of time purchased by the 'neighbourhood' customers\n",
    "    temp = temp.sort_values('count', ascending=False)\n",
    "    \n",
    "    # Select the Top 12 - if there are duplicate just select the first entry\n",
    "    temp = temp.nlargest(num_of_articles,'count', keep='first')\n",
    "    \n",
    "    # Isolate the Article Ids\n",
    "    temp = temp['article_id']\n",
    "    \n",
    "    # Convert to dataframe and reset index\n",
    "    df_article_list = pd.DataFrame({'article_id': temp})\n",
    "    df_article_list = df_article_list.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # Return the list of Top 12 articles\n",
    "    return df_article_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke Function to Build KNN Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Training dataframe index - remove old one; this allows for extraction of correct index records\n",
    "# returned by the NearestNeighbors function\n",
    "train.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function to set up NN learner\n",
    "hm_model_knn, df_cust_articles_nn = setup_NN_model(train, NN_metric = 'cosine', NN_algo = 'brute')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The solution file to have 1371980 prediction rows - use Sample Submission to construct  <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Sample Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Sample Submission File\n",
    "df_sub  = pd.read_csv('../input/sample_submission.csv',\n",
    "                            #usecols= ['customer_id'], \n",
    "                            dtype={'customer_id': 'string'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record record size of file for later validation.  <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numCustomers = df_sub.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function to read Customer IDs from Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return Customer Ids from sample submission file in sequence\n",
    "def get_next_SubmissionCustomer(df_full_cust_sub, idx):\n",
    "        \n",
    "    # Temp code to build submission framework\n",
    "    df_sub_sample = df_full_cust_sub.sample(frac=0.001, replace=False)  \n",
    "    \n",
    "    # Get Customer Id from Sample Sub\n",
    "    cust_id       = df_sub_sample.iloc[idx,0]\n",
    "    \n",
    "    df_cust_id = df_sub_sample.head(idx+1)\n",
    "    df_cust_id = df_cust_id.drop(['prediction'], axis = 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Return index of customer id in training dataframe and the customer id itself\n",
    "    return df_cust_id, cust_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Customer IDs from Submission File and Find Article Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Function to Obtain Customer Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_CustomerTrxn(rec):\n",
    "    \n",
    "    # Get Next Customer Id from Submission Table\n",
    "    df_one_cust, cust_id = get_next_SubmissionCustomer(df_sub, rec)\n",
    "\n",
    "    \n",
    "    # Merge Customer atributes from the Customer dataset based on a join with the 'customer_id' field\n",
    "    df_SubCust_Trxn = pd.merge(df_one_cust, df_trxns, how=\"left\", on=[\"customer_id\"])\n",
    "\n",
    "    \n",
    "    # On rare occassion that a customer has no transactions - create a dummy records to avoid a failure\n",
    "    if df_SubCust_Trxn.shape[0] == 0:\n",
    "        print('NO Transactions!')\n",
    "        # Create dummy dataframe\n",
    "        df_Dummy = pd.DataFrame([[cust_id,\n",
    "                '2016-10-25',\n",
    "                 651329001,\n",
    "                 0.021593,\n",
    "                 1]], columns=['customer_id','t_dat','article_id','price','sales_channel_id'])\n",
    "        # Stack the DataFrames on top of each other\n",
    "        df_SubCust_Trxn = pd.concat([df_SubCust_Trxn, df_Dummy], axis=0)\n",
    "        df_SubCust_Trxn.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    \n",
    "    # Use last transaction\n",
    "    df_SubCust_Trxn1 = df_SubCust_Trxn.sort_values(by=['t_dat'], ascending=False).head(1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df_SubCust_Trxn1, cust_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Function to Obtain Customer Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_CustomerAttrs(df):\n",
    "\n",
    "    \n",
    "    # Merge Customer atributes from the Customer dataset based on a join with the 'customer_id' field\n",
    "    df = pd.merge(df, df_customers, how=\"left\", on=[\"customer_id\"])\n",
    "    \n",
    "    # Filter out other Customer specific attributes apart from; age + fashion_news_frequency + club_member_status + postal code\n",
    "    # There are very many missing values in the other Customer dataset columns, so these will be ignored\n",
    "    df = df.iloc[:, [0, 1, 2, 3, 4, 7, 8, 9, 10]]\n",
    "\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Function to Obtain Article Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_ArticleAttrs(df):\n",
    "    \n",
    "    # Merge Articles atributes from the Articles dataset based on a join with the 'article_id' field\n",
    "    df = pd.merge(df, df_Articles, how=\"left\", on=[\"article_id\"])\n",
    "    \n",
    "    # Remove 'redundant' categorical columns from Articles (sample) dataframe\n",
    "    # Consider a categorical column Redundant when is 'partnered' with a numerical code \n",
    "    df.drop([\"prod_name\", \n",
    "             \"product_type_name\", \n",
    "             \"graphical_appearance_name\",\n",
    "             \"colour_group_name\",\n",
    "             \"perceived_colour_value_name\",\n",
    "             \"perceived_colour_master_name\",\n",
    "             \"department_name\",\n",
    "             \"index_name\",\n",
    "             \"index_group_name\",\n",
    "             \"section_name\",\n",
    "             \"garment_group_name\",\n",
    "             \"detail_desc\"], axis=1, inplace=True)\n",
    "    \n",
    "    # Encode Categorical Values\n",
    "    for column_name in article_cols_to_encode:\n",
    "        df = encode_Categor_Features(df, column_name)\n",
    "\n",
    "    \n",
    "    # NOT NEEDED!\n",
    "    # for data_types in scale_dtypes:\n",
    "    #    scale_trainset_features(df_SubCust_Trxn1, data_types, no_scale)\n",
    "    \n",
    "    \n",
    "    return df    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Function to Obtain Article Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ArticlePredictions(knn_mdl, df, iNum_of_neighbours, TopN):\n",
    "        \n",
    "    # Call function to find nearest neighbour Customer Ids and Article Ids\n",
    "    # Pass in index, parameters for Nearest Neighbour function, and the number of neighbours to retrun\n",
    "    df_neighbours = get_CustSub_NeighbourIDs_articles(knn_mdl, df, train, iNum_of_neighbours)\n",
    "    \n",
    "    # Call function to return list of the most popular Articles in terms of the \n",
    "    # purchases made by the customers neighbourhood\n",
    "    df_cust_preds = get_Top12_Articles_from_CustNNs(df_neighbours, TopN)\n",
    "\n",
    "    \n",
    "    return df_cust_preds    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Functions to Parse Article Predictions for Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return Article Id Predictions in String format required for Kaggle competition\n",
    "def get_str_Article_Predictions(df):\n",
    "    \n",
    "    article_pred = df['article_id'].values.tolist()\n",
    "    \n",
    "    article_pred = ['0' + str(article_id) for article_id in article_pred]\n",
    "    article_pred_str =  ' '.join(article_pred)\n",
    "    \n",
    "    str_article_preds = article_pred_str\n",
    "    \n",
    "    return str_article_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Build up dataframe for submission\n",
    "def build_up_submission_df(cust, str_preds):\n",
    "    \n",
    "    str = cust + ' ' + str_preds\n",
    "    #print(str)   \n",
    "    \n",
    "    # List1 \n",
    "    lst = [[cust, str_preds]]    \n",
    "    df = pd.DataFrame(lst, columns =['customer_id', 'prediction'])\n",
    "    final_df =df\n",
    "    \n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterate Through Submission File to Generate Article Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty Dataframe with column names only - will be filled with each Customer and their set of predictions\n",
    "df_Submission_Stack = pd.DataFrame(columns=['customer_id', 'prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up loop based on Submission file size\n",
    "customer_records = 100\n",
    "\n",
    "\n",
    "# Generate article predictions for each customer write to a new submission file\n",
    "for cust_rec in range(0, customer_records):\n",
    "    \n",
    "    print('cust_rec is... ' + str(cust_rec) +'\\n')\n",
    "    \n",
    "    df_SubCust_Trxn1, cust_id  = find_latest_CustomerTrxn(cust_rec)\n",
    "    \n",
    "    df_SubCust_Trxn1  = find_latest_CustomerAttrs(df_SubCust_Trxn1)\n",
    "    \n",
    "    df_SubCust_Trxn1  = find_latest_ArticleAttrs(df_SubCust_Trxn1)\n",
    "    \n",
    "    df_CustNNVectors  = df_SubCust_Trxn1[chk_cols]\n",
    "    \n",
    "    #display(df_CustNNVectors.iloc[0].values.reshape(1, -1))\n",
    "    \n",
    "    iNeighbours       = 450 # Number of Neighbours parameter for KNN model\n",
    "    TopN              = 12  # Number of Artcile predictions to return\n",
    "    df_cust_preds     = get_ArticlePredictions(hm_model_knn, df_CustNNVectors, iNeighbours, TopN)\n",
    "    \n",
    "    # Get a string output of Customer Article Predicitons - format as required for Kaggle competition\n",
    "    str_articles_predictions = get_str_Article_Predictions(df_cust_preds)\n",
    "    \n",
    "    # Format Customer ID and list of Article predictions - add to dataframe to be used for CSV output\n",
    "    df_NextCust_Preds = build_up_submission_df(cust_id, str_articles_predictions)\n",
    "    \n",
    "    \n",
    "    # Stack the DataFrames on top of each other\n",
    "    df_Submission_Stack = pd.concat([df_Submission_Stack, df_NextCust_Preds], axis=0)\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Predictions Submission File to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SetUp Disk Directory for Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up folder for submission file with predictions\n",
    "submission_path = '../submission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder above project directory\n",
    "os.makedirs(submission_path, exist_ok=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Submission .CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Submission_Stack = df_Submission_Stack.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write pre-test submission file to disk\n",
    "df_Submission_Stack.to_csv(submission_path+'/submission_pretest.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTempFinal  = pd.read_csv(submission_path+'/submission_pretest.csv')\n",
    "\n",
    "[print(\"Matching the number of dfSub rows to the correct submission size. \\nSubmission Row: {} v Expected Rows: {}\"\n",
    "                                                                                   .format(dfTempFinal.shape[0],numCustomers))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check format of final submission file\n",
    "dfTempFinal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write final submission file to disk\n",
    "dfTempFinal.to_csv(submission_path+'/submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a class=\"tocSkip\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "659px",
    "left": "0px",
    "right": "1227.5px",
    "top": "87px",
    "width": "341.5px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
