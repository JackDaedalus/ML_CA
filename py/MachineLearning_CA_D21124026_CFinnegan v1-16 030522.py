#!/usr/bin/env python
# coding: utf-8

# # Machine Learning - Continuous Assessment - Class Group: TU060 <a class="tocSkip">

# # MSc in Computer Science - Data Science (Part Time) <a class="tocSkip">

# # Student Name: Ciaran Finnegan  <a class="tocSkip">

# # Student No: D21124026 <a class="tocSkip">

# #### Student EMail: D21124026@mytudublin.ie <a class="tocSkip">

# ## May 2022<a class="tocSkip">

# # 

# # H&M Personalised Fashion Recommendations <a class="tocSkip">
# 

# #### This project takes the datasets provided by Kaggle for the H&M Personalised Fashion Recommendations competition.  <a class="tocSkip">
# 
# The objective of the Kaggle competition is to produce a list of 12 predicted purchases for each client for the seven days after the end of the provided transaction file. Kaggle submission are verified against unseen test data.
#     
# To train a model, this project uses the last calendar week of transaction data (mid September) as the basis for the 'actual data' and trains on customer purchases in the previous weeks (Aug - September) across the available years in the transaction files.
#     
# The list of 'Actual' purchases made by a customer in the mid September test data are compared against model predictons generated from top 12 purchases made by the 'Nearest Neigbours'.
#     
#     
# In line with standard ML workflows the data is cleaned, augmented, and scaled in advance of the modelling process.    

# ## <a class="tocSkip">

# #### The CA follows a 7-part structure. <a class="tocSkip">
# 
# Section 1: Python Library Imports
#     
# Section 2: Data Analysis, Import and Preparation
# 
# Section 3: Building of Training and Test Datasets
# 
# Section 4: Scaling and splitting the data
#     
# Section 5: Training a KNN 'Nearest Neighbour' model to generate predictions
#     
# Section 6: Evaluation of the predictions generated by the trained model against Test Data
#     
# Section 7: Submission of a test file to the Kaggle competition   
# 

# ### <a class="tocSkip">

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#Jupyter-Notebook-Set-Up" data-toc-modified-id="Jupyter-Notebook-Set-Up-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Jupyter Notebook Set Up</a></span><ul class="toc-item"><li><span><a href="#Import-Libraries" data-toc-modified-id="Import-Libraries-1.1"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>Import Libraries</a></span><ul class="toc-item"><li><span><a href="#General-Utility-Libraries" data-toc-modified-id="General-Utility-Libraries-1.1.1"><span class="toc-item-num">1.1.1&nbsp;&nbsp;</span>General Utility Libraries</a></span></li><li><span><a href="#Panda-+-Numpy" data-toc-modified-id="Panda-+-Numpy-1.1.2"><span class="toc-item-num">1.1.2&nbsp;&nbsp;</span>Panda + Numpy</a></span></li><li><span><a href="#Matplotlib-Library" data-toc-modified-id="Matplotlib-Library-1.1.3"><span class="toc-item-num">1.1.3&nbsp;&nbsp;</span>Matplotlib Library</a></span></li><li><span><a href="#Plotly-Libraries" data-toc-modified-id="Plotly-Libraries-1.1.4"><span class="toc-item-num">1.1.4&nbsp;&nbsp;</span>Plotly Libraries</a></span></li></ul></li></ul></li><li><span><a href="#Project-Data-Import,-Analysis-and-Preparation" data-toc-modified-id="Project-Data-Import,-Analysis-and-Preparation-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>Project Data Import, Analysis and Preparation</a></span><ul class="toc-item"><li><span><a href="#Load-and-Sample-Kaggle-H&amp;M-Datasets" data-toc-modified-id="Load-and-Sample-Kaggle-H&amp;M-Datasets-2.1"><span class="toc-item-num">2.1&nbsp;&nbsp;</span>Load and Sample Kaggle H&amp;M Datasets</a></span><ul class="toc-item"><li><span><a href="#Set-Up-File-Paths" data-toc-modified-id="Set-Up-File-Paths-2.1.1"><span class="toc-item-num">2.1.1&nbsp;&nbsp;</span>Set Up File Paths</a></span></li><li><span><a href="#H&amp;M-Transaction-Dataset" data-toc-modified-id="H&amp;M-Transaction-Dataset-2.1.2"><span class="toc-item-num">2.1.2&nbsp;&nbsp;</span>H&amp;M Transaction Dataset</a></span></li><li><span><a href="#H&amp;M-Articles-Dataset" data-toc-modified-id="H&amp;M-Articles-Dataset-2.1.3"><span class="toc-item-num">2.1.3&nbsp;&nbsp;</span>H&amp;M Articles Dataset</a></span></li><li><span><a href="#H&amp;M-Customers-Dataset" data-toc-modified-id="H&amp;M-Customers-Dataset-2.1.4"><span class="toc-item-num">2.1.4&nbsp;&nbsp;</span>H&amp;M Customers Dataset</a></span></li></ul></li><li><span><a href="#Exploratory-Data-Analysis" data-toc-modified-id="Exploratory-Data-Analysis-2.2"><span class="toc-item-num">2.2&nbsp;&nbsp;</span>Exploratory Data Analysis</a></span><ul class="toc-item"><li><span><a href="#H&amp;M-Sampled-Transaction-Dataset" data-toc-modified-id="H&amp;M-Sampled-Transaction-Dataset-2.2.1"><span class="toc-item-num">2.2.1&nbsp;&nbsp;</span>H&amp;M Sampled Transaction Dataset</a></span></li><li><span><a href="#H&amp;M-Sampled-Customer-Dataset" data-toc-modified-id="H&amp;M-Sampled-Customer-Dataset-2.2.2"><span class="toc-item-num">2.2.2&nbsp;&nbsp;</span>H&amp;M Sampled Customer Dataset</a></span></li><li><span><a href="#H&amp;M-Sampled-Articles-Dataset" data-toc-modified-id="H&amp;M-Sampled-Articles-Dataset-2.2.3"><span class="toc-item-num">2.2.3&nbsp;&nbsp;</span>H&amp;M Sampled Articles Dataset</a></span></li></ul></li><li><span><a href="#Data-Preparation-and-Feature-Enrichment" data-toc-modified-id="Data-Preparation-and-Feature-Enrichment-2.3"><span class="toc-item-num">2.3&nbsp;&nbsp;</span>Data Preparation and Feature Enrichment</a></span><ul class="toc-item"><li><span><a href="#Transaction-Data-Feature-Enrichment-(Sample)" data-toc-modified-id="Transaction-Data-Feature-Enrichment-(Sample)-2.3.1"><span class="toc-item-num">2.3.1&nbsp;&nbsp;</span>Transaction Data Feature Enrichment (Sample)</a></span></li><li><span><a href="#Extend-Dataframe-with-Customer-Data" data-toc-modified-id="Extend-Dataframe-with-Customer-Data-2.3.2"><span class="toc-item-num">2.3.2&nbsp;&nbsp;</span>Extend Dataframe with Customer Data</a></span></li><li><span><a href="#Extend-Dataframe-with-Articles-Data" data-toc-modified-id="Extend-Dataframe-with-Articles-Data-2.3.3"><span class="toc-item-num">2.3.3&nbsp;&nbsp;</span>Extend Dataframe with Articles Data</a></span></li><li><span><a href="#Final-Data-Preparation-Steps" data-toc-modified-id="Final-Data-Preparation-Steps-2.3.4"><span class="toc-item-num">2.3.4&nbsp;&nbsp;</span>Final Data Preparation Steps</a></span></li></ul></li></ul></li><li><span><a href="#Mark-Data-:-Train-and-Test-Sets" data-toc-modified-id="Mark-Data-:-Train-and-Test-Sets-3"><span class="toc-item-num">3&nbsp;&nbsp;</span>Mark Data : Train and Test Sets</a></span><ul class="toc-item"><li><span><a href="#Display-Data-Analysis-of-Merged-Dataset" data-toc-modified-id="Display-Data-Analysis-of-Merged-Dataset-3.1"><span class="toc-item-num">3.1&nbsp;&nbsp;</span>Display Data Analysis of Merged Dataset</a></span></li><li><span><a href="#Create-Training-Set-Based-on-Date-Rane-(-2-Wk-to--4-Wk)" data-toc-modified-id="Create-Training-Set-Based-on-Date-Rane-(-2-Wk-to--4-Wk)-3.2"><span class="toc-item-num">3.2&nbsp;&nbsp;</span>Create Training Set Based on Date Rane (-2 Wk to -4 Wk)</a></span></li><li><span><a href="#Create-Test-Set-Based-on-Date-Range-(-1-Wk)" data-toc-modified-id="Create-Test-Set-Based-on-Date-Range-(-1-Wk)-3.3"><span class="toc-item-num">3.3&nbsp;&nbsp;</span>Create Test Set Based on Date Range (-1 Wk)</a></span></li><li><span><a href="#Verify-Date-Ranges-in-Train/Test-Sets" data-toc-modified-id="Verify-Date-Ranges-in-Train/Test-Sets-3.4"><span class="toc-item-num">3.4&nbsp;&nbsp;</span>Verify Date Ranges in Train/Test Sets</a></span></li><li><span><a href="#Set-up-Train/Test-Sets" data-toc-modified-id="Set-up-Train/Test-Sets-3.5"><span class="toc-item-num">3.5&nbsp;&nbsp;</span>Set up Train/Test Sets</a></span><ul class="toc-item"><li><span><a href="#Mark-Date-Filtered-Dataframes" data-toc-modified-id="Mark-Date-Filtered-Dataframes-3.5.1"><span class="toc-item-num">3.5.1&nbsp;&nbsp;</span>Mark Date Filtered Dataframes</a></span></li><li><span><a href="#Concatenate-to-Combine-Date-Filtered-Dataframes" data-toc-modified-id="Concatenate-to-Combine-Date-Filtered-Dataframes-3.5.2"><span class="toc-item-num">3.5.2&nbsp;&nbsp;</span>Concatenate to Combine Date Filtered Dataframes</a></span></li></ul></li></ul></li><li><span><a href="#Scale-and-Split-Data:-Train-and-Test-Sets" data-toc-modified-id="Scale-and-Split-Data:-Train-and-Test-Sets-4"><span class="toc-item-num">4&nbsp;&nbsp;</span>Scale and Split Data: Train and Test Sets</a></span><ul class="toc-item"><li><span><a href="#Scale-Date-Filtered-Transactions" data-toc-modified-id="Scale-Date-Filtered-Transactions-4.1"><span class="toc-item-num">4.1&nbsp;&nbsp;</span>Scale Date Filtered Transactions</a></span></li><li><span><a href="#Split-Date-Filtered-Transactions:-Train-and-Test-Data" data-toc-modified-id="Split-Date-Filtered-Transactions:-Train-and-Test-Data-4.2"><span class="toc-item-num">4.2&nbsp;&nbsp;</span>Split Date Filtered Transactions: Train and Test Data</a></span></li></ul></li><li><span><a href="#K-Nearest-Neighbour-Operations" data-toc-modified-id="K-Nearest-Neighbour-Operations-5"><span class="toc-item-num">5&nbsp;&nbsp;</span>K Nearest Neighbour Operations</a></span><ul class="toc-item"><li><span><a href="#Set-Up-the-Columns-to-be-Input-to-KNN" data-toc-modified-id="Set-Up-the-Columns-to-be-Input-to-KNN-5.1"><span class="toc-item-num">5.1&nbsp;&nbsp;</span>Set Up the Columns to be Input to KNN</a></span></li><li><span><a href="#Create-Functions-for-NN-Analysis" data-toc-modified-id="Create-Functions-for-NN-Analysis-5.2"><span class="toc-item-num">5.2&nbsp;&nbsp;</span>Create Functions for NN Analysis</a></span><ul class="toc-item"><li><span><a href="#Set-up-function-to-Build-Nearest-Neighbours-Model" data-toc-modified-id="Set-up-function-to-Build-Nearest-Neighbours-Model-5.2.1"><span class="toc-item-num">5.2.1&nbsp;&nbsp;</span>Set up function to Build Nearest Neighbours Model</a></span></li><li><span><a href="#Set-up-function-to--Nearest-Neighbours-(+-Articles-bought)-for-a-Customer" data-toc-modified-id="Set-up-function-to--Nearest-Neighbours-(+-Articles-bought)-for-a-Customer-5.2.2"><span class="toc-item-num">5.2.2&nbsp;&nbsp;</span>Set up function to  Nearest Neighbours (+ Articles bought) for a Customer</a></span></li><li><span><a href="#Find-Top-Selling-Products-in-'Neighbourhood'-for-a-Customer" data-toc-modified-id="Find-Top-Selling-Products-in-'Neighbourhood'-for-a-Customer-5.2.3"><span class="toc-item-num">5.2.3&nbsp;&nbsp;</span>Find Top Selling Products in 'Neighbourhood' for a Customer</a></span></li><li><span><a href="#Invoke-Function-to-Build-KNN-Model" data-toc-modified-id="Invoke-Function-to-Build-KNN-Model-5.2.4"><span class="toc-item-num">5.2.4&nbsp;&nbsp;</span>Invoke Function to Build KNN Model</a></span></li></ul></li></ul></li><li><span><a href="#Evaluate-Model" data-toc-modified-id="Evaluate-Model-6"><span class="toc-item-num">6&nbsp;&nbsp;</span>Evaluate Model</a></span><ul class="toc-item"><li><span><a href="#Define-Functions-to-Extract-Customer-IDs-and-Article-Predictions-from-Test-Data" data-toc-modified-id="Define-Functions-to-Extract-Customer-IDs-and-Article-Predictions-from-Test-Data-6.1"><span class="toc-item-num">6.1&nbsp;&nbsp;</span>Define Functions to Extract Customer IDs and Article Predictions from Test Data</a></span><ul class="toc-item"><li><span><a href="#Define-Function-to-read-Customer-IDs-from-Submission-File" data-toc-modified-id="Define-Function-to-read-Customer-IDs-from-Submission-File-6.1.1"><span class="toc-item-num">6.1.1&nbsp;&nbsp;</span>Define Function to read Customer IDs from Submission File</a></span></li><li><span><a href="#Define-Function-to-Obtain-Customer-Transactions" data-toc-modified-id="Define-Function-to-Obtain-Customer-Transactions-6.1.2"><span class="toc-item-num">6.1.2&nbsp;&nbsp;</span>Define Function to Obtain Customer Transactions</a></span></li><li><span><a href="#Define-Function-to-Obtain-Customer-Features" data-toc-modified-id="Define-Function-to-Obtain-Customer-Features-6.1.3"><span class="toc-item-num">6.1.3&nbsp;&nbsp;</span>Define Function to Obtain Customer Features</a></span></li><li><span><a href="#Define-Function-to-Obtain-Article-Features" data-toc-modified-id="Define-Function-to-Obtain-Article-Features-6.1.4"><span class="toc-item-num">6.1.4&nbsp;&nbsp;</span>Define Function to Obtain Article Features</a></span></li><li><span><a href="#Define-Function-to-Obtain-Article-Predictions" data-toc-modified-id="Define-Function-to-Obtain-Article-Predictions-6.1.5"><span class="toc-item-num">6.1.5&nbsp;&nbsp;</span>Define Function to Obtain Article Predictions</a></span></li><li><span><a href="#Define-Functions-to-Parse-Article-Predictions-for-Kaggle-Submission" data-toc-modified-id="Define-Functions-to-Parse-Article-Predictions-for-Kaggle-Submission-6.1.6"><span class="toc-item-num">6.1.6&nbsp;&nbsp;</span>Define Functions to Parse Article Predictions for Kaggle Submission</a></span></li></ul></li><li><span><a href="#Define-Functions-to-Evaluate-Model-Predictions-aginst-Test-Data" data-toc-modified-id="Define-Functions-to-Evaluate-Model-Predictions-aginst-Test-Data-6.2"><span class="toc-item-num">6.2&nbsp;&nbsp;</span>Define Functions to Evaluate Model Predictions aginst Test Data</a></span><ul class="toc-item"><li><span><a href="#Set-up-function-to-return-latest-Customer-transactions-in-the-Test-Data" data-toc-modified-id="Set-up-function-to-return-latest-Customer-transactions-in-the-Test-Data-6.2.1"><span class="toc-item-num">6.2.1&nbsp;&nbsp;</span>Set up function to return latest Customer transactions in the Test Data</a></span></li><li><span><a href="#Define-Function-to-Return-Evaluation-Score" data-toc-modified-id="Define-Function-to-Return-Evaluation-Score-6.2.2"><span class="toc-item-num">6.2.2&nbsp;&nbsp;</span>Define Function to Return Evaluation Score</a></span></li><li><span><a href="#Iterate-Through-Test-Data-and-Generate-Predictions" data-toc-modified-id="Iterate-Through-Test-Data-and-Generate-Predictions-6.2.3"><span class="toc-item-num">6.2.3&nbsp;&nbsp;</span>Iterate Through Test Data and Generate Predictions</a></span></li><li><span><a href="#Present-Accuracy-of-Predictions" data-toc-modified-id="Present-Accuracy-of-Predictions-6.2.4"><span class="toc-item-num">6.2.4&nbsp;&nbsp;</span>Present Accuracy of Predictions</a></span></li></ul></li></ul></li><li><span><a href="#Build-Submission" data-toc-modified-id="Build-Submission-7"><span class="toc-item-num">7&nbsp;&nbsp;</span>Build Submission</a></span><ul class="toc-item"><li><span><a href="#Read-Sample-Submission-File" data-toc-modified-id="Read-Sample-Submission-File-7.1"><span class="toc-item-num">7.1&nbsp;&nbsp;</span>Read Sample Submission File</a></span></li><li><span><a href="#Iterate-Through-Submission-File-to-Generate-Article-Predictions" data-toc-modified-id="Iterate-Through-Submission-File-to-Generate-Article-Predictions-7.2"><span class="toc-item-num">7.2&nbsp;&nbsp;</span>Iterate Through Submission File to Generate Article Predictions</a></span></li><li><span><a href="#Write-Predictions-Submission-File-to-CSV" data-toc-modified-id="Write-Predictions-Submission-File-to-CSV-7.3"><span class="toc-item-num">7.3&nbsp;&nbsp;</span>Write Predictions Submission File to CSV</a></span><ul class="toc-item"><li><span><a href="#SetUp-Disk-Directory-for-Submission-File" data-toc-modified-id="SetUp-Disk-Directory-for-Submission-File-7.3.1"><span class="toc-item-num">7.3.1&nbsp;&nbsp;</span>SetUp Disk Directory for Submission File</a></span></li><li><span><a href="#Create-Submission-.CSV-File" data-toc-modified-id="Create-Submission-.CSV-File-7.3.2"><span class="toc-item-num">7.3.2&nbsp;&nbsp;</span>Create Submission .CSV File</a></span></li></ul></li></ul></li></ul></div>

# ## Jupyter Notebook Set Up

# ### Import Libraries

# Note: It is assumed that the matplotlib and plotly library has been installed on the user's machine before it is available for import.
# 
# ```bash
# > pip install matplotlib
# > pip install plotly
# ```
# 

# #### General Utility Libraries

# In[1]:


import os


# #### Panda + Numpy

# In[2]:


# Python libraries for data manipulation
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.neighbors import NearestNeighbors


# Set up additional libraries for graphical presentations. A number of earlier graphs use Matplotlib. In later sections, Plotly graphs are used.

# #### Matplotlib Library

# In[3]:


#import matplotlib.pyplot as plt


# #### Plotly Libraries

# In[4]:


# For interactive graphics and global displays
#import plotly.express as px
#import plotly.graph_objects as go


# ### <a class="tocSkip">

# In[5]:


import warnings
warnings.filterwarnings("ignore")


# ### <a class="tocSkip">

# ## Project Data Import, Analysis and Preparation

# ### Load and Sample Kaggle H&M Datasets

# ### The assignment starts with a Kaggle H&M Personalised Fashion Recommendations datasets. <a class="tocSkip">

# #### Set Up File Paths 

# In[6]:


# Set up a Global path file variable to use when loading each dataset
Path_To_Inputs = '../input'


# #### H&M Transaction Dataset 

# In[7]:


# Load H&M Transaction dataset
df_trxns = pd.read_csv(os.path.join(Path_To_Inputs,'transactions_train.csv'))


# In[8]:


# Display Number of Rows, Number of Columns in Transaction dataset prior to Sampling
df_trxns.shape


# In[9]:


# Extract random sample of Transactions (without replacement) - 10% of records
df_trxns_sample = df_trxns.sample(frac=0.1, replace=False)


# In[10]:


# Display Number of Rows, Number of Columns in Transaction dataset after Sampling
df_trxns_sample.shape


# In[11]:


# Check out the index
df_trxns_sample.head()


# #### H&M Articles Dataset 

# In[12]:


# Read file, located in 'input' folder above this notebook, for articles data - containing meta data on purchasable items.
df_Articles = pd.read_csv(os.path.join(Path_To_Inputs,'articles.csv'))


# In[13]:


# Display Number of Rows, Number of Columns in Articles dataset
df_Articles.shape


# In[14]:


# Extract only the H&M articles contained in the sample Transaction dataset 
articleIDs_in_Trxn_Sample  = set(df_trxns_sample["article_id"])


# In[15]:


# Create dataset that only contains H&M articles in the sample Transaction dataset
df_articles_sample = df_Articles[df_Articles["article_id"].isin(articleIDs_in_Trxn_Sample)]


# In[16]:


# Display Number of Rows, Number of Columns in Sample Articles dataset
df_articles_sample.shape


# #### H&M Customers Dataset 

# In[17]:


# Load H&M Customer dataset
df_customers = pd.read_csv(os.path.join(Path_To_Inputs,'customers.csv'))


# In[18]:


# Display Number of Rows, Number of Columns in Customers dataset prior to Sampling
df_customers.shape


# In[19]:


# Extract only the H&M Customers contained in the sample Transaction dataset 
custIDs_in_Trxn_Sample  = set(df_trxns_sample["customer_id"])


# In[20]:


#Count Customer Ids in Transaction Sample
len(custIDs_in_Trxn_Sample)


# In[21]:


# Create dataset that only contains H&M Customers from the sample Transaction dataset
df_customers_sample = df_customers[df_customers["customer_id"].isin(custIDs_in_Trxn_Sample)]


# In[22]:


# Display Number of Rows, Number of Columns in Sample Customers dataset
df_customers_sample.shape


# ###### 

# ### Exploratory Data Analysis

# ### Display key data attributes and distributions in Sampled H&M datasets. <a class="tocSkip">

# #### H&M Sampled Transaction Dataset 

# In[23]:


# Display basic column information for the initial Kaggle 'transaction_train' datatset - after Sampling of original dataset
df_trxns_sample.head()


# In[24]:


# Display datatype of colums in original H&M Transaction dataset
df_trxns_sample.dtypes


# #### H&M Sampled Customer Dataset 

# In[25]:


# Display basic column information for the initial Kaggle 'customers' datatset - after Sampling of original dataset
df_customers_sample.head()


# In[26]:


df_customers_sample.dtypes


# #### H&M Sampled Articles Dataset 

# In[27]:


# Display basic column information for the initial Kaggle 'article' datatset - after Sampling of original dataset
df_articles_sample.head()


# In[28]:


df_articles_sample.dtypes


# ###### 

# ### Data Preparation and Feature Enrichment

# ### Construct a dataset upon which to build ML models. <a class="tocSkip">

# ### Begin with Transaction Dataset (sampled) and merge in customer and article data. <a class="tocSkip">

# #### Transaction Data Feature Enrichment (Sample)

# In[29]:


# Create new dataset for modelling. Initial basis for new dataset is the sampled transaction dataset.
df_HM_CustTrxn = df_trxns_sample


# In[30]:


# Decompose Datetime information 
df_HM_CustTrxn['t_dat'] = pd.to_datetime(df_HM_CustTrxn['t_dat'], format="%Y-%m-%d")


# The decomposition of the timestamp is used to filter trxn data into Train and Test datsets
# Add Column for Transaction Year and convert to integer value
df_HM_CustTrxn['txn_year'] = df_HM_CustTrxn['t_dat'].dt.strftime('%Y')
df_HM_CustTrxn['txn_year'] = df_HM_CustTrxn['txn_year'].astype(str).astype(int)

# Add Column for Transaction Month and convert to integer value
df_HM_CustTrxn['txn_mth'] = df_HM_CustTrxn['t_dat'].dt.strftime('%m')
df_HM_CustTrxn['txn_mth'] = df_HM_CustTrxn['txn_mth'].astype(str).astype(int)

# Add Column for Transaction Day and convert to integer value
df_HM_CustTrxn['txn_day'] = df_HM_CustTrxn['t_dat'].dt.strftime('%d')
df_HM_CustTrxn['txn_day'] = df_HM_CustTrxn['txn_day'].astype(str).astype(int)


# In[31]:


# Show breakout of timestamp data
df_HM_CustTrxn.head()


# In[32]:


# Check for null values in the Transaction Dataframe
[print('Any Null Values in the Trxn (sample) dataframe?','\n' ,df_HM_CustTrxn.isnull().any())]


# #### Extend Dataframe with Customer Data

# In[33]:


# Confirm dimensions of Trxn (sample) dataframe BEFORE merging with Customer data
df_HM_CustTrxn.shape


# #### Merge Customer Attributes into the Trxn (Sample) Dataframe. <a class="tocSkip">

# In[34]:


# Merge Customer atributes from the Customer dataset based on a join with the 'customer_id' field
df_HM_CustTrxn = pd.merge(df_HM_CustTrxn, df_customers_sample, how="left", on=["customer_id"])


# In[35]:


# Confirm dimensions of Trxn (sample) dataframe AFTER merging with Customer data
df_HM_CustTrxn.shape


# In[36]:


df_HM_CustTrxn.head()


# In[37]:


# Filter out other Customer specific attributes apart from; age + fashion_news_frequency + club_member_status + postal code
# There are very many missing values in the other Customer dataset columns, so these will be ignored
#df_HM_CustTrxn = df_HM_CustTrxn.iloc[:, [0, 1, 2, 3, 4, 5, 6, 9, 10, 11, 12]]
df_HM_CustTrxn.drop(["FN", "Active"], axis=1, inplace=True)


# In[38]:


df_HM_CustTrxn.head()


# In[39]:


df_HM_CustTrxn.shape


# In[40]:


# Check for null values in the merged Trxn/Customer dataframe
[print('Any Null Values in the merged Trxn/Customer dataframe?','\n' ,df_HM_CustTrxn.isnull().any())]


# #### Remove Rows with Missing Club Member Status Attributes. <a class="tocSkip">

# In[41]:


# Check missing values in the club_member_status column  
df_null_club_member_status = df_HM_CustTrxn.loc[(df_HM_CustTrxn["club_member_status"].isnull())]

[print('Numbers of rows with missing club member status?','\n' ,len(df_null_club_member_status.index))]
[print('\nPrecentage of total rows in Trxn dataframe with missing club member status attributes?\n{:06.3f}%'
                               .format((len(df_null_club_member_status.index)/len(df_HM_CustTrxn.index))*100))]


# For simplicity the rows with missing values in the club member status will be ignored
# Re-Build Txn/Customer Dataframe with no missing values in the club member status
df_HM_CustTrxn = df_HM_CustTrxn.loc[(df_HM_CustTrxn["club_member_status"].notnull())]

# Confirm dimensions of Trxn (sample) dataframe AFTER removing rows with missing values in the club member status column
[print('\nCurrent Number of Rows/Columns in Trxn/Customer dataframe - with missing club mem status values removed','\n' ,
                                                                                                   df_HM_CustTrxn.shape)]


# #### Remove Rows with Missing Fashion News Frequency Attributes. <a class="tocSkip">

# In[42]:


# Check missing values in the club_member_status column  
df_null_fn_freq = df_HM_CustTrxn.loc[(df_HM_CustTrxn["fashion_news_frequency"].isnull())]

[print('Numbers of rows with missing fashion_news_frequency values?','\n' ,len(df_null_fn_freq.index))]
[print('\nPrecentage of total rows in Trxn dataframe with missing fashion_news_frequency attributes?\n{:06.3f}%'
                               .format((len(df_null_fn_freq.index)/len(df_HM_CustTrxn.index))*100))]

# For simplicity the rows with missing values in the fashion_news_frequency column will be ignored
# Re-Build Txn/Customer Dataframe with no missing values in the fashion_news_frequency column
df_HM_CustTrxn = df_HM_CustTrxn.loc[(df_HM_CustTrxn["fashion_news_frequency"].notnull())]

# Confirm dimensions of Trxn (sample) dataframe AFTER removing rows with missing values in the fashion_news_frequency column
[print('\nCurrent Number of Rows/Columns in Trxn/Customer dataframe - missing Fash News Freq values removed','\n' ,
                                                                                                   df_HM_CustTrxn.shape)]


# #### Remove Rows with Missing Age Attributes. <a class="tocSkip">

# In[43]:


# Check missing values in the age column  
df_null_age = df_HM_CustTrxn.loc[(df_HM_CustTrxn["age"].isnull())]

[print('Numbers of rows with missing age values?','\n' ,len(df_null_age.index))]
[print('\nPrecentage of total rows in Trxn dataframe with missing age attributes?\n{:06.3f}%'
                               .format((len(df_null_age.index)/len(df_HM_CustTrxn.index))*100))]

# Re-Build Txn/Customer Dataframe with no missing values in the age column
df_HM_CustTrxn = df_HM_CustTrxn.loc[(df_HM_CustTrxn["age"].notnull())]


# Confirm dimensions of Trxn (sample) dataframe AFTER removing rows with missing values in the age column
[print('\nCurrent Number of Rows/Columns in Trxn/Customer dataframe - missing ages removed','\n' ,df_HM_CustTrxn.shape)]


# #### Encode the club_member_status + fashion_news_frequency attributes <a class="tocSkip">

# In[44]:


df_HM_CustTrxn.head()


# In[45]:


def encode_Categor_Features(df, col_name):
       
    # The categorical values in dataframe must be encoded to numberical values
    df_encoded_cols = pd.get_dummies(df[col_name])
    df_encoded      = df.join(df_encoded_cols)
    
    # Remove the prior Categorical column from the Articles dataframe
    df_encoded.drop([col_name], axis=1, inplace=True)
    
    # Return the dataframe with encoded columsn added
    return df_encoded


# In[46]:


cust_cols_to_encode = ["club_member_status","fashion_news_frequency"]
for column_name in cust_cols_to_encode:
    df_HM_CustTrxn = encode_Categor_Features(df_HM_CustTrxn, column_name)


# In[47]:


# Drop Postal Code - a non numerical value that is not easily converted
df_HM_CustTrxn.drop(["postal_code"], axis=1, inplace=True)


# In[48]:


# Display basic column information for the initial Kaggle 'article' datatset - after Sampling of original dataset
df_HM_CustTrxn.head()


# In[49]:


# Confirm dimensions of Trxn (sample) dataframe AFTER encoding Categorical Values
[print('\nCurrent Number of Rows/Columns in Trxn/Customer dataframe - after encoding of categorical values completed','\n' ,
                                                                                                   df_HM_CustTrxn.shape)]


# In[50]:


# Check for null values in the Transaction Dataframe
[print('Any Null Values in the merged Trxn/Customer (sample) dataframe?','\n' ,df_HM_CustTrxn.isnull().any())]


# #### Extend Dataframe with Articles Data

# #### Remove 'Redundant' Categorical Columns in Articles (sample) Dataframe. <a class="tocSkip">

# In[51]:


# Display basic column information for the initial Kaggle 'article' datatset - after Sampling of original dataset
df_articles_sample.head()


# In[52]:


# Remove 'redundant' categotical columns from Articles (sample) dataframe
# Consider a categorical column redundant when is 'partnered' with a numerical code 

redundo_catcols_to_drop =  ["prod_name", 
                            "product_type_name", 
                            "graphical_appearance_name",
                            "colour_group_name",
                            "perceived_colour_value_name",
                            "perceived_colour_master_name",
                            "department_name",
                            "index_name",
                            "index_group_name",
                            "section_name",
                            "garment_group_name",
                            "detail_desc"]


df_articles_sample.drop(redundo_catcols_to_drop, axis=1, inplace=True)


# In[53]:


# Display basic column information for the initial Kaggle 'article' datatset - after Sampling of original dataset
df_articles_sample.head()


# #### Use One-Hot-Encoding on remaining categorical columns. <a class="tocSkip">

# In[54]:


article_cols_to_encode =['product_group_name', 'index_code']
for column_name in article_cols_to_encode:
    df_articles_sample = encode_Categor_Features(df_articles_sample, column_name)


# In[55]:


df_articles_sample.shape


# In[56]:


# Display basic column information for sampled articles dataframe after encoding
df_articles_sample.head()


# #### Merge Trxn/Customers Dataframe with Articles Dataframe (sample). <a class="tocSkip">

# In[57]:


# Merge Articles atributes from the Articles dataset based on a join with the 'article_id' field
df_HM_CustTrxnArtcls = pd.merge(df_HM_CustTrxn, df_articles_sample, how="left", on=["article_id"])


# In[58]:


# Confirm dimensions of Trxn (sample) dataframe AFTER merging with Articles data
df_HM_CustTrxnArtcls.shape


# In[59]:


# Display basic column information for the initial Kaggle 'transaction_train' datatset - AFTER merging with Articles data
df_HM_CustTrxnArtcls.head()


# In[60]:


# Check for null values in the merged Trxn/Customer/Articles dataframe
[print('Any Null Values in the merged Trxn/Customer/Articles dataframe?','\n' ,df_HM_CustTrxnArtcls.isnull().any())]


# #### Final Data Preparation Steps

# #### Remove Any Duplicates in Final Merged Dataframe. <a class="tocSkip">

# In[61]:


# run a check to see if any duplicate entires exist in the merged Trxn/Customer/Articles dataframe
[print('Numbers of duplicated rows in the final merged Trxn/Customer/Articles dataframe?','\n' 
                                                                       ,df_HM_CustTrxnArtcls.duplicated().sum())]


# In[62]:


# Drop duplicate rows in the merged Trxn/Customer/Articles dataframe
df_HM_CustTrxnArtcls = df_HM_CustTrxnArtcls.drop_duplicates()


# In[63]:


# Re-check: Find duplicate rows across all columns
duplicateRows = df_HM_CustTrxnArtcls[df_HM_CustTrxnArtcls.duplicated()]


# In[64]:


# This code will advise if duplicates are successfully removed.
if duplicateRows.shape[0] < 1:
    [print('Success! Duplicate Rows Removed from merged Trxn/Customer/Articles dataframe')]
else:
    [print('Warning! Duplicate Rows Remain in merged Trxn/Customer/Articles dataframe')] 


# In[65]:


# Confirm dimensions of Trxn/Customer/Articles (sample) dataframe AFTER removing duplicate rows
[print('\nCurrent Number of Rows/Columns in merged Trxn/Customer/Articles dataframe - duplicates removed','\n' ,
                                                                                           df_HM_CustTrxnArtcls.shape)]


# In[ ]:


# Confirm column datatypes of Trxn/Customer/Articles (sample) dataframe - useful references view
[print('\nDisplay Column Datatypes in merged Trxn/Customer/Articles dataframe;','\n' ,
                                                                                           df_HM_CustTrxnArtcls.dtypes)]


# ###### 

# ## Mark Data : Train and Test Sets

# ### Display Data Analysis of Merged Dataset

# In[66]:


df_HM_CustTrxnArtcls.shape


# In[67]:


df_HM_CustTrxnArtcls.sort_values('t_dat', ascending=False).head()


# ### Create Training Set Based on Date Rane (-2 Wk to -4 Wk)

# #### Transaction data runs up to 20th Sep 2020.  <a class="tocSkip">

# #### Given that the objective is to predict purchases for a following week, the Test data is taken from the last week of transactions for each year, and the Training dats is based on the previous three weeks for each year.  <a class="tocSkip">

# #### Training Data is split based on the August 23rd to September 14th period for each year in Transaction Data.  <a class="tocSkip">

# In[68]:


# Select rows from the merged Transaction/Customer/Articles dataframe for the Training Data period - all years
df_train_Wk321 = df_HM_CustTrxnArtcls.loc[  (
                                               (df_HM_CustTrxnArtcls['txn_mth'] == 9)  
                                             & (df_HM_CustTrxnArtcls['txn_day'] <= 14) # September 1 - 14
                                             
                                             )
                                          |
                                            (
                                                (df_HM_CustTrxnArtcls['txn_mth'] == 8)
                                              & (df_HM_CustTrxnArtcls['txn_day'] >= 23) # August 23 - 31
                                             )
                                          ]


# ### Create Test Set Based on Date Range (-1 Wk)

# #### Test Data is split based on the last week in Transaction Data (Sept 15th - 22nd) for each year.  <a class="tocSkip">

# In[69]:


# Select rows from the merged Transaction/Customer/Articles dataframe for the Test Data period - all years
df_test_LastWk = df_HM_CustTrxnArtcls.loc[  (df_HM_CustTrxnArtcls['txn_mth'] == 9)
                                   & (df_HM_CustTrxnArtcls['txn_day'] >= 15)
                                   & (df_HM_CustTrxnArtcls['txn_day'] <= 22)] # September 15 - 22


# ### Verify Date Ranges in Train/Test Sets

# In[70]:


# Set up a temp dataframe to display check the date ranges extracted for Training dataset
df_chkTraindf = pd.Series(df_train_Wk321['t_dat'].unique()).sort_values(ascending=False).head(200)

df_chkTraindf


# In[71]:


# Set up a temp dataframe to display check the date ranges extracted for Test dataset
df_chkTestdf = pd.Series(df_test_LastWk['t_dat'].unique()).sort_values(ascending=False).head(100)

df_chkTestdf


# ### Set up Train/Test Sets 

# #### Mark Dataframe as 'Train' and 'Test', then Concatenate. This will allow for later Scaling and Splitting  <a class="tocSkip">

# #### Mark Date Filtered Dataframes

# In[72]:


# Add 'marker' to time filtered data extracted from merged trxn dataframe to make Training dataset
df_train_Wk321['Split'] = 'Train'


# In[73]:


# Add 'marker' to time filtered data extracted from merged trxn dataframe to make Test dataset
df_test_LastWk['Split'] = 'Test'


# In[74]:


df_train_Wk321.head(2)


# In[75]:


df_test_LastWk.head(2)


# #### Concatenate to Combine Date Filtered Dataframes

# In[76]:


# Show concatenated dataframe
df_DateFilteted_Train_and_Test_Data = pd.concat([df_train_Wk321, df_test_LastWk])


# In[77]:


df_DateFilteted_Train_and_Test_Data


# ###### 

# ###### 

# ## Scale and Split Data: Train and Test Sets

# #### Run scaling routines on numerical data before splitting into Train/Test. <a class="tocSkip">

# ### Scale Date Filtered Transactions

# In[78]:


# Normalizing numerical features so that each feature has a value between 0 and 1
scaler = MinMaxScaler()


# In[79]:


# Define a function to scale numerical datatypes within a dataframe based on data type value
def scale_trainset_features(df, dtype, no_scale):
    for x in df.columns[df.dtypes == dtype]:
        if x != no_scale: # Allow for one particular column to avoid scalaing
            df[x] = scaler.fit_transform(np.array(df[x]).reshape(-1,1))    


# In[80]:


no_scale = 'article_id' # Allow for one particular colum to avoid scalaing - Article Id in this case

# Set up data types in dataframe to scale
scale_dtypes = ['int64','int32','float64']

# Call function to scale the numerical values in the datafeame created from transactions/customer/article data
for data_types in scale_dtypes:
    scale_trainset_features(df_DateFilteted_Train_and_Test_Data, data_types, no_scale)


# ### Split Date Filtered Transactions: Train and Test Data

# #### Split the Scaled Data - which is the date filtered trxn data - into Train/Test Datasets. <a class="tocSkip">

# In[81]:


train = df_DateFilteted_Train_and_Test_Data.loc[(df_DateFilteted_Train_and_Test_Data['Split'] == 'Train')]


# In[82]:


test = df_DateFilteted_Train_and_Test_Data.loc[(df_DateFilteted_Train_and_Test_Data['Split'] == 'Test')]


# In[83]:


train.head()


# In[84]:


test.head()


# In[85]:


# Output Confirmation of data split
dataDescription = 'H&M Sample Data Split'
print("\n{} Training Set Shape : \n\t".format(dataDescription))
print(train.shape)
print("\n{} Test Set Shape : \n\t".format(dataDescription))
print(test.shape)


# ###### 

# ###### 

# ###### 

# ## K Nearest Neighbour Operations

# ### Set Up the Columns to be Input to KNN

# In[86]:


# These are the subset of numerical columsn used in the KNN model building for Nearest Neighbours
nn_cols =    ['price',
              'age',
              'product_code',
              'product_type_no',
              'graphical_appearance_no',
              'colour_group_code',
              'perceived_colour_value_id',
              'perceived_colour_master_id',
              'department_no',
              'index_group_no',
              'section_no',
              'garment_group_no',
              'Accessories',
              'Bags',
              'Cosmetic',
              'Furniture',
              'Garment Full body',
              'Garment Lower body',
              'Garment Upper body',
              'Garment and Shoe care',
              'Items',
              'Nightwear',
              'Shoes',
              'Socks & Tights',
              'Stationery',
              'Swimwear',
              'Underwear',
              'Underwear/nightwear',
              'Unknown',
              'A',
              'B',
              'C',
              'D',
              'F',
              'G',
              'H',
              'I',
              'J',
              'S']


# ### Create Functions for NN Analysis

# #### Set up function to Build Nearest Neighbours Model 

# In[87]:


# Set up Unsupervised learner for implementing neighbour searches of Customer Transactions
# Set default parameters for NN algorithm - these can be overwritten during optimisationphase
def setup_NN_model(df_train, NN_metric = 'cosine', NN_algo = 'brute'):
       
    # Set up numerical data inputs for NearestNeighbors function
    df_cust_articles = df_train[nn_cols]
    
    # Set up parameters of NearestNeighbors function
    model_knn = NearestNeighbors(metric = NN_metric, algorithm = NN_algo)
    
    # Implement NearestNeighbors function on merged Trxn/Customer/Article data elements
    model_knn.fit(df_cust_articles)
    
    return model_knn, df_cust_articles


# #### Set up function to  Nearest Neighbours (+ Articles bought) for a Customer 

# In[88]:


# Use a function to return a list of Nearest Neighbours for a given customer.
# Return Neighbour Customer Ids and the articles they both
def get_CustSub_NeighbourIDs_articles(model_knn, df_cust_articles, df_train, NN_Neighbours):
     
    
    # Return index values for nearest neighbours - determined by 'Num_Neighbour' parameter
    distances, indices = model_knn.kneighbors(df_cust_articles.iloc[0].values.reshape(1, -1), 
                                              n_neighbors = NN_Neighbours)
    
    # Using the Index values returned by the Nearest neighbour model
    # Generate a list of Customer Ids and Article Ids for these neighbours
    for i in range(0, len(distances.flatten())):
        if i == 0:
            customer_list = []
            article_list  = []
        else:
            customer_list.append(df_train.loc[indices.flatten()[i],'customer_id'])
            article_list.append(df_train.loc[indices.flatten()[i],'article_id'])
        
    df_neighbourhood = pd.DataFrame({'customer_id': customer_list, 'article_id': article_list})
    del customer_list, article_list
    
    return df_neighbourhood  


# #### Find Top Selling Products in 'Neighbourhood' for a Customer 

# In[89]:


# Function to return Top 12 Articles Purchased by the Customer 'Neighbourhood' Or Actual Data on Purchases
def get_Top12_Articles_for_Cust(df, num_of_articles):
      
    # Finding number of times a particular item is purchased by customers in the neighbourhood
    temp = df.groupby(['article_id'])['customer_id'].agg('count').reset_index() 
    temp.columns = ['article_id','count']
    
    # Sort the articles by the number of time purchased by the 'neighbourhood' customers
    temp = temp.sort_values('count', ascending=False)
    
    # Select the Top 12 - if there are duplicate just select the first entry
    temp = temp.nlargest(num_of_articles,'count', keep='first')
    
    # Isolate the Article Ids
    temp = temp['article_id']
    
    # Convert to dataframe and reset index
    df_article_list = pd.DataFrame({'article_id': temp})
    df_article_list = df_article_list.reset_index(drop=True)
    
    
    # Return the list of Top 12 articles
    return df_article_list


# #### Invoke Function to Build KNN Model  

# In[90]:


# Reset Training dataframe index - remove old one; this allows for extraction of correct index records
# returned by the NearestNeighbors function
train.reset_index(drop=True,inplace=True)


# In[91]:


# Call function to set up NN learner
hm_model_knn, df_cust_articles_nn = setup_NN_model(train, NN_metric = 'cosine', NN_algo = 'brute')


# ###### 

# ###### 

# ###### 

# ## Evaluate Model

# #### The code segments below implement functions to extract predictions and evaluate results.  <a class="tocSkip">

# ### Define Functions to Extract Customer IDs and Article Predictions from Test Data

# #### Define Function to read Customer IDs from Submission File

# In[92]:


# Function to return Customer Ids from sample submission file in sequence
def get_next_SubmissionCustomer(idx):
           
    # Get Customer Id from Sample Sub
    cust_id       = df_sub_sample.iloc[idx,0]
    
    df_cust_id = df_sub_sample.head(idx+1)
    df_cust_id = df_cust_id.drop(['prediction'], axis = 1)
    
    
    
    # Return index of customer id in training dataframe and the customer id itself
    return df_cust_id, cust_id


# #### Define Function to Obtain Customer Transactions

# In[93]:


def find_latest_CustomerTrxn(rec):
    
    # Get Next Customer Id from Submission Table
    df_one_cust, cust_id = get_next_SubmissionCustomer(rec)

    
    # Merge Customer atributes from the Customer dataset based on a join with the 'customer_id' field
    df_SubCust_Trxn = pd.merge(df_one_cust, df_trxns, how="left", on=["customer_id"])

    
    # On rare occassion that a customer has no transactions - create a dummy records to avoid a failure
    if df_SubCust_Trxn.shape[0] == 0:
        print('NO Transactions!')
        # Create dummy dataframe
        df_Dummy = pd.DataFrame([[cust_id,
                '2016-10-25',
                 651329001,
                 0.021593,
                 1]], columns=['customer_id','t_dat','article_id','price','sales_channel_id'])
        # Stack the DataFrames on top of each other
        df_SubCust_Trxn = pd.concat([df_SubCust_Trxn, df_Dummy], axis=0)
        df_SubCust_Trxn.reset_index(drop=True, inplace=True)
    
    
    # Use last transaction
    df_SubCust_Trxn1 = df_SubCust_Trxn.sort_values(by=['t_dat'], ascending=False).head(1)
    
    
    
    return df_SubCust_Trxn1, cust_id


# #### Define Function to Obtain Customer Features

# In[94]:


def find_latest_CustomerAttrs(df):

    
    # Merge Customer atributes from the Customer dataset based on a join with the 'customer_id' field
    df = pd.merge(df, df_customers, how="left", on=["customer_id"])
    
    # Filter out other Customer specific attributes apart from; age + fashion_news_frequency + club_member_status + postal code
    # There are very many missing values in the other Customer dataset columns, so these will be ignored
    df = df.iloc[:, [0, 1, 2, 3, 4, 7, 8, 9, 10]]

    
    
    return df


# #### Define Function to Obtain Article Features

# In[95]:


def find_latest_ArticleAttrs(df):
    
    # Merge Articles atributes from the Articles dataset based on a join with the 'article_id' field
    df = pd.merge(df, df_Articles, how="left", on=["article_id"])
     
    
    # Remove 'redundant' categorical columns from Articles (sample) dataframe
    # Consider a categorical column Redundant when is 'partnered' with a numerical code    
    df.drop(redundo_catcols_to_drop, axis=1, inplace=True)

    # Encode Categorical Values
    for column_name in article_cols_to_encode:
        df = encode_Categor_Features(df, column_name)
    
    
    return df    


# #### Define Function to Obtain Article Predictions

# In[96]:


def get_ArticlePredictions(knn_mdl, df, iNum_of_neighbours, TopN):
    
    # Call function to find nearest neighbour Customer Ids and Article Ids
    # Pass in index, parameters for Nearest Neighbour function, and the number of neighbours to retrun
    df_neighbours = get_CustSub_NeighbourIDs_articles(knn_mdl, df, train, iNum_of_neighbours)
    
    # Call function to return list of the most popular Articles in terms of the 
    # purchases made by the customers neighbourhood
    df_cust_preds = get_Top12_Articles_for_Cust(df_neighbours, TopN)

    
    return df_cust_preds    


# #### Define Functions to Parse Article Predictions for Kaggle Submission

# In[97]:


# Function to return Article Id Predictions in String format required for Kaggle competition
def get_str_Article_Predictions(df):
    
    article_pred = df['article_id'].values.tolist()
    
    article_pred = ['0' + str(article_id) for article_id in article_pred]
    article_pred_str =  ' '.join(article_pred)
    
    str_article_preds = article_pred_str
    
    return str_article_preds


# In[98]:


# Function to Build up dataframe for submission
def build_up_submission_df(cust, str_preds, custid_str, last_col):
    
    lst = [[cust, str_preds]]    

    df = pd.DataFrame(lst, columns =[custid_str, last_col])
    final_df =df
    
    
    return final_df


# ### Define Functions to Evaluate Model Predictions aginst Test Data

# In[99]:


# Set up column layout for dataframe that will score match accuracy of the KNN model against Test Data
eval_Cols = ['customer_id', 'prediction_accuracy','Num_Act_Test_Purchases','Num_Pred_Purchases', 'Num_Act_Purch_Pred']


# #### Set up function to return latest Customer transactions in the Test Data 

# In[100]:


# Get list of unique customers in Test Data
def get_Latest_CustTxn_in_TestData():
        
    df = test.sort_values('t_dat', ascending=False).groupby('customer_id').head(1)
    
    return df


# #### Define Function to Return Evaluation Score

# In[101]:


def return_Cust_Prediction_Score(cust_id, df_pred, df_actual, df_score):  

    # Returns just the rows from the new dataframe that differ from the source dataframe
    merged_df = df_pred.merge(df_actual, indicator=True, how='outer')

    
    matched_rows_df = merged_df[merged_df['_merge'] == 'both']  

    df_no_pred_match = matched_rows_df.drop('_merge', axis=1)
    
    pred_scr = ( len(df_no_pred_match) / len(df_actual) )
    
    lst = [[cust_id, pred_scr, len(df_actual), len(df_pred), len(df_no_pred_match)]]    
    df = pd.DataFrame(lst, columns = eval_Cols)

    
    # Stack the DataFrames on top of each other
    df_score = pd.concat([df_score, df], axis=0)
    
    
    return df_score


# #### Iterate Through Test Data and Generate Predictions

# In[102]:


# Creating an empty Dataframe with column names only - will be filled with each predicton evaluation
df_ModelEvalResults_Stack = pd.DataFrame(columns= eval_Cols)


# #### The last transaction recorded for a given customer will be the basis of the 'Nearest Neighbour' prediction.  <a class="tocSkip">

# #### Look at the characteristics of the last customer trxn and find the 12 most propular products bought in 'neighbouring' transactions.  <a class="tocSkip">

# In[ ]:


## Work Through Test Data to obtain prediction list of Articles

df_Testdata_Customers_LastTrxn = get_Latest_CustTxn_in_TestData()
[print('Number of customers in Test Data is: {}\n'.format(len(df_Testdata_Customers_LastTrxn)))]

# Set loop based on number of customers in Trxn datafile
customer_records = (len(df_Testdata_Customers_LastTrxn))

#customer_records = 250 # Temp loop control for testing

# Generate article predictions for each customer write to a new submission file
for cust_rec in range(0, customer_records):
    
    #print('cust_rec is...  {}\n'.format(df_Testdata_Customers_LastTrxn.iloc[cust_rec]))
       
    # Extract last trxn record for a given customer record from trxn file.
    # This data will be the input vector to submit to find 'Nearest Neighbours'
    index_list = [cust_rec]
    df_CustRec = df_Testdata_Customers_LastTrxn.loc[df_Testdata_Customers_LastTrxn.index[index_list]]         
    df_CustTestNNVectors  = df_CustRec[nn_cols]
    
       
    
    # Obtain the KNN predictions for the Customer from the model
    iNeighbours       = 15 # Number of Neighbours parameter for KNN model - 15
    TopN              = 12  # Number of Article predictions/actual purchases to return     
    df_TestCust_Preds     = get_ArticlePredictions(hm_model_knn, df_CustTestNNVectors, iNeighbours, TopN) # Predictions
        
        
        
    # Obtain the 'actual' transactions made by the Customer - reading the Test dataset
    # Create a dataframe of all transactions by the customer in the Test Data
    C_Id = df_CustRec.iloc[0]['customer_id']  # Isolate Customer ID 
    df_CustRecId = df_CustRec[['customer_id']].copy()
    df_test = test.loc[test["customer_id"] == C_Id]  
    df_TestCust_ActualPurchases = get_Top12_Articles_for_Cust(df_test, TopN) # Actual purchases (from Test dataset)
    
    
    
    # Build up a cumulative set of results as the loop iterates through each customer
    df_ModelEvalResults_Stack = return_Cust_Prediction_Score(df_CustRecId['customer_id'], 
                                                             df_TestCust_Preds, 
                                                             df_TestCust_ActualPurchases,
                                                             df_ModelEvalResults_Stack)
        


# #### Present Accuracy of Predictions

# #### The determination of accuracy is based on;  <a class="tocSkip">

# #### ( (1/Number of Actual Purchases)  *  (Sum(Actual Purchases Predicted)/1) )   *  1/Total Number of Customers ) <a class="tocSkip">

# In[ ]:


# Reset Index on final dataframe with prediction / actual results
df_ModelEvalResults_Stack.reset_index(drop=True,inplace=True)


# In[ ]:


# Sum all results to present an overall accuracy
df_test_eval = df_ModelEvalResults_Stack.drop('customer_id', axis=1)
df_test_eval = pd.DataFrame(np.sum(df_test_eval.values, axis=0), columns=['sum'])
df_test_eval_T = df_test_eval.T # call transpose() function


# In[ ]:


# Add column to capture total numbers of customers
df_test_eval_T['tc'] = customer_records


# In[ ]:


# Rename columns for ease of understanding
df_test_eval_T.columns = ['Sum Prediction Accuracies', 'Actual Test Purchases', 'Predicted Purchases', 
                          'Matching Predictions', 'Total Customers']


# In[ ]:


# Display evaluation results 
df_test_eval_T


# In[ ]:


[print('Overall Accuracy Prediction with Test Data is: {:04.1f}%\n'
       .format( ((df_test_eval_T.iloc[0]['Sum Prediction Accuracies']/df_test_eval_T.iloc[0]['Total Customers'])*100) ))]


# ###### 

# ###### 

# ## Build Submission

# #### The solution file to have 1371980 prediction rows - use Sample Submission to construct  <a class="tocSkip">

# ### Read Sample Submission File

# In[ ]:


# Read in Full Sample Submission File
df_sub  = pd.read_csv('../input/sample_submission.csv', dtype={'customer_id': 'string'})


# In[ ]:


# Temp code to build submission framework based on a smaller fraction of the Sample Submission file
df_sub_sample = df_sub.sample(frac=0.001, replace=False)  


# In[ ]:


# Temp code to check dataframe matches file output
df_sub_sample.head()


# #### Record record size of file for later validation.  <a class="tocSkip">

# In[ ]:


numCustomers = df_sub.shape[0]


# ### Iterate Through Submission File to Generate Article Predictions

# In[ ]:


# Creating an empty Dataframe with column names only - will be filled with each Customer and their set of predictions
df_Submission_Stack = pd.DataFrame(columns=['customer_id', 'prediction'])


# In[ ]:


# Set up loop based on Submission file size
customer_records = 0 # Skip this processing if zero


# Generate article predictions for each customer write to a new submission file
for cust_rec in range(0, customer_records):
    
    print('cust_rec is... ' + str(cust_rec) +'\n')
    
    df_SubCust_Trxn1, cust_id  = find_latest_CustomerTrxn(cust_rec)
    
    df_SubCust_Trxn1  = find_latest_CustomerAttrs(df_SubCust_Trxn1)
    
    df_SubCust_Trxn1  = find_latest_ArticleAttrs(df_SubCust_Trxn1)
    
    df_CustNNVectors  = df_SubCust_Trxn1[nn_cols]
    
    display(df_CustNNVectors)
        
    iNeighbours       = 550 # Number of Neighbours parameter for KNN model
    TopN              = 12  # Number of Article predictions to return
    df_cust_preds     = get_ArticlePredictions(hm_model_knn, df_CustNNVectors, iNeighbours, TopN)
    
    # Get a string output of Customer Article Predicitons - format as required for Kaggle competition
    str_articles_predictions = get_str_Article_Predictions(df_cust_preds)
    
    # Format Customer ID and list of Article predictions - add to dataframe to be used for CSV output
    df_NextCust_Preds = build_up_submission_df(cust_id, str_articles_predictions,'customer_id','prediction')
    
    
    # Stack the DataFrames on top of each other
    df_Submission_Stack = pd.concat([df_Submission_Stack, df_NextCust_Preds], axis=0)
     
    


# ### Write Predictions Submission File to CSV

# #### SetUp Disk Directory for Submission File

# In[ ]:


# Set up folder for submission file with predictions
submission_path = '../submission'


# In[ ]:


# Create folder above project directory
os.makedirs(submission_path, exist_ok=True)  


# #### Create Submission .CSV File

# In[ ]:


df_Submission_Stack = df_Submission_Stack.reset_index(drop=True)


# In[ ]:


# Write pre-test submission file to disk
df_Submission_Stack.to_csv(submission_path+'/submission_pretest.csv',index=False)


# In[ ]:


dfTempFinal  = pd.read_csv(submission_path+'/submission_pretest.csv')

[print("Matching the number of dfSub rows to the correct submission size. \nSubmission Row: {} v Expected Rows: {}"
                                                                                   .format(dfTempFinal.shape[0],numCustomers))]


# In[ ]:


# Check format of final submission file
dfTempFinal.head()


# In[ ]:


# Write final submission file to disk
dfTempFinal.to_csv(submission_path+'/submission.csv',index=False)


# ###### 

# ###### 

# #### <a class="tocSkip">
